"""
QAç­”æ¡ˆç”Ÿæˆæœå‹™

å°ˆé–€è™•ç†ç­”æ¡ˆç”Ÿæˆçš„å„ç¨®å ´æ™¯
ä½¿ç”¨çµ±ä¸€ AI æŽ¥å£,æ”¯æŒç”¨æˆ¶æ¨¡åž‹åå¥½
"""
import logging
import json
from typing import List, Optional, Dict, Any, Tuple
from motor.motor_asyncio import AsyncIOMotorDatabase
from pydantic import ValidationError

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.core.config import settings
from app.models.vector_models import QueryRewriteResult, LLMContextDocument, SemanticSearchResult
from app.models.ai_models_simplified import AIDocumentAnalysisOutputDetail, AIGeneratedAnswerOutput
from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified, AIResponse as UnifiedAIResponse

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


class QAAnswerService:
    """QAç­”æ¡ˆç”Ÿæˆæœå‹™"""
    
    async def generate_answer(
        self,
        db: AsyncIOMotorDatabase,
        original_query: str,
        documents_for_context: List[Any],
        query_rewrite_result: QueryRewriteResult,
        detailed_document_data: Optional[List[Dict[str, Any]]],
        ai_generated_query_reasoning: Optional[str],
        user_id: Optional[str],
        request_id: Optional[str],
        model_preference: Optional[str] = None,
        ensure_chinese_output: Optional[bool] = None,
        detailed_text_max_length: Optional[int] = None,
        max_chars_per_doc: Optional[int] = None,
        conversation_history: Optional[str] = None,
        # æ–°å¢žï¼šå‘é‡æœç´¢çµæžœï¼Œç”¨æ–¼æä¾›ç²¾ç¢ºçš„ chunk å…§å®¹
        search_results: Optional[List[SemanticSearchResult]] = None
    ) -> Tuple[str, int, float, List[LLMContextDocument]]:
        """
        ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
        
        ä¿ç•™åŽŸæœ‰çš„èšç„¦ä¸Šä¸‹æ–‡é‚è¼¯å’Œé€šç”¨ä¸Šä¸‹æ–‡é‚è¼¯
        
        Returns:
            Tuple[answer_text, tokens_used, confidence, contexts_used]
        """
        actual_contexts_for_llm: List[LLMContextDocument] = []
        context_parts = []
        
        log_details = {
            "num_docs": len(documents_for_context),
            "has_detailed_data": bool(detailed_document_data)
        }
        
        await log_event(
            db=db,
            level=LogLevel.DEBUG,
            message="çµ„è£ä¸Šä¸‹æ–‡ä»¥ç”Ÿæˆç­”æ¡ˆ",
            source="service.qa_answer.generate",
            user_id=user_id,
            request_id=request_id,
            details=log_details
        )
        
        try:
            # === èšç„¦ä¸Šä¸‹æ–‡é‚è¼¯: å„ªå…ˆä½¿ç”¨è©³ç´°è³‡æ–™ ===
            if detailed_document_data and len(detailed_document_data) > 0:
                logger.info(f"ä½¿ç”¨èšç„¦ä¸Šä¸‹æ–‡: {len(detailed_document_data)} å€‹AIé¸ä¸­æ–‡ä»¶çš„è©³ç´°è³‡æ–™")
                
                for i, detail_item in enumerate(detailed_document_data):
                    doc_id = str(detail_item.get("_id", f"unknown_doc_{i}"))
                    detailed_data_str = json.dumps(detail_item, ensure_ascii=False, indent=2)
                    
                    context_preamble = f"æ™ºæ…§æŸ¥è©¢æ–‡ä»¶ {doc_id} çš„è©³ç´°è³‡æ–™:\n"
                    if i == 0 and ai_generated_query_reasoning:
                        context_preamble += f"AI æŸ¥è©¢æŽ¨ç†: {ai_generated_query_reasoning}\n\n"
                    
                    context_preamble += f"æŸ¥è©¢åˆ°çš„ç²¾æº–è³‡æ–™:\n{detailed_data_str}\n\n"
                    context_parts.append(context_preamble)
                    
                    actual_contexts_for_llm.append(LLMContextDocument(
                        document_id=doc_id,
                        content_used=detailed_data_str[:300],
                        source_type="ai_detailed_query"
                    ))
                
                logger.info(f"èšç„¦ä¸Šä¸‹æ–‡: {len(context_parts)} å€‹è©³ç´°æŸ¥è©¢çµæžœ")
            
            # === é€šç”¨ä¸Šä¸‹æ–‡é‚è¼¯: å„ªå…ˆä½¿ç”¨æœç´¢çµæžœçš„ chunk å…§å®¹ ===
            else:
                # ðŸš€ å„ªåŒ–ï¼šå¦‚æžœæœ‰æœç´¢çµæžœï¼Œå„ªå…ˆä½¿ç”¨æœç´¢åˆ°çš„ chunk å…§å®¹
                if search_results and len(search_results) > 0:
                    logger.info(f"ä½¿ç”¨å„ªåŒ–ä¸Šä¸‹æ–‡: {len(search_results)} å€‹æœç´¢çµæžœçš„ chunk å…§å®¹")
                    max_results = settings.MAX_CONTEXT_DOCUMENTS  # ä½¿ç”¨å…¨å±€é…ç½®
                    
                    # å»ºç«‹ document_id åˆ°æ–‡æª”çš„æ˜ å°„ï¼Œç”¨æ–¼ç²å–æ–‡ä»¶å
                    doc_map = {str(doc.id): doc for doc in documents_for_context if hasattr(doc, 'id')}
                    
                    for i, result in enumerate(search_results[:max_results], 1):
                        doc_id_str = result.document_id
                        
                        # ä½¿ç”¨æ–°å¢žçš„ document_summary æ¬„ä½ï¼ˆæ–‡ä»¶æ‘˜è¦ï¼‰
                        # å¦‚æžœæ²’æœ‰ï¼Œfallback åˆ° metadata ä¸­çš„ chunk_summary
                        doc_summary = result.document_summary or ""
                        if not doc_summary and result.metadata:
                            doc_summary = result.metadata.get('chunk_summary', '')
                        
                        chunk_type = result.metadata.get('type', 'unknown') if result.metadata else 'unknown'
                        
                        # ç²å–æ–‡ä»¶å
                        matching_doc = doc_map.get(doc_id_str)
                        filename = getattr(matching_doc, 'filename', 'Unknown') if matching_doc else 'Unknown'
                        
                        # æ§‹å»ºç²¾ç°¡ä¸Šä¸‹æ–‡ (åŒ…å«æ–‡ä»¶æ‘˜è¦ + åŒ¹é…çš„ç‰‡æ®µ)
                        context_content = f"""=== æ–‡æª” {i}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{i}ï¼‰: {filename} ===
ã€æ–‡ä»¶æ‘˜è¦ã€‘: {doc_summary}

ã€åŒ¹é…ç‰‡æ®µã€‘:
{result.summary_text}
"""
                        context_parts.append(context_content)
                        
                        actual_contexts_for_llm.append(LLMContextDocument(
                            document_id=doc_id_str,
                            content_used=result.summary_text[:300],
                            source_type=f"search_chunk_{chunk_type}"
                        ))
                    
                    logger.info(f"å„ªåŒ–ä¸Šä¸‹æ–‡: {len(context_parts)} å€‹æœç´¢çµæžœ chunk")
                    
                    # ðŸ” DEBUG: é¡¯ç¤ºå¯¦éš›æä¾›çµ¦ AI çš„ä¸Šä¸‹æ–‡å…§å®¹
                    logger.info("="*60)
                    logger.info("ðŸ” [DEBUG] å¯¦éš›æä¾›çµ¦ AI çš„ä¸Šä¸‹æ–‡å…§å®¹:")
                    logger.info("="*60)
                    for idx, ctx in enumerate(context_parts, 1):
                        # é™åˆ¶æ¯å€‹é¡¯ç¤ºçš„é•·åº¦ï¼Œé¿å… log å¤ªé•·
                        preview = ctx[:500] + "..." if len(ctx) > 500 else ctx
                        logger.info(f"\nðŸ“„ [ä¸Šä¸‹æ–‡ {idx}]\n{preview}")
                    logger.info("="*60)
                
                # Fallback: å¦‚æžœæ²’æœ‰æœç´¢çµæžœï¼Œä½¿ç”¨æ–‡ä»¶æ‘˜è¦
                else:
                    logger.info("ä½¿ç”¨ Fallback ä¸Šä¸‹æ–‡: æ–‡ä»¶æ‘˜è¦")
                    max_general_docs = settings.MAX_CONTEXT_DOCUMENTS  # ä½¿ç”¨å…¨å±€é…ç½®

                    for i, doc in enumerate(documents_for_context[:max_general_docs], 1):
                        doc_content = ""
                        content_source = "unknown"
                        doc_id_str = str(doc.id) if hasattr(doc, 'id') else f"unknown_doc_{i}"
                        
                        # å˜—è©¦ç²å– AI åˆ†æžçš„æ‘˜è¦
                        ai_summary = None
                        if hasattr(doc, 'analysis') and doc.analysis:
                            if hasattr(doc.analysis, 'ai_analysis_output') and \
                               isinstance(doc.analysis.ai_analysis_output, dict):
                                try:
                                    analysis_output = AIDocumentAnalysisOutputDetail(**doc.analysis.ai_analysis_output)
                                    if analysis_output.key_information and analysis_output.key_information.content_summary:
                                        ai_summary = analysis_output.key_information.content_summary
                                    elif analysis_output.initial_summary:
                                        ai_summary = analysis_output.initial_summary
                                except (ValidationError, Exception):
                                    pass
                        
                        # é¸æ“‡æœ€ä½³å…§å®¹ä¾†æº
                        if ai_summary:
                            doc_content = ai_summary
                            content_source = "ai_summary"
                        elif hasattr(doc, 'extracted_text') and doc.extracted_text:
                            # æˆªæ–·éŽé•·æ–‡æœ¬
                            doc_content = doc.extracted_text[:1000]
                            if len(doc.extracted_text) > 1000:
                                doc_content += "..."
                            content_source = "extracted_text"
                        else:
                            doc_content = f"æ–‡ä»¶ '{getattr(doc, 'filename', 'N/A')}' æ²’æœ‰å¯ç”¨å…§å®¹"
                            content_source = "placeholder"
                        
                        actual_contexts_for_llm.append(LLMContextDocument(
                            document_id=doc_id_str,
                            content_used=doc_content[:300],
                            source_type=content_source
                        ))
                        
                        # â­ ä½¿ç”¨çµ±ä¸€çš„å¼•ç”¨æ ¼å¼ï¼Œç¢ºä¿ AI ç”Ÿæˆçš„å¼•ç”¨èƒ½è¢«å‰ç«¯æ­£ç¢ºè§£æž
                        filename = getattr(doc, 'filename', 'Unknown')
                        context_parts.append(
                            f"=== æ–‡æª” {i}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{i}ï¼‰: {filename} ===\n{doc_content}"
                        )
                    
                    logger.info(f"Fallback ä¸Šä¸‹æ–‡: {len(context_parts)} å€‹æ–‡ä»¶æ‘˜è¦")
            
            # æº–å‚™æŸ¥è©¢
            query_for_answer = query_rewrite_result.rewritten_queries[0] \
                if query_rewrite_result.rewritten_queries else original_query
            
            # æ§‹å»ºæ¸…æ™°çš„ä¸Šä¸‹æ–‡çµæ§‹ï¼šæ˜Žç¢ºåˆ†é›¢å°è©±æ­·å²å’Œç•¶å‰æ–‡æª”
            final_context_parts = []
            
            # å¦‚æžœæœ‰å°è©±æ­·å²ï¼Œæ·»åŠ åˆ°é–‹é ­ä¸¦æ˜Žç¢ºæ¨™è¨»
            if conversation_history:
                final_context_parts.append(
                    f"=== å°è©±æ­·å²ï¼ˆåƒ…ä¾›ç†è§£å•é¡ŒèƒŒæ™¯ï¼Œä¸è¦ç›´æŽ¥å¼•ç”¨ï¼‰ ===\n{conversation_history}\n"
                    f"=== å°è©±æ­·å²çµæŸ ==="
                )
                logger.info("å·²æ·»åŠ å°è©±æ­·å²åˆ°ä¸Šä¸‹æ–‡ï¼ˆæ˜Žç¢ºæ¨™è¨»ï¼‰")
            
            # æ·»åŠ ç•¶å‰æª¢ç´¢åˆ°çš„æ–‡æª”
            if context_parts:
                final_context_parts.append(
                    f"\n=== ç•¶å‰æª¢ç´¢åˆ°çš„æ–‡æª”ï¼ˆè«‹åŸºæ–¼é€™äº›æ–‡æª”å›žç­”ï¼‰ ===\n" +
                    "\n\n".join(context_parts) +
                    "\n=== æ–‡æª”çµæŸ ==="
                )
                logger.info(f"å·²æ·»åŠ  {len(context_parts)} å€‹ç•¶å‰æª¢ç´¢æ–‡æª”ï¼ˆæ˜Žç¢ºæ¨™è¨»ï¼‰")
            
            # èª¿ç”¨çµ±ä¸€ AI æœå‹™ç”Ÿæˆç­”æ¡ˆ
            logger.info(f"èª¿ç”¨AIç”Ÿæˆç­”æ¡ˆ,ä½¿ç”¨æ¨¡åž‹åå¥½: {model_preference}")
            
            ai_response: UnifiedAIResponse = await unified_ai_service_simplified.generate_answer(
                user_question=query_for_answer,
                intent_analysis=query_rewrite_result.intent_analysis or "",
                document_context=final_context_parts,  # ä½¿ç”¨é‡æ–°çµ„ç¹”çš„ä¸Šä¸‹æ–‡
                db=db,
                model_preference=model_preference,  # å‚³éžç”¨æˆ¶æ¨¡åž‹åå¥½
                ai_ensure_chinese_output=ensure_chinese_output,
                detailed_text_max_length=detailed_text_max_length,
                max_chars_per_doc=max_chars_per_doc
            )
            
            tokens_used = ai_response.token_usage.total_tokens if ai_response.token_usage else 0
            answer_text = "Error: AIæœå‹™æœªè¿”å›žæˆåŠŸéŸ¿æ‡‰"
            confidence = 0.1
            
            if ai_response.success and ai_response.output_data:
                if isinstance(ai_response.output_data, AIGeneratedAnswerOutput):
                    answer_text = ai_response.output_data.answer_text
                else:
                    answer_text = f"Error: AIè¿”å›žäº†æ„å¤–æ ¼å¼"
                
                confidence = min(
                    0.9,
                    0.3 + (len(actual_contexts_for_llm) * 0.1) +
                    (0.1 if not answer_text.lower().startswith("error") else -0.2)
                )
                
                await log_event(
                    db=db,
                    level=LogLevel.INFO,
                    message="AIç­”æ¡ˆç”ŸæˆæˆåŠŸ",
                    source="service.qa_answer.generate_success",
                    user_id=user_id,
                    request_id=request_id,
                    details={
                        "model_used": ai_response.model_used,
                        "answer_length": len(answer_text),
                        "tokens": tokens_used,
                        "confidence": confidence
                    }
                )
            else:
                error_msg = ai_response.error_message or "AIç”Ÿæˆç­”æ¡ˆå¤±æ•—"
                await log_event(
                    db=db,
                    level=LogLevel.ERROR,
                    message=f"AIç­”æ¡ˆç”Ÿæˆå¤±æ•—: {error_msg}",
                    source="service.qa_answer.generate_error",
                    user_id=user_id,
                    request_id=request_id,
                    details={"error": error_msg}
                )
                answer_text = f"æŠ±æ­‰,ç„¡æ³•ç”Ÿæˆç­”æ¡ˆ: {error_msg}"
            
            return answer_text, tokens_used, confidence, actual_contexts_for_llm
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            await log_event(
                db=db,
                level=LogLevel.ERROR,
                message=f"ç”Ÿæˆç­”æ¡ˆç•°å¸¸: {str(e)}",
                source="service.qa_answer.generate_exception",
                user_id=user_id,
                request_id=request_id
            )
            return f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤: {str(e)}", 0, 0.0, actual_contexts_for_llm
    
    async def _fallback_basic_query(self, db: AsyncIOMotorDatabase, document) -> Dict[str, Any]:
        """å›žé€€åŸºæœ¬æŸ¥è©¢"""
        try:
            basic_projection = {
                "_id": 1,
                "filename": 1,
                "extracted_text": 1,
                "analysis.ai_analysis_output.key_information.content_summary": 1,
                "analysis.ai_analysis_output.key_information.semantic_tags": 1,
                "analysis.ai_analysis_output.key_information.key_concepts": 1
            }
            
            from app.services.qa_document_processor import remove_projection_path_collisions
            safe_projection = remove_projection_path_collisions(basic_projection)
            
            fetched_data = await db.documents.find_one(
                {"_id": document.id},
                projection=safe_projection
            )
            
            if fetched_data:
                def sanitize(data: Any) -> Any:
                    if isinstance(data, dict):
                        return {k: sanitize(v) for k, v in data.items()}
                    if isinstance(data, list):
                        return [sanitize(i) for i in data]
                    if isinstance(data, uuid.UUID):
                        return str(data)
                    return data
                
                return sanitize(fetched_data)
            
            return {}
            
        except Exception as e:
            logger.error(f"å›žé€€æŸ¥è©¢å¤±æ•—: {e}")
            return {}


# å‰µå»ºå…¨å±€å¯¦ä¾‹
qa_answer_service = QAAnswerService()

