import logging
import uuid
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
import io
from PIL import Image
import os
from fastapi import HTTPException, status

from motor.motor_asyncio import AsyncIOMotorDatabase

from ...models import Document, DocumentStatus, TokenUsage, AIImageAnalysisOutput, AITextAnalysisOutput
from ...crud import crud_documents
from ...core.config import Settings
from .document_processing_service import DocumentProcessingService, SUPPORTED_IMAGE_TYPES_FOR_AI
from ..ai.unified_ai_service_simplified import AIRequest, TaskType as AIServiceTaskType, unified_ai_service_simplified
from ..ai.unified_ai_config import unified_ai_config
from ...core.logging_utils import log_event, LogLevel, AppLogger
from .vectorization_queue import vectorization_queue
from .line_marker_service import add_line_markers, process_text_with_line_markers, remove_line_markers

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()

SUPPORTED_TEXT_TYPES_FOR_AI_PROCESSING = {
    "application/pdf",
    "text/plain",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document",  # .docx
    "application/msword",  # .doc
}

class DocumentTasksService:
    def __init__(self):
        pass

    async def _setup_and_validate_document_for_processing(self, doc_id: uuid.UUID, db: AsyncIOMotorDatabase, user_id_for_log: str, request_id_for_log: Optional[str]) -> Optional[Document]:
        """
        Helper function to perform initial setup and validation for document processing.
        Reloads AI config, fetches document by UUID, and validates file path.
        Returns the Document object if successful, None otherwise.
        """
        try:
            await unified_ai_config.reload_task_configs(db)
            logger.info(f"AIé…ç½®å·²é‡æ–°è¼‰å…¥ (task for doc_id: {doc_id})")
        except Exception as e:
            logger.error(f"å¾Œå°ä»»å‹™åˆå§‹è¨­å®šéŒ¯èª¤ (AI é…ç½®é‡è¼‰ for doc_id {doc_id}): {e}", exc_info=True)

        document_from_db = await crud_documents.get_document_by_id(db, doc_id)
        if not document_from_db:
            logger.error(f"ç„¡æ³•ç²å–æ–‡æª”: ID {doc_id}")
            await log_event(db=db, level=LogLevel.ERROR, message=f"æ–‡ä»¶è™•ç†å¤±æ•—: æ–‡ä»¶è¨˜éŒ„ä¸å­˜åœ¨ for {doc_id}",
                            source="doc_tasks_service._setup.doc_not_found", user_id=user_id_for_log, request_id=request_id_for_log)
            return None

        if not document_from_db.file_path:
            logger.error(f"æ–‡ä»¶è·¯å¾‘æœªè¨­å®š: ID {doc_id}")
            await crud_documents.update_document_status(db, doc_id, DocumentStatus.PROCESSING_ERROR, "æ–‡ä»¶è¨˜éŒ„ç¼ºå°‘è·¯å¾‘")
            await log_event(db=db, level=LogLevel.ERROR, message=f"æ–‡ä»¶è™•ç†å¤±æ•—: æ–‡ä»¶è¨˜éŒ„ç¼ºå°‘è·¯å¾‘ for {doc_id}",
                            source="doc_tasks_service._setup.path_missing", user_id=user_id_for_log, request_id=request_id_for_log)
            return None

        doc_path = Path(document_from_db.file_path)
        if not doc_path.exists():
            logger.error(f"Document file not found at path: {document_from_db.file_path} for doc ID: {doc_id}")
            await crud_documents.update_document_status(db, doc_id, DocumentStatus.PROCESSING_ERROR, "æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œç„¡æ³•è™•ç†")
            await log_event(db=db, level=LogLevel.ERROR, message=f"æ–‡ä»¶è™•ç†å¤±æ•—: æ–‡ä»¶ä¸å­˜åœ¨ {document_from_db.file_path}",
                            source="doc_tasks_service._setup.file_not_found", user_id=user_id_for_log, request_id=request_id_for_log,
                            details={"doc_id": str(doc_id), "path": document_from_db.file_path})
            return None
            
        return document_from_db

    async def _process_image_document(self, document: Document, db: AsyncIOMotorDatabase, user_id_for_log: str, request_id_for_log: Optional[str], ai_ensure_chinese_output: Optional[bool], ai_model_preference: Optional[str] = None, ai_max_output_tokens: Optional[int] = None) -> tuple[Optional[Dict[str, Any]], Optional[TokenUsage], Optional[str], DocumentStatus]:
        analysis_data_to_save: Optional[Dict[str, Any]] = None
        token_usage_to_save: Optional[TokenUsage] = None
        model_used_for_analysis: Optional[str] = None
        new_status = DocumentStatus.ANALYZING

        doc_uuid = document.id
        doc_file_path_str = str(document.file_path) if document.file_path else None
        doc_mime_type = document.file_type
        logger.info(f"Performing AI image analysis for document ID: {doc_uuid}, MIME: {doc_mime_type}")

        try:
            if not doc_file_path_str:
                raise ValueError("File path is missing for image document.")
            temp_doc_processing_service = DocumentProcessingService()
            image_bytes = await temp_doc_processing_service.get_image_bytes(doc_file_path_str) # type: ignore
            if not image_bytes:
                logger.error(f"ç„¡æ³•è®€å–åœ–ç‰‡æ–‡ä»¶å­—ç¯€æ•¸æ“š for doc ID: {doc_uuid}")
                return None, None, None, DocumentStatus.PROCESSING_ERROR
            
            pil_image = Image.open(io.BytesIO(image_bytes))
            ai_response = await unified_ai_service_simplified.analyze_image( # type: ignore
                image=pil_image, model_preference=ai_model_preference, db=db
            )
            
            if not ai_response.success or not isinstance(ai_response.output_data, AIImageAnalysisOutput):
                error_msg = ai_response.error_message or "AI image analysis returned unsuccessful or invalid content type."
                logger.error(f"åœ–ç‰‡åˆ†æå¤±æ•— for doc ID {doc_uuid}: {error_msg}")
                return None, None, None, DocumentStatus.ANALYSIS_FAILED
            
            ai_image_output = ai_response.output_data
            token_usage_to_save = ai_response.token_usage
            model_used_for_analysis = ai_response.model_used
            analysis_data_to_save = ai_image_output.model_dump()
            
            # ğŸ¯ çµ±ä¸€åœ–ç‰‡æ–‡æª”è™•ç†ï¼šå¾å¸¶è¡Œè™Ÿçš„ OCR çµæœç”Ÿæˆç´”æ–‡æœ¬å’Œè¡Œè™Ÿæ˜ å°„
            # AI è¿”å›çš„ extracted_text æ ¼å¼å¦‚ï¼š[L001] å…§å®¹\n[L002] å…§å®¹...
            extracted_text_with_markers = analysis_data_to_save.get("extracted_text", "")
            if extracted_text_with_markers:
                # ç§»é™¤è¡Œè™Ÿæ¨™è¨˜ï¼Œå„²å­˜ç´”æ–‡æœ¬
                clean_extracted_text = remove_line_markers(extracted_text_with_markers)
                
                # åŸºæ–¼å¸¶è¡Œè™Ÿçš„æ–‡æœ¬ç”Ÿæˆ line_mappingï¼ˆç”¨æ–¼å¾ŒçºŒå‘é‡åŒ–æ™‚çš„åº§æ¨™å®šä½ï¼‰
                # æ³¨æ„ï¼šé€™è£¡çš„ line_mapping æ˜¯åŸºæ–¼å¸¶è¡Œè™Ÿæ–‡æœ¬çš„ï¼Œä½† extracted_text å„²å­˜ç´”æ–‡æœ¬
                _, line_mapping = add_line_markers(clean_extracted_text)
                
                # æ›´æ–° analysis_data ä¸­çš„ extracted_text ç‚ºç´”æ–‡æœ¬ç‰ˆæœ¬
                analysis_data_to_save["extracted_text"] = clean_extracted_text
                # å°‡ line_mapping åŠ å…¥ analysis_dataï¼Œä¾›å‘é‡åŒ–æ™‚ä½¿ç”¨
                analysis_data_to_save["_line_mapping"] = line_mapping
                
                logger.info(f"åœ–ç‰‡ {doc_uuid} OCR æ–‡æœ¬è™•ç†å®Œæˆ: {len(clean_extracted_text)} å­—ç¬¦, {len(line_mapping)} è¡Œ")
            
            new_status = DocumentStatus.ANALYSIS_COMPLETED
            if ai_image_output.error_message or (hasattr(ai_image_output, 'content_type') and "Error" in str(ai_image_output.content_type)):
                new_status = DocumentStatus.ANALYSIS_FAILED
            logger.info(f"Image AI analysis status for {doc_uuid}: {new_status.value}")

        except ValueError as ve:
            logger.error(f"ValueError during image processing for doc ID {doc_uuid}: {ve}", exc_info=True)
            new_status = DocumentStatus.PROCESSING_ERROR
        except Exception as e:
            logger.error(f"Unexpected error during image processing for doc ID {doc_uuid}: {e}", exc_info=True)
            new_status = DocumentStatus.PROCESSING_ERROR
        return analysis_data_to_save, token_usage_to_save, model_used_for_analysis, new_status

    async def _process_text_document(self, document: Document, db: AsyncIOMotorDatabase, user_id_for_log: str, request_id_for_log: Optional[str], settings_obj: Settings, ai_ensure_chinese_output: Optional[bool], ai_model_preference: Optional[str] = None, ai_max_output_tokens: Optional[int] = None) -> tuple[Optional[Dict[str, Any]], Optional[TokenUsage], Optional[str], DocumentStatus]:
        analysis_data_to_save: Optional[Dict[str, Any]] = None
        token_usage_to_save: Optional[TokenUsage] = None
        model_used_for_analysis: Optional[str] = None
        new_status = DocumentStatus.ANALYZING
        extracted_text_content: Optional[str] = None

        doc_uuid = document.id
        doc_path = Path(str(document.file_path)) if document.file_path else None
        doc_mime_type = document.file_type
        logger.info(f"Performing text extraction and AI analysis for document ID: {doc_uuid}, MIME: {doc_mime_type}")

        try:
            if not doc_path or not document.file_path:
                logger.error(f"File path is missing for text document ID: {doc_uuid}")
                return None, None, None, DocumentStatus.PROCESSING_ERROR

            doc_processing_service_instance = DocumentProcessingService()
            extracted_text_result, extraction_status, extraction_error = \
                await doc_processing_service_instance.extract_text_from_document(str(doc_path), doc_path.suffix)

            if extraction_status == DocumentStatus.PROCESSING_ERROR or not extracted_text_result or not extracted_text_result.strip():
                error_detail = extraction_error or "æœªèƒ½å¾æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆæ–‡æœ¬å…§å®¹"
                logger.error(f"å¾æ–‡æª” {doc_uuid} ({doc_mime_type}) æå–æ–‡æœ¬æ™‚å‡ºéŒ¯æˆ–çµæœç‚ºç©º: {error_detail}", exc_info=bool(extraction_error))
                await crud_documents.update_document_status(db, doc_uuid, DocumentStatus.EXTRACTION_FAILED, error_detail)
                return None, None, None, DocumentStatus.EXTRACTION_FAILED

            extracted_text_content = extracted_text_result

            # ğŸ¯ Phase 1: ç‚ºæå–çš„æ–‡æœ¬æ·»åŠ è¡Œè™Ÿæ¨™è¨˜
            # é€™æ˜¯ Meta-Chunking é·ç§»çš„é—œéµæ­¥é©Ÿï¼Œç‚º AI é‚è¼¯åˆ†å¡Šæä¾›åº§æ¨™ç³»çµ±
            file_extension = doc_path.suffix if doc_path else ""
            marked_text, line_mapping, batches = process_text_with_line_markers(
                extracted_text_content, file_extension
            )
            logger.info(f"æ–‡æª” {doc_uuid} è¡Œè™Ÿæ¨™è¨˜å®Œæˆ: {len(line_mapping)} è¡Œ" +
                       (f"ï¼Œåˆ†ç‚º {len(batches)} æ‰¹" if batches else ""))

            # å„²å­˜åŸå§‹æ–‡æœ¬å’Œè¡Œè™Ÿæ˜ å°„åˆ° MongoDB
            await crud_documents.update_document_on_extraction_success(
                db, doc_uuid, extracted_text_content, line_mapping
            )
            logger.info(f"æˆåŠŸå¾ {doc_uuid} æå– {len(extracted_text_content)} å­—å…ƒçš„æ–‡æœ¬ä¸¦å„²å­˜è¡Œè™Ÿæ˜ å°„ã€‚é–‹å§‹ AI åˆ†æ...")

            # ğŸ¯ Phase 2: å‚³éå¸¶è¡Œè™Ÿæ¨™è¨˜çš„æ–‡æœ¬çµ¦ AI åˆ†æ
            # AI éœ€è¦çœ‹åˆ°è¡Œè™Ÿæ¨™è¨˜æ‰èƒ½åœ¨ logical_chunks ä¸­è¼¸å‡ºæ­£ç¢ºçš„åº§æ¨™
            text_for_ai = marked_text
            max_prompt_len = settings_obj.AI_MAX_INPUT_CHARS_TEXT_ANALYSIS
            if len(text_for_ai) > max_prompt_len:
                logger.warning(f"å¸¶è¡Œè™Ÿæ–‡æœ¬é•·åº¦ ({len(text_for_ai)}) è¶…éæœ€å¤§å…è¨±é•·åº¦ ({max_prompt_len})ã€‚å°‡é€²è¡Œæˆªæ–·ã€‚ Doc ID: {doc_uuid}")
                text_for_ai = text_for_ai[:max_prompt_len]

            ai_response = await unified_ai_service_simplified.analyze_text( # type: ignore
                text=text_for_ai, model_preference=ai_model_preference, db=db
            )
            
            if not ai_response.success or not isinstance(ai_response.output_data, AITextAnalysisOutput):
                error_msg = ai_response.error_message or "AI text analysis returned unsuccessful or invalid content type."
                logger.error(f"æ–‡æœ¬åˆ†æå¤±æ•— for doc ID {doc_uuid}: {error_msg}")
                return None, None, None, DocumentStatus.ANALYSIS_FAILED
                
            ai_text_output: AITextAnalysisOutput = ai_response.output_data
            token_usage_to_save = ai_response.token_usage
            model_used_for_analysis = ai_response.model_used
            analysis_data_to_save = ai_text_output.model_dump()
            new_status = DocumentStatus.ANALYSIS_COMPLETED
            if ai_text_output.error_message or (hasattr(ai_text_output, 'content_type') and "Error" in str(ai_text_output.content_type)):
                new_status = DocumentStatus.ANALYSIS_FAILED
            logger.info(f"Text AI analysis status for {doc_uuid}: {new_status.value}")

        except Exception as e:
            logger.error(f"Unexpected error during text processing for doc ID {doc_uuid}: {e}", exc_info=True)
            new_status = DocumentStatus.PROCESSING_ERROR
        return analysis_data_to_save, token_usage_to_save, model_used_for_analysis, new_status

    async def _save_analysis_results(self, document: Document, db: AsyncIOMotorDatabase, user_id_for_log: str, request_id_for_log: Optional[str], analysis_data: Optional[Dict[str, Any]], token_usage: Optional[TokenUsage], model_used: Optional[str], processing_status: DocumentStatus, processing_type: str) -> None:
        doc_uuid = document.id
        try:
            if analysis_data and token_usage and processing_status in [DocumentStatus.ANALYSIS_COMPLETED, DocumentStatus.ANALYSIS_FAILED]:
                # ğŸ¯ åœ–ç‰‡æ–‡æª”ï¼šå„²å­˜ç´”æ–‡æœ¬å’Œè¡Œè™Ÿæ˜ å°„
                # å¾ analysis_data ä¸­æå– _line_mappingï¼ˆç”± _process_image_document ç”Ÿæˆï¼‰
                line_mapping_from_image = analysis_data.pop("_line_mapping", None)
                extracted_text_from_image = analysis_data.get("extracted_text", "")
                
                await crud_documents.set_document_analysis(
                    db=db, document_id=doc_uuid, analysis_data_dict=analysis_data,
                    token_usage_dict=token_usage.model_dump(), model_used_str=model_used,
                    analysis_status_enum=processing_status, 
                    analyzed_content_type_str=analysis_data.get("content_type", "Unknown")
                )
                
                # å¦‚æœæ˜¯åœ–ç‰‡åˆ†æä¸”æœ‰ extracted_textï¼Œå„²å­˜åˆ°æ–‡æª”çš„é ‚å±¤æ¬„ä½
                if processing_type == "image_analysis" and extracted_text_from_image:
                    image_update_dict: Dict[str, Any] = {
                        "extracted_text": extracted_text_from_image
                    }
                    if line_mapping_from_image:
                        image_update_dict["line_mapping"] = line_mapping_from_image
                    
                    await crud_documents.update_document(db, doc_uuid, image_update_dict)
                    logger.info(f"åœ–ç‰‡æ–‡æª” {doc_uuid} å·²å„²å­˜ç´”æ–‡æœ¬ ({len(extracted_text_from_image)} å­—ç¬¦) å’Œè¡Œè™Ÿæ˜ å°„")
                
                # æ–°å¢: å¦‚æœåˆ†ææˆåŠŸ,é€²è¡Œå¯¦é«”æå–å’Œèªç¾©è±å¯ŒåŒ–
                if processing_status == DocumentStatus.ANALYSIS_COMPLETED:
                    try:
                        from app.services.document.entity_extraction_service import EntityExtractionService
                        
                        entity_service = EntityExtractionService()
                        enriched_data = await entity_service.enrich_document(db, document, analysis_data)
                        
                        # æ›´æ–°æ–‡æª”çš„ enriched_data å’Œ clustering_status
                        # æ³¨æ„: æ–‡æœ¬å…§å®¹å·²å­˜å„²åœ¨ extracted_text æ¬„ä½ä¸­,ç„¡éœ€é¡å¤–å„²å­˜
                        update_dict = {
                            "enriched_data": enriched_data,
                            "clustering_status": "pending"
                        }
                        
                        await crud_documents.update_document(db, doc_uuid, update_dict)
                        
                        logger.info(f"æˆåŠŸç‚ºæ–‡æª” {doc_uuid} ç”Ÿæˆenriched_data")
                        await log_event(
                            db=db, level=LogLevel.INFO,
                            message=f"æ–‡æª” {doc_uuid} å¯¦é«”æå–å®Œæˆ",
                            source="doc_tasks_service.entity_extraction",
                            user_id=user_id_for_log, request_id=request_id_for_log,
                            details={"doc_id": str(doc_uuid), "title": enriched_data.get("title")}
                        )
                        
                        # æª¢æŸ¥æ˜¯å¦éœ€è¦è§¸ç™¼èšé¡
                        # await self._check_trigger_clustering(db, str(document.owner_id))
                        # TODO: å¯¦ç¾è‡ªå‹•è§¸ç™¼èšé¡é‚è¼¯
                        
                    except Exception as enrich_error:
                        logger.error(f"å¯¦é«”æå–å¤±æ•— for doc {doc_uuid}: {enrich_error}", exc_info=True)
                        await log_event(
                            db=db, level=LogLevel.ERROR,
                            message=f"æ–‡æª” {doc_uuid} å¯¦é«”æå–å¤±æ•—: {str(enrich_error)}",
                            source="doc_tasks_service.entity_extraction.error",
                            user_id=user_id_for_log, request_id=request_id_for_log,
                            details={"doc_id": str(doc_uuid), "error": str(enrich_error)}
                        )
                
                log_level = LogLevel.INFO if processing_status == DocumentStatus.ANALYSIS_COMPLETED else LogLevel.ERROR
                log_message = f"AI {processing_type} for doc ID {doc_uuid} status: {processing_status.value}."
                
                # ğŸ¯ è‡ªå‹•å‘é‡åŒ–: ç•¶ AI åˆ†ææˆåŠŸå®Œæˆå¾Œï¼ŒåŠ å…¥å‘é‡åŒ–éšŠåˆ—
                if processing_status == DocumentStatus.ANALYSIS_COMPLETED:
                    logger.info(f"âœ¨ AI åˆ†æå®Œæˆï¼Œå°‡æ–‡æª” {doc_uuid} åŠ å…¥å‘é‡åŒ–éšŠåˆ—")
                    try:
                        # ä½¿ç”¨éšŠåˆ—ç®¡ç†å‘é‡åŒ–ä»»å‹™ï¼Œé¿å…ä¸¦ç™¼è¡çª
                        await vectorization_queue.add_task(
                            document_id=str(doc_uuid),
                            db=db
                        )
                        logger.info(f"âœ… æ–‡æª” {doc_uuid} å·²åŠ å…¥å‘é‡åŒ–éšŠåˆ—")
                        
                        # ç²å–éšŠåˆ—ç‹€æ…‹
                        queue_status = vectorization_queue.get_status()
                        logger.info(f"ğŸ“Š éšŠåˆ—ç‹€æ…‹: {queue_status}")
                        
                        await log_event(
                            db=db, level=LogLevel.INFO,
                            message=f"æ–‡æª” {doc_uuid} åŠ å…¥å‘é‡åŒ–éšŠåˆ—",
                            source="doc_tasks_service._save_results.auto_vectorize",
                            user_id=user_id_for_log, request_id=request_id_for_log,
                            details={
                                "doc_id": str(doc_uuid), 
                                "trigger": "auto",
                                "queue_status": queue_status
                            }
                        )
                    except Exception as auto_vec_error:
                        # å‘é‡åŒ–å¤±æ•—ä¸æ‡‰å½±éŸ¿åˆ†æç‹€æ…‹ï¼Œåªè¨˜éŒ„éŒ¯èª¤
                        logger.error(f"âŒ æ–‡æª” {doc_uuid} åŠ å…¥å‘é‡åŒ–éšŠåˆ—å¤±æ•—: {auto_vec_error}", exc_info=True)
                        await log_event(
                            db=db, level=LogLevel.ERROR,
                            message=f"æ–‡æª” {doc_uuid} åŠ å…¥å‘é‡åŒ–éšŠåˆ—å¤±æ•—: {str(auto_vec_error)}",
                            source="doc_tasks_service._save_results.auto_vectorize_error",
                            user_id=user_id_for_log, request_id=request_id_for_log,
                            details={"doc_id": str(doc_uuid), "error": str(auto_vec_error)}
                        )
            elif processing_status != DocumentStatus.ANALYZING: # Error or unsupported before full analysis
                await crud_documents.update_document_status(db, doc_uuid, processing_status, f"Processing concluded: {processing_status.value}")
                log_level = LogLevel.WARNING
                log_message = f"Document {doc_uuid} processing ended with status {processing_status.value} without new AI data."
            else: # Should not happen if logic is correct
                await crud_documents.update_document_status(db, doc_uuid, DocumentStatus.PROCESSING_ERROR, "Unknown state in save_analysis_results")
                log_level = LogLevel.ERROR
                log_message = f"Document {doc_uuid} in unexpected state {processing_status.value} at save_analysis_results."
            
            logger.log(logging.getLevelName(log_level.value), log_message) # Use logger's log method for dynamic level
            await log_event(
                db=db, level=log_level, message=log_message,
                source=f"doc_tasks_service._save_results.{processing_type}", user_id=user_id_for_log, request_id=request_id_for_log,
                details={"doc_id": str(doc_uuid), "status": processing_status.value, "tokens": token_usage.total_tokens if token_usage else 0}
            )
        except Exception as e:
            logger.error(f"Error saving analysis results for doc ID {doc_uuid}: {e}", exc_info=True)
            try:
                await crud_documents.update_document_status(db, doc_uuid, DocumentStatus.PROCESSING_ERROR, f"Save results error: {str(e)[:50]}")
            except Exception as db_err: #Nested
                logger.critical(f"CRITICAL: Failed to update doc status after save error for {doc_uuid}: {db_err}", exc_info=True)
            await log_event(db=db, level=LogLevel.CRITICAL, message=f"Critical error saving analysis results for {doc_uuid}: {e}",
                            source="doc_tasks_service._save_results.critical_failure", user_id=user_id_for_log, request_id=request_id_for_log)

    async def process_document_content_analysis(self, doc_id_str: str, db: AsyncIOMotorDatabase, user_id_for_log: str, request_id_for_log: Optional[str], settings_obj: Settings, ai_ensure_chinese_output: Optional[bool] = True, ai_model_preference: Optional[str] = None, ai_max_output_tokens: Optional[int] = None) -> None:
        try:
            doc_uuid = uuid.UUID(doc_id_str)
        except ValueError:
            logger.error(f"Background task: Invalid UUID string for doc_id: {doc_id_str}. Cannot proceed.")
            await log_event(db=db, level=LogLevel.CRITICAL, message=f"Invalid document ID format '{doc_id_str}' in background task.",
                            source="doc_tasks_service.process_doc_content.id_conversion_error", user_id=user_id_for_log, request_id=request_id_for_log,
                            details={"original_doc_id": doc_id_str})
            return

        document = await self._setup_and_validate_document_for_processing(doc_uuid, db, user_id_for_log, request_id_for_log)
        if document is None: return

        logger.info(f"Service task starting processing for document ID (UUID): {document.id}, Original passed ID: {doc_id_str}")
        analysis_data: Optional[Dict[str, Any]] = None
        token_usage: Optional[TokenUsage] = None
        model_used: Optional[str] = None
        processing_status = DocumentStatus.ANALYZING
        processing_type = "unknown"
        
        try:
            if document.file_type and document.file_type in SUPPORTED_IMAGE_TYPES_FOR_AI.values():
                processing_type = "image_analysis"
                analysis_data, token_usage, model_used, processing_status = await self._process_image_document(
                    document, db, user_id_for_log, request_id_for_log, ai_ensure_chinese_output, ai_model_preference, ai_max_output_tokens)
            elif document.file_type and document.file_type in SUPPORTED_TEXT_TYPES_FOR_AI_PROCESSING:
                processing_type = "text_analysis"
                analysis_data, token_usage, model_used, processing_status = await self._process_text_document(
                    document, db, user_id_for_log, request_id_for_log, settings_obj, ai_ensure_chinese_output, ai_model_preference, ai_max_output_tokens)
            else:
                processing_type = "unsupported"
                logger.warning(f"Document ID: {document.id} MIME type '{document.file_type}' unsupported for AI processing.")
                processing_status = DocumentStatus.PROCESSING_ERROR 
            
            await self._save_analysis_results(document, db, user_id_for_log, request_id_for_log, analysis_data, token_usage, model_used, processing_status, processing_type)
        except Exception as e:
            logger.error(f"Service task for doc {document.id} encountered top-level error: {e}", exc_info=True)
            await self._save_analysis_results(document, db, user_id_for_log, request_id_for_log, None, None, None, DocumentStatus.PROCESSING_ERROR, processing_type)

    async def trigger_document_analysis(self, db: AsyncIOMotorDatabase, doc_processor: DocumentProcessingService, document_id: uuid.UUID, current_user_id: uuid.UUID, settings_obj: Settings, processing_strategy: Optional[str] = None, custom_prompt_id: Optional[str] = None, analysis_type: Optional[str] = None, task_type_str: Optional[str] = None, request_id: Optional[str] = None, ai_model_preference: Optional[str] = None, ai_ensure_chinese_output: Optional[bool] = None, ai_max_output_tokens: Optional[int] = None) -> Document:
        logger.debug(f"(Service) Triggering analysis for doc ID: {document_id}, strategy: {processing_strategy}")
        
        # Use the centralized setup and validation method
        document = await self._setup_and_validate_document_for_processing(document_id, db, str(current_user_id), request_id)
        if not document:
            # _setup_and_validate_document_for_processing already logs and sets status
            raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"Document {document_id} not found or setup failed.")

        # Ownership Check (already part of _setup_and_validate if user_id_for_log is current_user_id, but explicit check here is also fine)
        if document.owner_id != current_user_id:
            logger.warning(f"(Service) User {current_user_id} attempt to analyze document {document_id} owned by {document.owner_id}")
            raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="ç„¡æ¬Šé™åˆ†ææ­¤æ–‡ä»¶")

        await crud_documents.update_document_status(db, document.id, DocumentStatus.ANALYZING, "Analysis triggered by service (trigger_document_analysis).")

        effective_task_type: Optional[AIServiceTaskType] = None
        if task_type_str:
            try: effective_task_type = AIServiceTaskType[task_type_str.upper()]
            except KeyError: logger.warning(f"ç„¡æ•ˆçš„ task_type_str: {task_type_str}, å°‡å›é€€åˆ°æ ¹æ“šæ–‡ä»¶é¡å‹æ±ºå®šã€‚")
        
        if not effective_task_type:
            if document.file_type and document.file_type in SUPPORTED_IMAGE_TYPES_FOR_AI.values():
                effective_task_type = AIServiceTaskType.IMAGE_ANALYSIS
            elif document.file_type and document.file_type in SUPPORTED_TEXT_TYPES_FOR_AI_PROCESSING:
                effective_task_type = AIServiceTaskType.TEXT_GENERATION
            else:
                await self._save_analysis_results(document, db, str(current_user_id), request_id, None, None, None, DocumentStatus.PROCESSING_ERROR, "unsupported_type_determination")
                raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶é¡å‹ ({document.file_type}) æˆ–ç„¡æ³•ç¢ºå®šä»»å‹™é¡å‹ã€‚")

        content_for_ai: Any
        processing_type_for_save = "unknown_triggered_analysis"

        try:
            if effective_task_type == AIServiceTaskType.IMAGE_ANALYSIS:
                processing_type_for_save = "image_analysis_triggered"
                # This part is similar to _process_image_document's content prep
                if not document.file_path or not os.path.exists(document.file_path):
                    raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="åœ–ç‰‡æ–‡ä»¶è·¯å¾‘ä¸å­˜åœ¨ã€‚")
                image_bytes = await doc_processor.get_image_bytes(document.file_path) # type: ignore
                if not image_bytes:
                    raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="ç„¡æ³•è®€å–åœ–ç‰‡æ–‡ä»¶ã€‚")
                content_for_ai = Image.open(io.BytesIO(image_bytes))
            elif effective_task_type == AIServiceTaskType.TEXT_GENERATION:
                processing_type_for_save = "text_analysis_triggered"
                # This part is similar to _process_text_document's content prep
                if not document.extracted_text:
                    if not document.file_path or not os.path.exists(document.file_path): # Check path before extraction
                         raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="æ–‡æœ¬æ–‡ä»¶è·¯å¾‘ä¸å­˜åœ¨ã€‚")
                    extracted_text_result, extraction_status, extraction_error = \
                        await doc_processor.extract_text_from_document(str(document.file_path), Path(str(document.file_path)).suffix if document.file_path else "") # type: ignore
                    if extraction_status == DocumentStatus.PROCESSING_ERROR or not extracted_text_result:
                        error_detail = extraction_error or "æœªèƒ½å¾æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆæ–‡æœ¬å…§å®¹ä»¥é€²è¡Œåˆ†æ"
                        await self._save_analysis_results(document, db, str(current_user_id), request_id, None, None, None, DocumentStatus.EXTRACTION_FAILED, processing_type_for_save)
                        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=error_detail)

                    # ğŸ¯ Phase 1: ç‚ºæå–çš„æ–‡æœ¬æ·»åŠ è¡Œè™Ÿæ¨™è¨˜
                    file_extension = Path(str(document.file_path)).suffix if document.file_path else ""
                    marked_text, line_mapping, batches = process_text_with_line_markers(
                        extracted_text_result, file_extension
                    )
                    logger.info(f"æ–‡æª” {document.id} è¡Œè™Ÿæ¨™è¨˜å®Œæˆ: {len(line_mapping)} è¡Œ" +
                               (f"ï¼Œåˆ†ç‚º {len(batches)} æ‰¹" if batches else ""))

                    document.extracted_text = extracted_text_result
                    await crud_documents.update_document_on_extraction_success(
                        db, document.id, extracted_text_result, line_mapping
                    )
                    # ğŸ¯ Phase 2: ä½¿ç”¨å¸¶è¡Œè™Ÿçš„æ–‡æœ¬é€²è¡Œ AI åˆ†æ
                    content_for_ai = marked_text
                else:
                    # å·²æœ‰æå–æ–‡æœ¬ï¼Œé‡æ–°ç”Ÿæˆå¸¶è¡Œè™Ÿæ–‡æœ¬ç”¨æ–¼ AI åˆ†æ
                    file_extension = Path(str(document.file_path)).suffix if document.file_path else ""
                    marked_text, _, _ = process_text_with_line_markers(
                        document.extracted_text, file_extension
                    )
                    content_for_ai = marked_text

                if not content_for_ai:
                    raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="æ–‡æª”ç„¡æå–æ–‡æœ¬å¯ä¾›åˆ†æã€‚")
                max_prompt_len = settings_obj.AI_MAX_INPUT_CHARS_TEXT_ANALYSIS
                if len(content_for_ai) > max_prompt_len:
                    logger.warning(f"å¸¶è¡Œè™Ÿæ–‡æœ¬é•·åº¦ ({len(content_for_ai)}) è¶…éæœ€å¤§å…è¨±é•·åº¦ ({max_prompt_len})ã€‚å°‡é€²è¡Œæˆªæ–·ã€‚ Doc ID: {document.id}")
                    content_for_ai = content_for_ai[:max_prompt_len]
            else: # Should have been caught by earlier task type check
                raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"ä¸æ”¯æŒçš„AIä»»å‹™é¡å‹é€²è¡Œå…§éƒ¨è§¸ç™¼: {effective_task_type}")

            ai_request = AIRequest(
                task_type=effective_task_type, content=content_for_ai, model_preference=ai_model_preference,
                require_language_consistency=ai_ensure_chinese_output if ai_ensure_chinese_output is not None else True,
                generation_params_override={"max_output_tokens": ai_max_output_tokens} if ai_max_output_tokens else None,
                prompt_params={"user_query": custom_prompt_id} if custom_prompt_id else None, user_id=str(current_user_id)
            )
            ai_response = await unified_ai_service_simplified.process_request(ai_request, db) # type: ignore

            analysis_data_to_save: Optional[dict] = None
            token_usage_to_save: Optional[TokenUsage] = None
            model_used: Optional[str] = None
            final_status: DocumentStatus

            if ai_response.success and ai_response.output_data:
                analysis_data_to_save = ai_response.output_data.model_dump()
                token_usage_to_save = ai_response.token_usage # type: ignore
                model_used = ai_response.model_used # type: ignore
                final_status = DocumentStatus.ANALYSIS_COMPLETED
                logger.info(f"æ–‡æª” {document.id} çš„ AI åˆ†ææˆåŠŸå®Œæˆ (triggered)ã€‚")
            else:
                final_status = DocumentStatus.ANALYSIS_FAILED
                logger.error(f"æ–‡æª” {document.id} çš„ AI åˆ†æå¤±æ•— (triggered): {ai_response.error_message}")
                # analysis_data_to_save can include error details if needed, currently handled by _save_analysis_results
                analysis_data_to_save = {"error": ai_response.error_message or "Unknown AI error during trigger"}

            await self._save_analysis_results(document, db, str(current_user_id), request_id, 
                                              analysis_data_to_save, token_usage_to_save, model_used, 
                                              final_status, processing_type_for_save)
            
            updated_document = await crud_documents.get_document_by_id(db, document.id)
            if not updated_document:
                # This should ideally not happen if the document existed and status was updated
                raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="åˆ†æå¾Œç„¡æ³•é‡æ–°ç²å–æ–‡æª”ã€‚")
            return updated_document

        except HTTPException as http_exc: # Re-raise HTTPExceptions directly
            # If status was already ANALYSIS_FAILED by _save_analysis_results, this is fine.
            # Otherwise, ensure a generic error status if one wasn't set.
            if document and document.status == DocumentStatus.ANALYZING: # If still analyzing, mark as error
                 await self._save_analysis_results(document, db, str(current_user_id), request_id, 
                                                  {"error": str(http_exc.detail)}, None, None, 
                                                  DocumentStatus.ANALYSIS_FAILED, processing_type_for_save)
            raise http_exc
        except Exception as e:
            logger.error(f"(Service Trigger) åˆ†ææ–‡æª” {document.id} æ™‚ç™¼ç”Ÿé ‚å±¤éŒ¯èª¤: {e}", exc_info=True)
            if document: # Ensure document exists before trying to save results
                 await self._save_analysis_results(document, db, str(current_user_id), request_id, 
                                                  {"error": str(e)}, None, None, 
                                                  DocumentStatus.PROCESSING_ERROR, processing_type_for_save)
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"è§¸ç™¼æ–‡ä»¶åˆ†ææ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {str(e)}")
    
    async def _check_trigger_clustering(self, db: AsyncIOMotorDatabase, owner_id: str) -> None:
        """
        æª¢æŸ¥æ˜¯å¦éœ€è¦è§¸ç™¼èšé¡ (ç´¯ç©>=20å€‹å¾…åˆ†é¡æ–‡æª”)
        
        Args:
            db: æ•¸æ“šåº«é€£æ¥
            owner_id: ç”¨æˆ¶ID (å­—ç¬¦ä¸²æ ¼å¼)
        """
        try:
            # è¨ˆç®—pendingç‹€æ…‹ä¸”æœ‰enriched_dataçš„æ–‡æª”æ•¸é‡
            pending_count = await db["documents"].count_documents({
                "owner_id": uuid.UUID(owner_id),
                "clustering_status": "pending",
                "enriched_data": {"$ne": None}
            })
            
            logger.debug(f"ç”¨æˆ¶ {owner_id} æœ‰ {pending_count} å€‹å¾…èšé¡æ–‡æª”")
            
            # å¦‚æœç´¯ç©>=20å€‹,è§¸ç™¼å¾Œå°èšé¡ä»»å‹™
            if pending_count >= 20:
                logger.info(f"ç”¨æˆ¶ {owner_id} é”åˆ°èšé¡é–¾å€¼ ({pending_count}å€‹æ–‡æª”),æº–å‚™è§¸ç™¼èšé¡")
                
                # æ³¨æ„: é€™è£¡ä¸ç›´æ¥åŸ·è¡Œèšé¡,è€Œæ˜¯è¨˜éŒ„æ—¥èªŒ
                # å¯¦éš›çš„èšé¡è§¸ç™¼æœƒåœ¨clustering_serviceå‰µå»ºå¾Œå¯¦ç¾
                # æˆ–é€šéå®šæœŸä»»å‹™åŸ·è¡Œ
                await log_event(
                    db=db,
                    level=LogLevel.INFO,
                    message=f"ç”¨æˆ¶ {owner_id} é”åˆ°èšé¡é–¾å€¼,å¾…èšé¡æ–‡æª”æ•¸: {pending_count}",
                    source="doc_tasks_service.check_trigger_clustering",
                    user_id=owner_id,
                    details={"pending_count": pending_count}
                )
                
                # TODO: ç•¶ ClusteringService å¯¦ç¾å¾Œ,åœ¨é€™è£¡è§¸ç™¼èšé¡
                # from app.services.external.clustering_service import ClusteringService
                # clustering_service = ClusteringService()
                # background_tasks.add_task(
                #     clustering_service.run_clustering_for_user,
                #     db, uuid.UUID(owner_id)
                # )
                
        except Exception as e:
            logger.error(f"æª¢æŸ¥èšé¡è§¸ç™¼æ¢ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤ (ç”¨æˆ¶ {owner_id}): {e}", exc_info=True)
            await log_event(
                db=db,
                level=LogLevel.ERROR,
                message=f"æª¢æŸ¥èšé¡è§¸ç™¼æ¢ä»¶å¤±æ•—: {str(e)}",
                source="doc_tasks_service.check_trigger_clustering.error",
                user_id=owner_id,
                details={"error": str(e)}
            )


