"""
çµ±ä¸€å°è©±ä¸Šä¸‹æ–‡ç®¡ç†å™¨

æ ¸å¿ƒè·è²¬:
1. æœƒè©±ç‹€æ…‹ç®¡ç†: è¿½è¹¤æ•´å€‹å°è©±çš„ç‹€æ…‹å’Œé€²åº¦
2. æ–‡æª”æ± ç®¡ç†: ç®¡ç†æœ¬æ¬¡å°è©±å¼•ç”¨çš„æ‰€æœ‰æ–‡æª”
3. ä¸Šä¸‹æ–‡è¼‰å…¥: çµ±ä¸€çš„è¼‰å…¥æ¥å£ï¼Œè‡ªå‹•é¸æ“‡åˆé©çš„æ ¼å¼
4. ä¸Šä¸‹æ–‡æ§‹å»º: ç‚ºä¸åŒéšæ®µæ§‹å»ºå°ˆé–€å„ªåŒ–çš„ä¸Šä¸‹æ–‡
5. ç·©å­˜ç®¡ç†: çµ±ä¸€çš„ Redis + MongoDB ç·©å­˜ç­–ç•¥

è¨­è¨ˆåŸå‰‡:
- å–®ä¸€è²¬ä»»: åªè² è²¬ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œä¸æ¶‰åŠæ¥­å‹™é‚è¼¯
- çµ±ä¸€æ¥å£: å°å¤–æä¾›çµ±ä¸€çš„ APIï¼Œéš±è—å…§éƒ¨è¤‡é›œæ€§
- æ ¼å¼é©é…: æ ¹æ“šä½¿ç”¨å ´æ™¯è‡ªå‹•æä¾›æœ€å„ªæ ¼å¼
- æœƒè©±æ„ŸçŸ¥: ç†è§£å°è©±çš„é€£çºŒæ€§å’Œæ–‡æª”å¼•ç”¨é—œä¿‚
"""

import logging
import re
from typing import Optional, List, Dict, Any, Tuple
from datetime import datetime, UTC
from uuid import UUID
from enum import Enum
from dataclasses import dataclass, field
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger
from app.models.context_config import context_config

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


class ContextPurpose(str, Enum):
    """ä¸Šä¸‹æ–‡ä½¿ç”¨ç›®çš„"""
    CLASSIFICATION = "classification"        # æ„åœ–åˆ†é¡
    ANSWER_GENERATION = "answer_generation"  # ç­”æ¡ˆç”Ÿæˆ
    SEARCH_RETRIEVAL = "search_retrieval"    # æ–‡æª”æª¢ç´¢
    CLARIFICATION = "clarification"          # æ¾„æ¸…å•é¡Œç”Ÿæˆ


@dataclass
class DocumentRef:
    """
    æ–‡æª”å¼•ç”¨ - æœƒè©±æ–‡æª”æ± ä¸­çš„æ–‡æª”è¨˜éŒ„
    
    æ³¨æ„: é€™å€‹çµæ§‹æœƒåºåˆ—åŒ–åˆ° MongoDB çš„ cached_document_data å­—æ®µä¸­
    åªä¿å­˜æ‘˜è¦ç´šåˆ¥çš„ä¿¡æ¯ï¼Œä¸ä¿å­˜å®Œæ•´æ–‡æª”å…§å®¹
    """
    document_id: str
    filename: str
    
    # æ‘˜è¦ä¿¡æ¯ (å±¤æ¬¡1 - ç´„500 tokens)
    summary: Optional[str] = None           # 100-200å­—æ‘˜è¦
    key_concepts: List[str] = field(default_factory=list)  # é—œéµæ¦‚å¿µ
    semantic_tags: List[str] = field(default_factory=list)  # èªç¾©æ¨™ç±¤
    
    # æœƒè©±ç´šå…ƒæ•¸æ“š
    first_mentioned_round: int = 1          # é¦–æ¬¡æåŠçš„è¼ªæ¬¡
    last_accessed_round: int = 1            # æœ€å¾Œè¨ªå•çš„è¼ªæ¬¡
    relevance_score: float = 0.8            # ç›¸é—œæ€§è©•åˆ† (0-1)
    access_count: int = 1                   # è¨ªå•æ¬¡æ•¸
    topic: Optional[str] = None             # ä¸»é¡Œæ¨™ç±¤
    
    def decay_relevance(self, current_round: int, decay_rate: float = 0.1):
        """éš¨æ™‚é–“è¡°æ¸›ç›¸é—œæ€§"""
        rounds_passed = current_round - self.last_accessed_round
        self.relevance_score = max(0.3, self.relevance_score - (rounds_passed * decay_rate))
    
    def boost_relevance(self, boost: float = 0.1):
        """æå‡ç›¸é—œæ€§ï¼ˆç•¶å†æ¬¡è¢«è¨ªå•æ™‚ï¼‰"""
        self.relevance_score = min(1.0, self.relevance_score + boost)
        self.access_count += 1
    
    def boost_citation(self, citation_boost: float = 0.2):
        """æå‡ç›¸é—œæ€§ï¼ˆç•¶ AI ç”Ÿæˆç­”æ¡ˆæ™‚å¼•ç”¨äº†æ­¤æ–‡æª”ï¼‰"""
        self.relevance_score = min(1.0, self.relevance_score + citation_boost)
        logger.info(f"ğŸ“Œ æ–‡æª”è¢«å¼•ç”¨ï¼Œç›¸é—œæ€§æå‡: {self.filename} -> {self.relevance_score:.2f}")
    
    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼Œç”¨æ–¼åºåˆ—åŒ–åˆ° MongoDB"""
        return {
            "document_id": self.document_id,
            "filename": self.filename,
            "summary": self.summary,
            "key_concepts": self.key_concepts,
            "semantic_tags": self.semantic_tags,
            "first_mentioned_round": self.first_mentioned_round,
            "last_accessed_round": self.last_accessed_round,
            "relevance_score": self.relevance_score,
            "access_count": self.access_count,
            "topic": self.topic
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> 'DocumentRef':
        """å¾å­—å…¸å‰µå»º DocumentRef å¯¦ä¾‹"""
        return cls(
            document_id=data.get("document_id", ""),
            filename=data.get("filename", ""),
            summary=data.get("summary"),
            key_concepts=data.get("key_concepts", []),
            semantic_tags=data.get("semantic_tags", []),
            first_mentioned_round=data.get("first_mentioned_round", 1),
            last_accessed_round=data.get("last_accessed_round", 1),
            relevance_score=data.get("relevance_score", 0.8),
            access_count=data.get("access_count", 1),
            topic=data.get("topic")
        )


@dataclass
class Message:
    """æ¨™æº–åŒ–çš„æ¶ˆæ¯çµæ§‹"""
    role: str  # "user" | "assistant"
    content: str
    round_number: int
    created_at: datetime
    tokens_used: Optional[int] = None
    source_documents: Optional[List[str]] = None  # æœ¬è¼ªå›ç­”å¼•ç”¨çš„æ–‡æª”
    
    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return {
            "role": self.role,
            "content": self.content,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }
    
    def to_formatted_text(self, max_length: Optional[int] = None) -> str:
        """è½‰æ›ç‚ºæ ¼å¼åŒ–æ–‡æœ¬"""
        role_name = "ç”¨æˆ¶" if self.role == "user" else "åŠ©æ‰‹"
        content = self.content
        if max_length and len(content) > max_length:
            content = content[:max_length] + "..."
        return f"{role_name}: {content}"


@dataclass
class ContextBundle:
    """
    ä¸Šä¸‹æ–‡åŒ… - æ ¹æ“šä¸åŒç›®çš„æ‰“åŒ…çš„ä¸Šä¸‹æ–‡æ•¸æ“š
    
    é€™æ˜¯çµ±ä¸€æ¥å£è¿”å›çš„æ¨™æº–æ ¼å¼ï¼Œä¸åŒç›®çš„æœƒå¡«å……ä¸åŒçš„å­—æ®µ
    """
    purpose: ContextPurpose
    
    # å°è©±æ­·å² (ä¸åŒæ ¼å¼)
    conversation_history_list: Optional[List[Dict]] = None      # åˆ—è¡¨æ ¼å¼
    conversation_history_text: Optional[str] = None             # æ–‡æœ¬æ ¼å¼
    
    # æ–‡æª”æ± ä¿¡æ¯
    document_pool: Optional[Dict[str, DocumentRef]] = None      # å®Œæ•´æ–‡æª”æ± 
    cached_documents_info: Optional[List[Dict]] = None          # æ–‡æª”æ‘˜è¦åˆ—è¡¨
    priority_document_ids: Optional[List[str]] = None           # å„ªå…ˆæª¢ç´¢æ–‡æª”ID
    
    # æœƒè©±ç‹€æ…‹
    current_round: Optional[int] = None
    message_count: Optional[int] = None
    session_state: Optional[Dict] = None
    
    # æª¢ç´¢å»ºè­°
    should_reuse_cached: bool = False
    search_expansion_needed: bool = True


class ConversationContextManager:
    """
    çµ±ä¸€å°è©±ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    
    ç¤ºä¾‹ç”¨æ³•:
    
    # å‰µå»ºç®¡ç†å™¨
    manager = ConversationContextManager(
        db=db,
        conversation_id="uuid",
        user_id="uuid"
    )
    
    # ç‚ºæ„åœ–åˆ†é¡è¼‰å…¥ä¸Šä¸‹æ–‡
    context = await manager.load_context(
        purpose=ContextPurpose.CLASSIFICATION
    )
    # è¿”å›: conversation_history_list + cached_documents_info
    
    # ç‚ºç­”æ¡ˆç”Ÿæˆè¼‰å…¥ä¸Šä¸‹æ–‡
    context = await manager.load_context(
        purpose=ContextPurpose.ANSWER_GENERATION,
        current_documents=retrieved_documents
    )
    # è¿”å›: æ ¼å¼åŒ–æ–‡æœ¬ï¼Œæ˜ç¢ºåˆ†é›¢æ­·å²å’Œç•¶å‰æ–‡æª”
    
    # ä¿å­˜å•ç­”å°
    await manager.add_qa_pair(
        question="æ—©é¤æ”¶æ“š",
        answer="æ ¹æ“šæ–‡æª”...",
        source_documents=["doc_id_1"]
    )
    """
    
    def __init__(
        self,
        db: AsyncIOMotorDatabase,
        conversation_id: str,
        user_id: str,
        enable_caching: bool = True
    ):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            db: MongoDB é€£æ¥
            conversation_id: å°è©±ID
            user_id: ç”¨æˆ¶ID
            enable_caching: æ˜¯å¦å•Ÿç”¨ Redis ç·©å­˜
        """
        self.db = db
        self.conversation_id = conversation_id
        self.user_id = user_id
        self.enable_caching = enable_caching
        
        # æœƒè©±ç‹€æ…‹
        self.conversation_uuid = UUID(conversation_id)
        self.user_uuid = UUID(user_id)
        self.current_round = 0
        
        # å…§å­˜ç·©å­˜
        self._message_cache: Optional[List[Message]] = None
        self._document_pool: Optional[Dict[str, DocumentRef]] = None
        self._cache_loaded = False
        
        logger.debug(f"å‰µå»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨: conversation={conversation_id}, user={user_id}")
    
    async def load_context(
        self,
        purpose: ContextPurpose,
        current_documents: Optional[List[Any]] = None,
        max_history_messages: int = 10
    ) -> ContextBundle:
        """
        çµ±ä¸€çš„ä¸Šä¸‹æ–‡è¼‰å…¥æ¥å£
        
        æ ¹æ“šä¸åŒçš„ä½¿ç”¨ç›®çš„ï¼Œè‡ªå‹•æ§‹å»ºæœ€å„ªçš„ä¸Šä¸‹æ–‡æ ¼å¼
        
        Args:
            purpose: ä¸Šä¸‹æ–‡ä½¿ç”¨ç›®çš„
            current_documents: ç•¶å‰æª¢ç´¢åˆ°çš„æ–‡æª” (åƒ… ANSWER_GENERATION éœ€è¦)
            max_history_messages: æœ€å¤§æ­·å²æ¶ˆæ¯æ•¸
            
        Returns:
            ContextBundle: ä¸Šä¸‹æ–‡åŒ…
        """
        # ç¢ºä¿ç·©å­˜å·²è¼‰å…¥
        await self._ensure_cache_loaded()
        
        bundle = ContextBundle(purpose=purpose)
        bundle.current_round = self.current_round
        bundle.message_count = len(self._message_cache) if self._message_cache else 0
        
        if purpose == ContextPurpose.CLASSIFICATION:
            return await self._build_classification_context(bundle, max_history_messages)
        
        elif purpose == ContextPurpose.ANSWER_GENERATION:
            return await self._build_answer_generation_context(
                bundle, current_documents, max_history_messages
            )
        
        elif purpose == ContextPurpose.SEARCH_RETRIEVAL:
            return await self._build_search_context(bundle)
        
        elif purpose == ContextPurpose.CLARIFICATION:
            return await self._build_clarification_context(bundle, max_history_messages)
        
        return bundle
    
    async def add_qa_pair(
        self,
        question: str,
        answer: str,
        source_documents: Optional[List[str]] = None,
        tokens_used: int = 0
    ) -> bool:
        """
        ä¿å­˜å•ç­”å°åˆ°å°è©±ï¼Œä¸¦æ›´æ–°æ–‡æª”æ± 
        
        Args:
            question: ç”¨æˆ¶å•é¡Œ
            answer: AIå›ç­”
            source_documents: å¼•ç”¨çš„æ–‡æª”IDåˆ—è¡¨
            tokens_used: ä½¿ç”¨çš„tokenæ•¸
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            from app.crud import crud_conversations
            
            self.current_round += 1
            
            # æ·»åŠ ç”¨æˆ¶å•é¡Œ
            await crud_conversations.add_message_to_conversation(
                db=self.db,
                conversation_id=self.conversation_uuid,
                user_id=self.user_uuid,
                role="user",
                content=question,
                tokens_used=None
            )
            
            # æ·»åŠ AIå›ç­”
            await crud_conversations.add_message_to_conversation(
                db=self.db,
                conversation_id=self.conversation_uuid,
                user_id=self.user_uuid,
                role="assistant",
                content=answer,
                tokens_used=tokens_used
            )
            
            # âš ï¸ å…ˆç¢ºä¿æ–‡æª”æ± å·²è¼‰å…¥
            await self._ensure_cache_loaded()
            
            # ğŸ”„ å°æ‰€æœ‰æœªè¢«å¼•ç”¨çš„æ–‡æª”é€²è¡Œç›¸é—œæ€§è¡°æ¸›
            if self._document_pool:
                source_doc_set = set(source_documents or [])
                decay_rate = context_config.RELEVANCE_DECAY_RATE
                
                for doc_id, doc_ref in self._document_pool.items():
                    if doc_id not in source_doc_set:
                        # æœªè¢«å¼•ç”¨çš„æ–‡æª”ï¼Œé€²è¡Œè¡°æ¸›
                        old_score = doc_ref.relevance_score
                        doc_ref.decay_relevance(self.current_round, decay_rate)
                        if old_score != doc_ref.relevance_score:
                            logger.debug(
                                f"ğŸ“‰ æ–‡æª”ç›¸é—œæ€§è¡°æ¸›: {doc_ref.filename} "
                                f"{old_score:.2f} â†’ {doc_ref.relevance_score:.2f}"
                            )
            
            # æ›´æ–°æ–‡æª”æ± ï¼ˆæ·»åŠ æ–°æ–‡æª”ï¼‰
            if source_documents and len(source_documents) > 0:
                logger.debug(f"æ›´æ–°å‰æ–‡æª”æ± : {len(self._document_pool)} å€‹æ–‡æª”")
                await self._update_document_pool(source_documents)
                logger.debug(f"æ›´æ–°å¾Œæ–‡æª”æ± : {len(self._document_pool)} å€‹æ–‡æª”")
                
                # ğŸ·ï¸ å¼·åˆ¶æ·»åŠ å¼•ç”¨æ¨™è¨»ï¼ˆå¦‚æœ AI æ²’æœ‰æ¨™è¨»ï¼‰
                # å‚³å…¥ source_documents ä»¥ä¿æŒèˆ‡ AI çœ‹åˆ°çš„é †åºä¸€è‡´
                answer = self.enforce_citations(answer, source_document_ids=source_documents)
                
                # ğŸ“ˆ æª¢æ¸¬ç­”æ¡ˆä¸­çš„å¼•ç”¨ä¸¦çµ¦ç›¸æ‡‰æ–‡æª”åŠ åˆ†ï¼ˆä½¿ç”¨æ–‡æª”ååŒ¹é…ï¼‰
                await self.boost_cited_documents(answer)
            
            # ğŸ—‘ï¸ æ¸…ç†ä½ç›¸é—œæ€§æ–‡æª”
            await self.cleanup_low_relevance_docs()
            
            # ä¿å­˜æ–‡æª”æ± åˆ° cached_document_data
            if self._document_pool:
                await self._save_document_pool_to_db()
                
                # æ›´æ–° cached_documents (æ–‡æª”IDåˆ—è¡¨)
                all_doc_ids = list(self._document_pool.keys())
                await crud_conversations.update_cached_documents(
                    db=self.db,
                    conversation_id=self.conversation_uuid,
                    user_id=self.user_uuid,
                    document_ids=all_doc_ids,
                    document_data=None  # cached_document_data å·²åœ¨ä¸Šé¢ä¿å­˜
                )
            
            # âš ï¸ ä¸è¦æ¸…é™¤ç·©å­˜ï¼é€™æ¨£ä¸‹æ¬¡ä¿å­˜æ™‚æ‰èƒ½ä¿ç•™æ­·å²æ–‡æª”
            # await self._invalidate_cache()  # ç§»é™¤é€™è¡Œï¼
            
            logger.info(f"âœ… ä¿å­˜å•ç­”å°æˆåŠŸ: round={self.current_round}, docs={len(source_documents or [])}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å•ç­”å°å¤±æ•—: {e}", exc_info=True)
            return False
    
    async def cleanup_low_relevance_docs(
        self,
        min_score: float = None,
        max_idle_rounds: int = None
    ):
        """
        æ¸…ç†ä½ç›¸é—œæ€§æ–‡æª”
        
        Args:
            min_score: æœ€ä½åˆ†æ•¸é–¾å€¼ï¼ˆä½æ–¼æ­¤åˆ†æ•¸çš„æ–‡æª”æœƒè¢«æ¸…ç†ï¼‰
            max_idle_rounds: æœ€å¤§é–’ç½®è¼ªæ¬¡ï¼ˆè¶…éæ­¤è¼ªæ¬¡æœªè¨ªå•çš„ä½åˆ†æ–‡æª”æœƒè¢«æ¸…ç†ï¼‰
        """
        # ä½¿ç”¨çµ±ä¸€é…ç½®çš„é»˜èªå€¼
        if min_score is None:
            min_score = context_config.MIN_RELEVANCE_SCORE
        if max_idle_rounds is None:
            max_idle_rounds = context_config.MAX_IDLE_ROUNDS
        
        if not self._document_pool:
            return
        
        to_remove = []
        current_round = self.current_round
        
        for doc_id, doc_ref in self._document_pool.items():
            idle_rounds = current_round - doc_ref.last_accessed_round
            
            # æ¸…ç†æ¢ä»¶ï¼šåˆ†æ•¸ä½æ–¼é–¾å€¼ ä¸” é•·æœŸæœªè¨ªå•
            if doc_ref.relevance_score <= min_score and idle_rounds >= max_idle_rounds:
                to_remove.append((doc_id, doc_ref))
                logger.info(
                    f"ğŸ—‘ï¸ æ¨™è¨˜æ¸…ç†æ–‡æª”: {doc_ref.filename} "
                    f"(score: {doc_ref.relevance_score:.2f}, idle: {idle_rounds} è¼ª)"
                )
        
        # åŸ·è¡Œæ¸…ç†
        if to_remove:
            for doc_id, doc_ref in to_remove:
                del self._document_pool[doc_id]
            
            logger.info(f"âœ… å·²æ¸…ç† {len(to_remove)} å€‹ä½ç›¸é—œæ€§æ–‡æª”")
            
            # ä¿å­˜æ›´æ–°å¾Œçš„æ–‡æª”æ± 
            await self._save_document_pool_to_db()
        else:
            logger.debug("âœ“ ç„¡éœ€æ¸…ç†æ–‡æª”")
    
    async def get_retrieval_priority_docs(
        self,
        top_k: int = 5,
        min_relevance: float = 0.5
    ) -> List[str]:
        """
        ç²å–æª¢ç´¢å„ªå…ˆæ–‡æª”IDåˆ—è¡¨
        
        æ ¹æ“šç›¸é—œæ€§è©•åˆ†è¿”å›æœ€ç›¸é—œçš„æ–‡æª”
        
        Args:
            top_k: è¿”å›å‰Kå€‹æ–‡æª”
            min_relevance: æœ€ä½ç›¸é—œæ€§é–¾å€¼
            
        Returns:
            List[str]: æ–‡æª”IDåˆ—è¡¨ï¼ŒæŒ‰ç›¸é—œæ€§æ’åº
        """
        await self._ensure_cache_loaded()
        
        if not self._document_pool:
            return []
        
        # å…ˆå°æ‰€æœ‰æ–‡æª”é€²è¡Œç›¸é—œæ€§è¡°æ¸›
        for doc_ref in self._document_pool.values():
            doc_ref.decay_relevance(self.current_round)
        
        # è‡ªå‹•æ¸…ç†ä½ç›¸é—œæ€§æ–‡æª”ï¼ˆæ¯æ¬¡æª¢ç´¢æ™‚è§¸ç™¼ï¼Œä½¿ç”¨çµ±ä¸€é…ç½®ï¼‰
        await self.cleanup_low_relevance_docs()
        
        # æŒ‰ç›¸é—œæ€§æ’åº
        sorted_docs = sorted(
            self._document_pool.values(),
            key=lambda x: (x.relevance_score, x.access_count),
            reverse=True
        )
        
        # éæ¿¾ä¸¦è¿”å›é«˜ç›¸é—œæ€§æ–‡æª”
        priority_docs = [
            doc.document_id
            for doc in sorted_docs
            if doc.relevance_score >= min_relevance
        ][:top_k]
        
        logger.info(f"ğŸ¯ å„ªå…ˆæª¢ç´¢æ–‡æª”: {len(priority_docs)} å€‹ (é–¾å€¼: {min_relevance})")
        return priority_docs
    
    async def should_reuse_cached_docs(
        self,
        query: str,
        similarity_threshold: float = 0.7
    ) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦æ‡‰è©²é‡ç”¨ç·©å­˜æ–‡æª”
        
        åŸºæ–¼æŸ¥è©¢ç›¸ä¼¼åº¦å’Œæ–‡æª”æ± ç‹€æ…‹åˆ¤æ–·
        
        Args:
            query: ç•¶å‰æŸ¥è©¢
            similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            bool: æ˜¯å¦æ‡‰è©²é‡ç”¨ç·©å­˜
        """
        await self._ensure_cache_loaded()
        
        # å¦‚æœæ–‡æª”æ± ç‚ºç©ºï¼Œä¸é‡ç”¨
        if not self._document_pool:
            return False
        
        # å¦‚æœæœ€è¿‘ä¸€è¼ªæœ‰æ–‡æª”å¼•ç”¨ï¼Œå‚¾å‘æ–¼é‡ç”¨
        if self._message_cache and len(self._message_cache) > 0:
            last_msg = self._message_cache[-1]
            if last_msg.role == "assistant" and last_msg.source_documents:
                logger.info("ğŸ“ æœ€è¿‘ä¸€è¼ªæœ‰æ–‡æª”å¼•ç”¨ï¼Œå»ºè­°é‡ç”¨ç·©å­˜")
                return True
        
        # TODO: å¯¦ç¾åŸºæ–¼èªç¾©ç›¸ä¼¼åº¦çš„åˆ¤æ–·
        # é€™è£¡å¯ä»¥èª¿ç”¨ embedding æœå‹™æ¯”è¼ƒæŸ¥è©¢èˆ‡æ–‡æª”ä¸»é¡Œçš„ç›¸ä¼¼åº¦
        
        return False
    
    # ========== ç§æœ‰æ–¹æ³• ==========
    
    async def _ensure_cache_loaded(self):
        """ç¢ºä¿ç·©å­˜å·²è¼‰å…¥"""
        if self._cache_loaded:
            return
        
        await self._load_messages()
        await self._load_document_pool()
        self._cache_loaded = True
    
    async def _load_messages(self):
        """å¾æ•¸æ“šåº«è¼‰å…¥æ¶ˆæ¯"""
        try:
            from app.crud import crud_conversations
            
            messages = await crud_conversations.get_recent_messages(
                db=self.db,
                conversation_id=self.conversation_uuid,
                user_id=self.user_uuid,
                limit=50  # è¼‰å…¥æ›´å¤šæ¶ˆæ¯ç”¨æ–¼åˆ†æ
            )
            
            if messages:
                self._message_cache = [
                    Message(
                        role=msg.role,
                        content=msg.content,
                        round_number=(i + 1) // 2 + 1,  # æ¯å…©æ¢æ¶ˆæ¯ç‚ºä¸€è¼ª
                        created_at=msg.timestamp,  # ConversationMessage ä½¿ç”¨ timestampï¼Œä¸æ˜¯ created_at
                        tokens_used=getattr(msg, 'tokens_used', None),
                        source_documents=getattr(msg, 'source_documents', None)
                    )
                    for i, msg in enumerate(messages)
                ]
                self.current_round = max(m.round_number for m in self._message_cache)
                logger.debug(f"è¼‰å…¥äº† {len(self._message_cache)} æ¢æ¶ˆæ¯ï¼Œç•¶å‰è¼ªæ¬¡: {self.current_round}")
            else:
                self._message_cache = []
                self.current_round = 0
                
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¶ˆæ¯å¤±æ•—: {e}", exc_info=True)
            self._message_cache = []
    
    def enforce_citations(self, answer_text: str, source_document_ids: Optional[List[str]] = None) -> str:
        """
        å¼·åˆ¶æ·»åŠ å¼•ç”¨æ¨™è¨»ï¼ˆå¦‚æœ AI æ²’æœ‰æ¨™è¨»ï¼‰
        
        æ”¹é€²ç‰ˆï¼šä½¿ç”¨æ–‡æª” ID ä½œç‚ºå¼•ç”¨æ¨™è­˜ï¼Œé¿å…é †åºä¸ä¸€è‡´å•é¡Œ
        
        Args:
            answer_text: AI ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬
            source_document_ids: æœ¬è¼ªä½¿ç”¨çš„æ–‡æª” ID åˆ—è¡¨ï¼ˆæŒ‰é †åºï¼Œèˆ‡ AI çœ‹åˆ°çš„é †åºä¸€è‡´ï¼‰
            
        Returns:
            str: æ·»åŠ å¼•ç”¨æ¨™è¨»å¾Œçš„ç­”æ¡ˆæ–‡æœ¬
        """
        if not answer_text or not self._document_pool:
            logger.debug(f"è·³éå¼•ç”¨å¼·åˆ¶ï¼šanswer_text={bool(answer_text)}, pool={bool(self._document_pool)}")
            return answer_text
        
        logger.info(f"ğŸ“ é–‹å§‹å¼·åˆ¶å¼•ç”¨æ¨™è¨»ï¼Œæ–‡æª”æ± å¤§å°: {len(self._document_pool)}")
        logger.debug(f"ç­”æ¡ˆå…§å®¹ï¼ˆå‰ 200 å­—ç¬¦ï¼‰: {answer_text[:200]}")
        
        # å¦‚æœæä¾›äº† source_document_idsï¼Œä½¿ç”¨å®ƒä¾†å»ºç«‹ç·¨è™Ÿæ˜ å°„
        # å¦å‰‡ä½¿ç”¨æ–‡æª”æ± ä¸­çš„é †åºï¼ˆæŒ‰ç›¸é—œæ€§æ’åºï¼‰
        if source_document_ids:
            # ä½¿ç”¨èˆ‡ AI çœ‹åˆ°çš„ç›¸åŒé †åº
            filename_to_citation: Dict[str, int] = {}
            for idx, doc_id in enumerate(source_document_ids, 1):
                if doc_id in self._document_pool:
                    doc = self._document_pool[doc_id]
                    filename_to_citation[doc.filename] = idx
                    logger.debug(f"  æ–‡æª” {idx}: {doc.filename} (from source_documents)")
        else:
            # Fallback: æŒ‰ç›¸é—œæ€§æ’åº
            sorted_docs = sorted(
                self._document_pool.values(),
                key=lambda x: x.relevance_score,
                reverse=True
            )
            filename_to_citation = {}
            for idx, doc in enumerate(sorted_docs, 1):
                filename_to_citation[doc.filename] = idx
                logger.debug(f"  æ–‡æª” {idx}: {doc.filename} (score: {doc.relevance_score:.2f})")
        
        modified_text = answer_text
        added_count = 0
        
        # æŒ‰æ–‡ä»¶åé•·åº¦é™åºæ’åºï¼ˆå„ªå…ˆåŒ¹é…é•·æ–‡ä»¶åï¼Œé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
        sorted_filenames = sorted(filename_to_citation.keys(), key=len, reverse=True)
        
        for filename in sorted_filenames:
            citation_num = filename_to_citation[filename]
            
            # è·³éå·²ç¶“æœ‰ä»»ä½•å¼•ç”¨æ ¼å¼çš„ï¼ˆä¸é™å®šç·¨è™Ÿï¼‰
            already_cited_pattern = rf'\[([^\]]*{re.escape(filename)}[^\]]*)\]\(citation:\d+\)'
            if re.search(already_cited_pattern, modified_text):
                logger.debug(f"æ–‡æª” {filename} å·²æœ‰å¼•ç”¨æ¨™è¨»ï¼Œè·³é")
                continue
            
            # æŸ¥æ‰¾æœªæ¨™è¨»çš„æ–‡æª”åæåŠ
            # åŒ¹é…ï¼šæ–‡æª”åï¼ˆä½†ä¸åœ¨ Markdown éˆæ¥ä¸­ï¼‰
            # ä½¿ç”¨è² å‘å‰ç»å’Œè² å‘å¾Œé¡§é¿å…åŒ¹é…å·²ç¶“åœ¨éˆæ¥ä¸­çš„æ–‡æª”å
            pattern = rf'(?<!\]\()(?<!\[)({re.escape(filename)})(?!\]\(citation:)'
            
            def replace_with_citation(match):
                nonlocal added_count
                text = match.group(1)
                added_count += 1
                logger.info(f"  âœ… ç‚º '{text}' æ·»åŠ å¼•ç”¨ citation:{citation_num}")
                return f"[{text}](citation:{citation_num})"
            
            # åªæ›¿æ›ç¬¬ä¸€æ¬¡å‡ºç¾ï¼ˆé¿å…éåº¦æ¨™è¨»ï¼‰
            modified_text = re.sub(pattern, replace_with_citation, modified_text, count=1)
        
        if added_count > 0:
            logger.info(f"ğŸ”— è‡ªå‹•æ·»åŠ äº† {added_count} å€‹å¼•ç”¨æ¨™è¨»")
        else:
            logger.warning("âš ï¸ æœªæ·»åŠ ä»»ä½•å¼•ç”¨æ¨™è¨»ï¼ˆå¯èƒ½ AI å·²ç¶“æ¨™è¨»äº†ï¼Œæˆ–æ–‡æª”åæœªå‡ºç¾åœ¨ç­”æ¡ˆä¸­ï¼‰")
        
        return modified_text
    
    async def boost_cited_documents(self, answer_text: str):
        """
        æª¢æ¸¬ç­”æ¡ˆä¸­çš„å¼•ç”¨ä¸¦çµ¦ç›¸æ‡‰æ–‡æª”åŠ åˆ†
        
        æ”¹é€²ç‰ˆï¼šé€šéæ–‡æª”ååŒ¹é…è€Œä¸æ˜¯ç·¨è™ŸåŒ¹é…ï¼Œé¿å…é †åºä¸ä¸€è‡´å•é¡Œ
        
        Args:
            answer_text: AI ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬
        """
        if not answer_text or not self._document_pool:
            logger.debug(f"è·³éå¼•ç”¨åŠ åˆ†ï¼šanswer_text={bool(answer_text)}, pool={bool(self._document_pool)}")
            return
        
        logger.info(f"ğŸ” é–‹å§‹æª¢æ¸¬å¼•ç”¨ï¼Œæ–‡æª”æ± å¤§å°: {len(self._document_pool)}")
        logger.debug(f"ç­”æ¡ˆå…§å®¹ï¼ˆå‰ 200 å­—ç¬¦ï¼‰: {answer_text[:200]}")
        
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é… [æ–‡æœ¬](citation:æ•¸å­—) æ ¼å¼
        # æå–å¼•ç”¨ä¸­çš„æ–‡æœ¬ï¼ˆé€šå¸¸åŒ…å«æ–‡æª”åï¼‰
        citation_pattern = r'\[([^\]]+)\]\(citation:(\d+)\)'
        matches = re.findall(citation_pattern, answer_text)
        
        if not matches:
            logger.warning("âš ï¸ ç­”æ¡ˆä¸­æœªç™¼ç¾ä»»ä½•å¼•ç”¨æ¨™è¨» [xxx](citation:N)")
            return
        
        # æå–å¼•ç”¨ä¸­çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ–‡æª”åæˆ–åŒ…å«æ–‡æª”åï¼‰
        cited_texts = [match[0] for match in matches]
        logger.info(f"ğŸ“Œ æª¢æ¸¬åˆ° {len(cited_texts)} å€‹å¼•ç”¨æ–‡æœ¬: {cited_texts}")
        
        # é€šéæ–‡æª”ååŒ¹é…ï¼ˆè€Œä¸æ˜¯ç·¨è™Ÿï¼‰
        boosted_count = 0
        boosted_docs = set()  # é¿å…é‡è¤‡åŠ åˆ†
        
        for cited_text in cited_texts:
            # åœ¨æ–‡æª”æ± ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡æª”
            for doc_id, doc in self._document_pool.items():
                if doc_id in boosted_docs:
                    continue
                    
                # æª¢æŸ¥æ–‡æª”åæ˜¯å¦å‡ºç¾åœ¨å¼•ç”¨æ–‡æœ¬ä¸­
                if doc.filename in cited_text or cited_text in doc.filename:
                    doc.boost_citation(citation_boost=0.2)
                    boosted_docs.add(doc_id)
                    boosted_count += 1
                    logger.info(f"  âœ… æ–‡æª” '{doc.filename}' è¢«å¼•ç”¨ï¼Œç›¸é—œæ€§æå‡")
                    break
        
        if boosted_count > 0:
            logger.info(f"âœ… å·²çµ¦ {boosted_count} å€‹è¢«å¼•ç”¨æ–‡æª”åŠ åˆ†")
            # ä¿å­˜æ›´æ–°å¾Œçš„æ–‡æª”æ± 
            await self._save_document_pool_to_db()
        else:
            logger.warning("âš ï¸ æœªèƒ½åŒ¹é…ä»»ä½•æ–‡æª”ï¼ˆå¼•ç”¨æ–‡æœ¬å¯èƒ½ä¸åŒ…å«æ–‡æª”åï¼‰")
    
    async def _load_document_pool(self):
        """
        å¾æ•¸æ“šåº«è¼‰å…¥æ–‡æª”æ± 
        
        å„ªå…ˆå¾ cached_document_data è¼‰å…¥ï¼ˆå·²åŒ…å«æ‘˜è¦ï¼‰ï¼Œ
        å¦‚æœä¸å­˜åœ¨å‰‡å¾æ–‡æª”åº«æŸ¥è©¢ä¸¦æ§‹å»º
        """
        try:
            from app.crud import crud_conversations
            from app.crud.crud_documents import get_documents_by_ids
            
            # å…ˆå˜—è©¦å¾ cached_document_data è¼‰å…¥
            cached_doc_ids, cached_doc_data = await crud_conversations.get_cached_documents(
                db=self.db,
                conversation_id=self.conversation_uuid,
                user_id=self.user_uuid
            )
            
            if not cached_doc_ids:
                self._document_pool = {}
                return
            
            self._document_pool = {}
            
            # å¦‚æœæœ‰ cached_document_dataï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦ä¿®å¾©
            if cached_doc_data and isinstance(cached_doc_data, dict):
                logger.debug(f"å¾æ•¸æ“šåº«è®€å–åˆ° cached_document_data: {len(cached_doc_data)} å€‹æ–‡æª”")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ "unknown" æ–‡ä»¶åï¼ˆèˆŠæ•¸æ“šï¼‰
                has_unknown = any(
                    doc_data.get('filename') == 'unknown' 
                    for doc_data in cached_doc_data.values() 
                    if isinstance(doc_data, dict)
                )
                
                if has_unknown:
                    # æœ‰èˆŠæ•¸æ“šï¼Œéœ€è¦é‡æ–°æŸ¥è©¢ä¿®å¾©
                    logger.info(f"æª¢æ¸¬åˆ°èˆŠçš„æ–‡æª”æ± æ•¸æ“šï¼ˆåŒ…å« unknownï¼‰ï¼Œé‡æ–°æŸ¥è©¢ä¿®å¾©...")
                else:
                    # æ•¸æ“šå®Œæ•´ï¼Œç›´æ¥ä½¿ç”¨
                    logger.debug(f"é–‹å§‹å¾ cached_document_data è¼‰å…¥æ–‡æª”æ± ï¼Œç¸½å…± {len(cached_doc_data)} å€‹æ–‡æª”")
                    for doc_id, doc_data in cached_doc_data.items():
                        if isinstance(doc_data, dict):
                            try:
                                self._document_pool[doc_id] = DocumentRef.from_dict(doc_data)
                            except Exception as e:
                                logger.warning(f"âš ï¸ è¼‰å…¥æ–‡æª” {doc_id} å¤±æ•—: {e}")
                        else:
                            logger.warning(f"âš ï¸ æ–‡æª” {doc_id} çš„æ•¸æ“šæ ¼å¼éŒ¯èª¤: {type(doc_data)}")
                    
                    logger.debug(f"å¾ cached_document_data è¼‰å…¥äº†æ–‡æª”æ± : {len(self._document_pool)} å€‹æ–‡æª”ï¼ˆåŸå§‹æ•¸æ“š {len(cached_doc_data)} å€‹ï¼‰")
                    return
            
            # å¦‚æœæ²’æœ‰ cached_document_dataï¼Œå¾æ–‡æª”åº«æŸ¥è©¢ä¸¦æ§‹å»º
            logger.info("cached_document_data ç‚ºç©ºï¼Œå¾æ–‡æª”åº«æ§‹å»ºæ–‡æª”æ± ")
            documents = await get_documents_by_ids(self.db, cached_doc_ids)
            
            for idx, doc in enumerate(documents, 1):
                doc_id = str(doc.id)
                
                # æå–æ‘˜è¦å’Œé—œéµä¿¡æ¯
                summary = ""
                key_concepts = []
                semantic_tags = []
                
                try:
                    # å¾ enriched_data ç²å–
                    if hasattr(doc, 'enriched_data') and isinstance(doc.enriched_data, dict):
                        summary = doc.enriched_data.get('summary', '')
                        key_concepts = doc.enriched_data.get('key_concepts', [])
                        semantic_tags = doc.enriched_data.get('semantic_tags', [])
                    
                    # å¾ analysis ç²å–ï¼ˆå‚™ç”¨ï¼‰
                    if not summary and hasattr(doc, 'analysis') and doc.analysis:
                        if hasattr(doc.analysis, 'ai_analysis_output'):
                            ai_output = doc.analysis.ai_analysis_output
                            if isinstance(ai_output, dict):
                                key_info = ai_output.get('key_information', {})
                                if isinstance(key_info, dict):
                                    summary = key_info.get('content_summary', '')
                                    key_concepts = key_info.get('key_concepts', [])
                                    semantic_tags = key_info.get('semantic_tags', [])
                except Exception as e:
                    logger.warning(f"æå–æ–‡æª” {doc_id} ä¿¡æ¯å¤±æ•—: {e}")
                
                # å‰µå»ºæ–‡æª”å¼•ç”¨ï¼ˆåªä¿å­˜æ‘˜è¦ç´šåˆ¥ï¼‰
                self._document_pool[doc_id] = DocumentRef(
                    document_id=doc_id,
                    filename=doc.filename,
                    summary=summary[:200] if summary else None,  # é™åˆ¶æ‘˜è¦é•·åº¦
                    key_concepts=key_concepts[:10] if key_concepts else [],  # æœ€å¤š10å€‹æ¦‚å¿µ
                    semantic_tags=semantic_tags[:5] if semantic_tags else [],  # æœ€å¤š5å€‹æ¨™ç±¤
                    first_mentioned_round=1,
                    last_accessed_round=self.current_round,
                    relevance_score=0.8,
                )
            
            # ä¿å­˜åˆ° cached_document_dataï¼ˆä¸‹æ¬¡ç›´æ¥ç”¨ï¼‰
            await self._save_document_pool_to_db()
            
            logger.debug(f"æ§‹å»ºä¸¦ä¿å­˜äº†æ–‡æª”æ± : {len(self._document_pool)} å€‹æ–‡æª”")
            
        except Exception as e:
            logger.error(f"è¼‰å…¥æ–‡æª”æ± å¤±æ•—: {e}", exc_info=True)
            self._document_pool = {}
    
    async def _update_document_pool(self, new_document_ids: List[str]):
        """æ›´æ–°æ–‡æª”æ± ï¼Œæ·»åŠ æ–°æ–‡æª”æˆ–æ›´æ–°ç¾æœ‰æ–‡æª”"""
        # ç¢ºä¿æ–‡æª”æ± å·²åˆå§‹åŒ–
        if self._document_pool is None:
            self._document_pool = {}
        
        # æ‰¹é‡æŸ¥è©¢æ–°æ–‡æª”ä¿¡æ¯ï¼ˆå„ªåŒ–æ€§èƒ½ï¼‰
        new_doc_ids = [doc_id for doc_id in new_document_ids if doc_id not in self._document_pool]
        
        if new_doc_ids:
            try:
                from app.crud.crud_documents import get_documents_by_ids
                new_documents = await get_documents_by_ids(self.db, new_doc_ids)
                
                # å»ºç«‹ doc_id -> document çš„æ˜ å°„
                doc_map = {str(doc.id): doc for doc in new_documents}
            except Exception as e:
                logger.warning(f"æ‰¹é‡æŸ¥è©¢æ–°æ–‡æª”å¤±æ•—: {e}")
                doc_map = {}
        else:
            doc_map = {}
        
        for doc_id in new_document_ids:
            if doc_id in self._document_pool:
                # å·²å­˜åœ¨ï¼Œæå‡ç›¸é—œæ€§
                self._document_pool[doc_id].boost_relevance()
                self._document_pool[doc_id].last_accessed_round = self.current_round
            else:
                # æ–°æ–‡æª”ï¼Œå¾æŸ¥è©¢çµæœä¸­ç²å–ä¿¡æ¯
                doc = doc_map.get(doc_id)
                if doc:
                    # æˆåŠŸç²å–æ–‡æª”ä¿¡æ¯ï¼Œæå– summary å’Œé—œéµä¿¡æ¯
                    summary = ""
                    key_concepts = []
                    semantic_tags = []
                    
                    try:
                        # å¾ enriched_data ç²å–
                        if hasattr(doc, 'enriched_data') and isinstance(doc.enriched_data, dict):
                            summary = doc.enriched_data.get('summary', '')
                            key_concepts = doc.enriched_data.get('key_concepts', [])
                            semantic_tags = doc.enriched_data.get('semantic_tags', [])
                        
                        # å¾ analysis ç²å–ï¼ˆå‚™ç”¨ï¼‰
                        if not summary and hasattr(doc, 'analysis') and doc.analysis:
                            if hasattr(doc.analysis, 'ai_analysis_output'):
                                ai_output = doc.analysis.ai_analysis_output
                                if isinstance(ai_output, dict):
                                    key_info = ai_output.get('key_information', {})
                                    if isinstance(key_info, dict):
                                        summary = key_info.get('content_summary', '')
                                        key_concepts = key_info.get('key_concepts', [])
                                        semantic_tags = key_info.get('semantic_tags', [])
                    except Exception as e:
                        logger.warning(f"æå–æ–‡æª” {doc_id} ä¿¡æ¯å¤±æ•—: {e}")
                    
                    self._document_pool[doc_id] = DocumentRef(
                        document_id=doc_id,
                        filename=doc.filename,
                        summary=summary[:200] if summary else None,  # é™åˆ¶æ‘˜è¦é•·åº¦
                        key_concepts=key_concepts[:10] if key_concepts else [],  # æœ€å¤š10å€‹æ¦‚å¿µ
                        semantic_tags=semantic_tags[:5] if semantic_tags else [],  # æœ€å¤š5å€‹æ¨™ç±¤
                        first_mentioned_round=self.current_round,
                        last_accessed_round=self.current_round,
                        relevance_score=1.0  # æ–°æåŠçš„æ–‡æª”ç›¸é—œæ€§æœ€é«˜
                    )
                else:
                    # æŸ¥è©¢å¤±æ•—ï¼Œä½¿ç”¨å ä½ç¬¦
                    logger.warning(f"æ–‡æª” {doc_id} ä¸å­˜åœ¨ï¼Œä½¿ç”¨å ä½ç¬¦")
                    self._document_pool[doc_id] = DocumentRef(
                        document_id=doc_id,
                        filename=f"Document_{doc_id[:8]}",
                        first_mentioned_round=self.current_round,
                        last_accessed_round=self.current_round,
                        relevance_score=1.0
                    )
        
        # æª¢æŸ¥ä¸¦è£å‰ªæ–‡æª”æ± å¤§å°
        max_pool_size = context_config.MAX_DOCUMENT_POOL_SIZE
        if len(self._document_pool) > max_pool_size:
            await self._trim_document_pool(max_pool_size)
    
    async def _trim_document_pool(self, max_size: int):
        """
        è£å‰ªæ–‡æª”æ± åˆ°æŒ‡å®šå¤§å°
        
        æŒ‰ç…§ (ç›¸é—œæ€§ * 0.7 + æ™‚æ•ˆæ€§ * 0.3) çš„å„ªå…ˆç´šæ’åºï¼Œ
        ä¿ç•™å„ªå…ˆç´šæœ€é«˜çš„æ–‡æª”ï¼Œç§»é™¤å…¶é¤˜çš„ã€‚
        
        Args:
            max_size: æ–‡æª”æ± æœ€å¤§å¤§å°
        """
        if len(self._document_pool) <= max_size:
            return
        
        # è¨ˆç®—æ¯å€‹æ–‡æª”çš„å„ªå…ˆç´šåˆ†æ•¸
        def compute_priority(doc_ref: DocumentRef) -> float:
            # æ™‚æ•ˆæ€§ï¼šæœ€è¿‘è¨ªå•çš„æ–‡æª”åˆ†æ•¸æ›´é«˜
            idle_rounds = self.current_round - doc_ref.last_accessed_round
            recency_score = 1 / (idle_rounds + 1)  # é¿å…é™¤ä»¥é›¶
            
            # ç¶œåˆåˆ†æ•¸ï¼šç›¸é—œæ€§æ¬Šé‡ 0.7ï¼Œæ™‚æ•ˆæ€§æ¬Šé‡ 0.3
            return doc_ref.relevance_score * 0.7 + recency_score * 0.3
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        sorted_docs = sorted(
            self._document_pool.items(),
            key=lambda x: compute_priority(x[1]),
            reverse=True
        )
        
        # è¨ˆç®—éœ€è¦ç§»é™¤çš„æ–‡æª”
        to_remove = sorted_docs[max_size:]
        
        logger.info(
            f"ğŸ—‘ï¸ æ–‡æª”æ± å·²æ»¿ ({len(self._document_pool)}/{max_size})ï¼Œ"
            f"ç§»é™¤ {len(to_remove)} å€‹ä½å„ªå…ˆç´šæ–‡æª”"
        )
        
        # åŸ·è¡Œç§»é™¤
        for doc_id, doc_ref in to_remove:
            logger.debug(
                f"  ç§»é™¤: {doc_ref.filename} "
                f"(score: {doc_ref.relevance_score:.2f}, "
                f"idle: {self.current_round - doc_ref.last_accessed_round} è¼ª)"
            )
            del self._document_pool[doc_id]
        
        logger.info(f"âœ… æ–‡æª”æ± è£å‰ªå®Œæˆï¼Œç•¶å‰å¤§å°: {len(self._document_pool)}")
    
    async def _save_document_pool_to_db(self):
        """
        ä¿å­˜æ–‡æª”æ± åˆ° MongoDB çš„ cached_document_data å­—æ®µ
        åªåœ¨æ–‡æª”æ± æ›´æ–°æ™‚èª¿ç”¨
        """
        try:
            if not self._document_pool:
                return
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
            cached_doc_data = {
                doc_id: doc_ref.to_dict()
                for doc_id, doc_ref in self._document_pool.items()
            }
            
            logger.debug(f"æº–å‚™ä¿å­˜æ–‡æª”æ± : {len(self._document_pool)} å€‹æ–‡æª”, æ–‡æª”ID: {list(self._document_pool.keys())[:3]}...")
            
            # æ›´æ–°åˆ°æ•¸æ“šåº«
            from app.crud import crud_conversations
            
            # âœ… åŒæ—¶æ›´æ–° cached_documents æ•°ç»„å’Œ cached_document_data
            doc_ids = list(self._document_pool.keys())
            result = await self.db.conversations.update_one(
                {
                    "_id": self.conversation_uuid,
                    "user_id": self.user_uuid
                },
                {
                    "$set": {
                        "cached_documents": doc_ids,  # âœ… æ›´æ–°æ–‡æ¡£IDæ•°ç»„
                        "cached_document_data": cached_doc_data,
                        "updated_at": datetime.now(UTC)
                    }
                }
            )
            
            if result.modified_count > 0:
                logger.debug(f"âœ… å·²ä¿å­˜æ–‡æª”æ± åˆ°æ•¸æ“šåº«: {len(self._document_pool)} å€‹æ–‡æª”")
            else:
                logger.warning(f"âš ï¸ æ–‡æª”æ± ä¿å­˜æœªä¿®æ”¹æ•¸æ“šåº«ï¼ˆå¯èƒ½æ•¸æ“šç›¸åŒï¼‰")
            
        except Exception as e:
            logger.warning(f"ä¿å­˜æ–‡æª”æ± å¤±æ•—: {e}")
    
    async def _invalidate_cache(self):
        """æ¸…é™¤ç·©å­˜"""
        self._cache_loaded = False
        self._message_cache = None
        self._document_pool = {}  # è¨­ç½®ç‚ºç©ºå­—å…¸è€Œä¸æ˜¯ None
        
        # æ¸…é™¤ Redis ç·©å­˜
        if self.enable_caching:
            try:
                from app.services.cache import unified_cache, CacheNamespace
                cache_key = f"{self.user_uuid}:{self.conversation_uuid}"
                await unified_cache.delete(key=cache_key, namespace=CacheNamespace.CONVERSATION)
            except Exception as e:
                logger.warning(f"æ¸…é™¤ Redis ç·©å­˜å¤±æ•—: {e}")
    
    async def _build_classification_context(
        self,
        bundle: ContextBundle,
        max_messages: int
    ) -> ContextBundle:
        """æ§‹å»ºç”¨æ–¼æ„åœ–åˆ†é¡çš„ä¸Šä¸‹æ–‡"""
        # åˆ—è¡¨æ ¼å¼çš„æ­·å²
        if self._message_cache:
            bundle.conversation_history_list = [
                msg.to_dict()
                for msg in self._message_cache[-max_messages:]
            ]
        
        # æ–‡æª”æ‘˜è¦åˆ—è¡¨ï¼ˆæŒ‰ç›¸é—œæ€§æ’åºï¼‰
        # âš ï¸ é‡è¦ï¼šreference_number å¿…é ˆèˆ‡ AI ç”Ÿæˆç­”æ¡ˆæ™‚çœ‹åˆ°çš„é †åºä¸€è‡´
        # é€™æ¨£ç”¨æˆ¶èªªã€Œç¬¬ä¸€å€‹æ–‡ä»¶ã€æ™‚ï¼ŒAI æ‰èƒ½æ­£ç¢ºç†è§£
        if self._document_pool:
            # æŒ‰ç›¸é—œæ€§æ’åºæ–‡æª”ï¼ˆé€™å€‹é †åºæœƒå‚³çµ¦ AIï¼‰
            sorted_docs = sorted(
                self._document_pool.values(),
                key=lambda x: x.relevance_score,
                reverse=True
            )
            
            # â­ é—œéµï¼šä¿å­˜æ’åºå¾Œçš„æ–‡æª”é †åºï¼Œä¾›å¾ŒçºŒå¼•ç”¨è§£æä½¿ç”¨
            # reference_number å¾ 1 é–‹å§‹ï¼Œèˆ‡ citation:1, citation:2 å°æ‡‰
            bundle.cached_documents_info = [
                {
                    "document_id": doc.document_id,
                    "filename": doc.filename,
                    "reference_number": idx,  # â­ é€™å€‹ç·¨è™Ÿèˆ‡ citation:N å°æ‡‰
                    "summary": doc.summary or "",
                    "relevance_score": doc.relevance_score,
                    "access_count": doc.access_count,
                    "key_concepts": doc.key_concepts[:5] if doc.key_concepts else [],
                    "semantic_tags": doc.semantic_tags[:3] if doc.semantic_tags else []
                }
                for idx, doc in enumerate(sorted_docs, 1)
            ]
            
            # â­ åŒæ™‚ä¿å­˜æ–‡æª”é †åºæ˜ å°„ï¼Œä¾›å¾ŒçºŒä½¿ç”¨
            # é€™ç¢ºä¿äº† reference_number -> document_id çš„æ˜ å°„æ˜¯ç©©å®šçš„
            logger.debug(f"ğŸ“‹ æ–‡æª”æ± é †åºï¼ˆç”¨æ–¼å¼•ç”¨ï¼‰: {[(d['reference_number'], d['filename']) for d in bundle.cached_documents_info[:5]]}")
        
        return bundle
    
    async def _build_answer_generation_context(
        self,
        bundle: ContextBundle,
        current_documents: Optional[List[Any]],
        max_messages: int
    ) -> ContextBundle:
        """æ§‹å»ºç”¨æ–¼ç­”æ¡ˆç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼ˆæ˜ç¢ºåˆ†é›¢æ­·å²å’Œç•¶å‰æ–‡æª”ï¼‰"""
        # æ ¼å¼åŒ–å°è©±æ­·å²
        if self._message_cache:
            history_lines = [
                msg.to_formatted_text(max_length=800)
                for msg in self._message_cache[-max_messages:]
            ]
            bundle.conversation_history_text = "\n".join(history_lines)
        
        # æ–‡æª”æ± å¼•ç”¨ï¼ˆåƒ…ä¾›åƒè€ƒï¼‰
        if self._document_pool:
            bundle.document_pool = self._document_pool
        
        return bundle
    
    async def _build_search_context(
        self,
        bundle: ContextBundle
    ) -> ContextBundle:
        """æ§‹å»ºç”¨æ–¼æ–‡æª”æª¢ç´¢çš„ä¸Šä¸‹æ–‡"""
        # å„ªå…ˆæ–‡æª”ID
        bundle.priority_document_ids = await self.get_retrieval_priority_docs()
        
        # æª¢ç´¢å»ºè­°
        bundle.should_reuse_cached = len(bundle.priority_document_ids) > 0
        bundle.search_expansion_needed = len(bundle.priority_document_ids) < 3
        
        return bundle
    
    async def _build_clarification_context(
        self,
        bundle: ContextBundle,
        max_messages: int
    ) -> ContextBundle:
        """æ§‹å»ºç”¨æ–¼æ¾„æ¸…å•é¡Œç”Ÿæˆçš„ä¸Šä¸‹æ–‡"""
        # ä¿ç•™å®Œæ•´å°è©±æ­·å²ï¼ˆä¸æˆªæ–·ï¼‰
        if self._message_cache:
            history_lines = [
                msg.to_formatted_text(max_length=None)  # ä¸æˆªæ–·
                for msg in self._message_cache[-max_messages:]
            ]
            bundle.conversation_history_text = "\n".join(history_lines)
        
        # æ–‡æª”æ± ä¿¡æ¯
        if self._document_pool:
            bundle.cached_documents_info = [
                {
                    "document_id": doc.document_id,
                    "filename": doc.filename,
                    "summary": doc.summary or ""
                }
                for doc in self._document_pool.values()
            ]
        
        return bundle


# å…¨å±€å·¥å» å‡½æ•¸
async def create_context_manager(
    db: AsyncIOMotorDatabase,
    conversation_id: str,
    user_id: str
) -> ConversationContextManager:
    """
    å‰µå»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„å·¥å» å‡½æ•¸
    
    Args:
        db: MongoDB é€£æ¥
        conversation_id: å°è©±ID
        user_id: ç”¨æˆ¶ID
        
    Returns:
        ConversationContextManager: ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯¦ä¾‹
    """
    return ConversationContextManager(
        db=db,
        conversation_id=conversation_id,
        user_id=user_id,
        enable_caching=True
    )
