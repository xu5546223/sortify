"""
è¤‡é›œåˆ†æè™•ç†å™¨

è™•ç†è¤‡é›œçš„åˆ†æå•é¡Œ,ä½¿ç”¨å®Œæ•´çš„RAGæµç¨‹,ä¿æŒé«˜è³ªé‡
ä½¿ç”¨æ–°çš„æ¨¡å¡ŠåŒ–æœå‹™,ç¢ºä¿æ‰€æœ‰åŠŸèƒ½(MongoDBæŸ¥è©¢ã€AIæ–‡æª”é¸æ“‡ç­‰)éƒ½ä¿ç•™
"""
import time
import logging
from typing import Optional
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import (
    AIQARequest,
    AIQAResponse,
    QueryRewriteResult,
    SemanticContextDocument
)
from app.models.question_models import QuestionClassification
from app.services.qa_core.qa_query_rewriter import qa_query_rewriter
from app.services.qa_core.qa_search_coordinator import qa_search_coordinator
from app.services.qa_core.qa_document_processor import qa_document_processor
from app.services.qa_core.qa_answer_service import qa_answer_service
from app.services.qa_workflow.conversation_helper import conversation_helper
from app.crud.crud_documents import get_documents_by_ids

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


class ComplexAnalysisHandler:
    """
    è¤‡é›œåˆ†æè™•ç†å™¨ - ä½¿ç”¨å®Œæ•´RAGæµç¨‹ï¼ˆçµ±ä¸€ç­–ç•¥ç‰ˆï¼‰
    
    çµ±ä¸€ç­–ç•¥ï¼ˆ2024å„ªåŒ–ç‰ˆï¼‰:
    - âœ… AIæŸ¥è©¢é‡å¯«
    - âœ… RRFèåˆæª¢ç´¢ï¼ˆå„ªå…ˆæœç´¢æ–‡æª”æ± ï¼‰
    - âœ… AIæ™ºèƒ½æ–‡æª”é¸æ“‡
    - âœ… MongoDBè©³ç´°æŸ¥è©¢
    - âœ… çµ±ä¸€å°è©±æ­·å²è¼‰å…¥ï¼ˆunified_context_helperï¼‰
    - âœ… æ–‡æª”æ± å„ªå…ˆç´šæ”¯æŒ
    - âœ… ç­”æ¡ˆç”Ÿæˆ
    
    å„ªå‹¢:
    - èˆ‡å…¶ä»–ç­–ç•¥ä¿æŒä¸€è‡´çš„ä¸Šä¸‹æ–‡è™•ç†
    - å„ªå…ˆä½¿ç”¨æ–‡æª”æ± æé«˜ç›¸é—œæ€§
    - å®Œæ•´RAGæµç¨‹ä¿è­‰é«˜è³ªé‡
    """
    
    async def handle(
        self,
        request: AIQARequest,
        classification: QuestionClassification,
        context: Optional[dict],
        db: Optional[AsyncIOMotorDatabase] = None,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """è™•ç†è¤‡é›œåˆ†æè«‹æ±‚ï¼ˆçµ±ä¸€ç­–ç•¥ç‰ˆï¼‰"""
        start_time = time.time()
        total_tokens = 0
        
        logger.info(f"è¤‡é›œåˆ†æ(çµ±ä¸€ç­–ç•¥): {request.question}")
        
        try:
            # ç²å–æ–‡æª”æ± å„ªå…ˆç´šä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            priority_document_ids = context.get('priority_document_ids', []) if context else []
            should_reuse_cached = context.get('should_reuse_cached', False) if context else False
            
            if priority_document_ids:
                logger.info(f"ğŸ¯ æ–‡æª”æ± åŒ…å« {len(priority_document_ids)} å€‹å„ªå…ˆæ–‡æª”")
            
            # Step 1: æŸ¥è©¢é‡å¯«ï¼ˆä¼ é€’ @ æ–‡ä»¶ä¿¡æ¯ï¼‰
            # âœ… å¦‚æœç”¨æˆ· @ äº†æ–‡ä»¶ï¼Œå‘Šè¯‰æŸ¥è¯¢é‡å†™å™¨
            document_context = None
            if request.document_ids:
                logger.info(f"ğŸ¯ æŸ¥è¯¢é‡å†™ï¼šç”¨æˆ·é€‰æ‹©äº† {len(request.document_ids)} ä¸ªæ–‡ä»¶")
                document_context = {
                    "document_ids": request.document_ids,
                    "document_count": len(request.document_ids)
                }
            
            query_rewrite_result, rewrite_tokens = await qa_query_rewriter.rewrite_query(
                db=db,
                original_query=request.question,
                user_id=str(user_id) if user_id else None,
                request_id=request_id,
                document_context=document_context  # âœ… ä¼ é€’æ–‡æ¡£ä¸Šä¸‹æ–‡
            )
            total_tokens += rewrite_tokens
            
            # Step 2: RRFèåˆæª¢ç´¢ï¼ˆå„ªå…ˆä½¿ç”¨æ–‡æª”æ± ï¼‰
            search_strategy = qa_search_coordinator.extract_search_strategy(query_rewrite_result)
            
            # âœ… ä¼˜å…ˆçº§ï¼š1. request.document_ids (@ æ–‡ä»¶) 2. priority_document_ids (å¦‚æœå»ºè®®é‡ç”¨)
            document_ids_filter = None
            if request.document_ids:
                document_ids_filter = request.document_ids
                logger.info(f"ğŸ¯ ä½¿ç”¨ @ æ–‡ä»¶: {len(request.document_ids)} å€‹")
            elif priority_document_ids and should_reuse_cached:
                document_ids_filter = priority_document_ids
                logger.info(f"ğŸ¯ ä½¿ç”¨å„ªå…ˆæ–‡æª”æ± : {len(priority_document_ids)} å€‹")
            
            semantic_results = await qa_search_coordinator.coordinate_search(
                db=db,
                query=query_rewrite_result.rewritten_queries[0] if query_rewrite_result.rewritten_queries else request.question,
                user_id=str(user_id) if user_id else None,
                search_strategy=search_strategy,
                top_k=getattr(request, 'max_documents_for_selection', 8),
                similarity_threshold=getattr(request, 'similarity_threshold', 0.3),
                document_ids=document_ids_filter  # å„ªå…ˆæœç´¢ @ æ–‡ä»¶æˆ–æ–‡æª”æ± 
            )
            
            semantic_contexts = [
                SemanticContextDocument(
                    document_id=r.document_id,
                    summary_or_chunk_text=r.summary_text,
                    similarity_score=r.similarity_score,
                    metadata=r.metadata
                )
                for r in semantic_results
            ]
            
            if not semantic_results:
                return self._create_no_results_response(
                    request, query_rewrite_result, semantic_contexts,
                    total_tokens, time.time() - start_time, classification, db, user_id
                )
            
            # Step 3: ç²å–ä¸¦éæ¿¾æ–‡æª”
            documents = await get_documents_by_ids(db, [r.document_id for r in semantic_results])
            if user_id:
                from uuid import UUID
                user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
                documents = [doc for doc in documents if hasattr(doc, 'owner_id') and doc.owner_id == user_uuid]
            
            if not documents:
                return self._create_no_results_response(
                    request, query_rewrite_result, semantic_contexts,
                    total_tokens, time.time() - start_time, classification, db, user_id
                )
            
            # Step 4: AIé¸æ“‡æ–‡æª”
            selected_doc_ids = await qa_document_processor.select_documents_for_detailed_query(
                db=db,
                user_question=request.question,
                semantic_contexts=semantic_contexts,
                user_id=str(user_id) if user_id else None,
                request_id=request_id,
                ai_selection_limit=getattr(request, 'ai_selection_limit', 3)
            )
            
            # Step 5: MongoDBè©³ç´°æŸ¥è©¢
            detailed_data = []
            if selected_doc_ids:
                schema_info = {"description": "MongoDBæ–‡ä»¶Schema", "fields": {"filename": "æ–‡ä»¶å", "extracted_text": "æ–‡æœ¬", "analysis": "AIåˆ†æ"}}
                
                for doc_id in selected_doc_ids:
                    detail = await qa_document_processor.query_document_details(
                        db=db,
                        document_id=doc_id,
                        user_question=request.question,
                        document_schema_info=schema_info,
                        user_id=str(user_id) if user_id else None,
                        model_preference=request.model_preference
                    )
                    if detail:
                        detailed_data.append(detail)
            
            # Step 6: è¼‰å…¥å°è©±æ­·å²ï¼ˆçµ±ä¸€æ–¹å¼ï¼‰
            from app.services.qa_workflow.unified_context_helper import unified_context_helper
            
            conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
                db=db,
                conversation_id=request.conversation_id,
                user_id=user_id,
                limit=5,
                max_content_length=2000
            )
            
            logger.info(f"è¼‰å…¥å°è©±æ­·å²: {len(conversation_history_text) if conversation_history_text else 0} å­—ç¬¦")
            
            # Step 7: ç”Ÿæˆç­”æ¡ˆ
            answer, answer_tokens, confidence, contexts = await qa_answer_service.generate_answer(
                db=db,
                original_query=request.question,
                documents_for_context=documents,
                query_rewrite_result=query_rewrite_result,
                detailed_document_data=detailed_data if detailed_data else None,
                ai_generated_query_reasoning=None,
                user_id=str(user_id) if user_id else None,
                request_id=request_id,
                model_preference=request.model_preference,
                conversation_history=conversation_history_text
            )
            total_tokens += answer_tokens
            
            processing_time = time.time() - start_time
            
            # ä¿å­˜å°è©±
            if db is not None:
                # âœ… åˆä½µæœç´¢çµæœ + ç”¨æˆ¶ @ çš„æ–‡ä»¶
                all_doc_ids = set(str(d.id) for d in documents)
                if request.document_ids:
                    all_doc_ids.update(request.document_ids)
                
                await conversation_helper.save_qa_to_conversation(
                    db=db,
                    conversation_id=request.conversation_id,
                    user_id=str(user_id) if user_id else None,
                    question=request.question,
                    answer=answer,
                    tokens_used=total_tokens,
                    source_documents=list(all_doc_ids)
                )
            
            # âœ… æ­£ç¡®è®¡ç®—æ˜¯å¦ä½¿ç”¨äº†æ–‡æ¡£æ± 
            used_document_pool = bool(request.document_ids) or (bool(priority_document_ids) and should_reuse_cached)
            doc_pool_size = len(request.document_ids) if request.document_ids else (len(priority_document_ids) if priority_document_ids else 0)
            
            logger.info(
                f"è¤‡é›œåˆ†æå®Œæˆ: {processing_time:.2f}ç§’, Token: {total_tokens}, "
                f"ä½¿ç”¨ @ æ–‡ä»¶: {bool(request.document_ids)}, "
                f"ä½¿ç”¨æ–‡æª”æ± : {used_document_pool}, "
                f"æ–‡æª”æ•¸: {doc_pool_size}"
            )
            
            return AIQAResponse(
                answer=answer,
                source_documents=[str(d.id) for d in documents],
                confidence_score=confidence,
                tokens_used=total_tokens,
                processing_time=processing_time,
                query_rewrite_result=query_rewrite_result,
                semantic_search_contexts=semantic_contexts,
                session_id=request.session_id,
                llm_context_documents=contexts,
                classification=classification,
                workflow_state={
                    "current_step": "completed",
                    "strategy_used": "complex_analysis_unified",
                    "api_calls": 4 + len(selected_doc_ids) if selected_doc_ids else 4,
                    "used_conversation_history": bool(conversation_history_text),
                    "used_document_pool": used_document_pool,
                    "document_pool_size": doc_pool_size,
                    "used_at_mention_files": bool(request.document_ids)
                },
                detailed_document_data_from_ai_query=detailed_data if detailed_data else None
            )
            
        except Exception as e:
            logger.error(f"è¤‡é›œåˆ†æå¤±æ•—: {e}", exc_info=True)
            return self._create_error_response(request, str(e), time.time() - start_time, total_tokens, classification)
    
    def _create_no_results_response(self, request, query_rewrite_result, semantic_contexts, tokens_used, processing_time, classification, db, user_id):
        """å‰µå»ºç„¡çµæœéŸ¿æ‡‰"""
        answer = "æŠ±æ­‰,æˆ‘åœ¨æ‚¨çš„æ–‡æª”åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œå…§å®¹ã€‚"
        return AIQAResponse(
            answer=answer,
            source_documents=[],
            confidence_score=0.0,
            tokens_used=tokens_used,
            processing_time=processing_time,
            query_rewrite_result=query_rewrite_result,
            semantic_search_contexts=semantic_contexts,
            session_id=request.session_id,
            classification=classification
        )
    
    def _create_error_response(self, request, error_msg, processing_time, tokens_used, classification):
        """å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰"""
        return AIQAResponse(
            answer=f"è™•ç†è¤‡é›œåˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}",
            source_documents=[],
            confidence_score=0.0,
            tokens_used=tokens_used,
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={},
                intent_analysis="è™•ç†å¤±æ•—"
            ),
            semantic_search_contexts=[],
            session_id=request.session_id,
            classification=classification,
            error_message=error_msg
        )


# å‰µå»ºå…¨å±€å¯¦ä¾‹
complex_analysis_handler = ComplexAnalysisHandler()
