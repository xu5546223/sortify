"""
æ–‡æª”è©³ç´°æŸ¥è©¢è™•ç†å™¨

è™•ç†å°å·²çŸ¥æ–‡æª”çš„è©³ç´°æ•¸æ“šæŸ¥è©¢ï¼Œä½¿ç”¨ MongoDB è©³ç´°æŸ¥è©¢åŠŸèƒ½æå–ç²¾ç¢ºä¿¡æ¯
"""
import time
import logging
import json
import uuid
from typing import Optional, List, Dict, Any
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import (
    AIQARequest,
    AIQAResponse,
    QueryRewriteResult
)
from app.models.question_models import QuestionClassification
from app.models.ai_models_simplified import AIMongoDBQueryDetailOutput
from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified
from app.services.qa_workflow.conversation_helper import conversation_helper
from app.crud.crud_documents import get_documents_by_ids

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


def remove_projection_path_collisions(projection: dict) -> dict:
    """ç§»é™¤ MongoDB projection ä¸­çš„çˆ¶å­æ¬„ä½è¡çª"""
    if not projection or not isinstance(projection, dict):
        return projection
    keys = list(projection.keys())
    keys_to_remove = set()
    for k in keys:
        for other in keys:
            if k == other:
                continue
            if k.startswith(other + "."):
                keys_to_remove.add(other)
            elif other.startswith(k + "."):
                keys_to_remove.add(k)
    for k in keys_to_remove:
        projection.pop(k, None)
    return projection


class DocumentDetailQueryHandler:
    """æ–‡æª”è©³ç´°æŸ¥è©¢è™•ç†å™¨ - å°å·²çŸ¥æ–‡æª”åŸ·è¡Œ MongoDB ç²¾ç¢ºæŸ¥è©¢"""
    
    async def handle(
        self,
        request: AIQARequest,
        classification: QuestionClassification,
        context: Optional[dict],
        db: Optional[AsyncIOMotorDatabase] = None,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        è™•ç†æ–‡æª”è©³ç´°æŸ¥è©¢è«‹æ±‚
        
        æµç¨‹:
        1. å¾å°è©±ä¸Šä¸‹æ–‡ç²å–å·²çŸ¥çš„æ–‡æª”ID
        2. è«‹æ±‚ç”¨æˆ¶æ‰¹å‡†è©³ç´°æŸ¥è©¢
        3. ä½¿ç”¨ AI ç”Ÿæˆ MongoDB æŸ¥è©¢
        4. åŸ·è¡ŒæŸ¥è©¢ç²å–ç²¾ç¢ºæ•¸æ“š
        5. ç”Ÿæˆç­”æ¡ˆ
        """
        start_time = time.time()
        api_calls = 0
        
        logger.info(f"è™•ç†æ–‡æª”è©³ç´°æŸ¥è©¢: {request.question}")
        
        # æª¢æŸ¥å·¥ä½œæµæ“ä½œ
        workflow_action = getattr(request, 'workflow_action', None)
        
        # æ­¥é©Ÿ1: ç²å–å·²çŸ¥çš„æ–‡æª”IDï¼ˆå¾å°è©±ä¸Šä¸‹æ–‡æˆ–ç·©å­˜ï¼‰
        cached_doc_ids = []
        if context and context.get('cached_document_ids'):
            cached_doc_ids = context['cached_document_ids']
            logger.info(f"å¾ä¸Šä¸‹æ–‡ç²å– {len(cached_doc_ids)} å€‹å·²çŸ¥æ–‡æª”")
        
        if not cached_doc_ids:
            # æ²’æœ‰å·²çŸ¥æ–‡æª”ï¼Œé€€åŒ–ç‚º document_search
            logger.warning("æ²’æœ‰æ‰¾åˆ°å·²çŸ¥æ–‡æª”ï¼Œè½‰ç‚ºæ–‡æª”æœç´¢")
            from app.services.intent_handlers.document_search_handler import document_search_handler
            return await document_search_handler.handle(
                request, classification, context, db, user_id, request_id
            )
        
        # å¦‚æœç”¨æˆ¶é¸æ“‡è·³éï¼Œä½¿ç”¨æ‘˜è¦å›ç­”
        if workflow_action == 'skip_detail_query':
            logger.info("ç”¨æˆ¶è·³éè©³ç´°æŸ¥è©¢ï¼Œä½¿ç”¨æ–‡æª”æ‘˜è¦å›ç­”")
            # è½‰ç™¼çµ¦ simple_factual_handler ä½¿ç”¨æ‘˜è¦
            from app.services.intent_handlers.simple_factual_handler import simple_factual_handler
            return await simple_factual_handler.handle(
                request, classification, db, user_id, request_id
            )
        
        # æ­¥é©Ÿ2: å¦‚æœç”¨æˆ¶æœªæ‰¹å‡†ï¼Œç²å–ç›®æ¨™æ–‡æª”ç„¶å¾Œè«‹æ±‚æ‰¹å‡†
        if workflow_action != 'approve_detail_query' and not getattr(request, 'skip_classification', False):
            logger.info(f"ç²å–ç›®æ¨™æ–‡æª”ï¼ˆå…± {len(cached_doc_ids)} å€‹å€™é¸ï¼‰")
            
            # å„ªå…ˆå¾åˆ†é¡çµæœç²å–ç›®æ¨™æ–‡æª” IDï¼ˆåˆ†é¡å™¨å·²ç¶“è­˜åˆ¥éäº†ï¼‰
            target_doc_ids = []
            if classification.target_document_ids:
                target_doc_ids = classification.target_document_ids
                logger.info(f"âœ… å¾åˆ†é¡å™¨ç›´æ¥ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹ï¼Œè·³éé‡è¤‡è­˜åˆ¥")
                if classification.target_document_reasoning:
                    logger.info(f"åˆ†é¡å™¨æ¨ç†: {classification.target_document_reasoning}")
            else:
                # å›é€€ï¼šå¦‚æœåˆ†é¡å™¨æ²’æœ‰è­˜åˆ¥ï¼Œä½¿ç”¨æ‰€æœ‰ç·©å­˜æ–‡æª”ï¼ˆæœ€å¤š3å€‹ï¼‰
                logger.warning("âš ï¸ åˆ†é¡å™¨æœªæä¾›ç›®æ¨™æ–‡æª”ï¼Œä½¿ç”¨å‰3å€‹ç·©å­˜æ–‡æª”ä½œç‚ºå›é€€")
                target_doc_ids = cached_doc_ids[:3]
            
            processing_time = time.time() - start_time
            
            # ç²å–ç›®æ¨™æ–‡æª”åç¨±ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰
            doc_names = []
            try:
                documents = await get_documents_by_ids(db, target_doc_ids)
                doc_names = [doc.filename for doc in documents if hasattr(doc, 'filename')]
            except Exception as e:
                logger.warning(f"ç²å–æ–‡æª”åç¨±å¤±æ•—: {e}")
            
            return AIQAResponse(
                answer="",
                source_documents=[],
                confidence_score=0.0,
                tokens_used=0,
                processing_time=processing_time,
                query_rewrite_result=QueryRewriteResult(
                    original_query=request.question,
                    rewritten_queries=[request.question],
                    extracted_parameters={},
                    intent_analysis=classification.reasoning
                ),
                semantic_search_contexts=[],
                session_id=request.session_id,
                classification=classification,
                workflow_state={
                    "current_step": "awaiting_detail_query_approval",
                    "strategy_used": "document_detail_query",
                    "api_calls": 0,
                    "target_documents": target_doc_ids,  # åªé¡¯ç¤ºè­˜åˆ¥å‡ºçš„ç›®æ¨™æ–‡æª”
                    "document_names": doc_names,
                    "query_type": "è©³ç´°æ•¸æ“šæŸ¥è©¢",
                    "estimated_time": "2-4ç§’"
                },
                next_action="approve_detail_query",
                pending_approval="detail_query"
            )
        
        # ç”¨æˆ¶å·²æ‰¹å‡†ï¼ŒåŸ·è¡Œè©³ç´°æŸ¥è©¢
        logger.info("ç”¨æˆ¶å·²æ‰¹å‡†è©³ç´°æŸ¥è©¢ï¼Œé–‹å§‹åŸ·è¡Œ")
        
        # æ­¥é©Ÿ3: ç²å–ç›®æ¨™æ–‡æª”IDï¼ˆä¸‰ç¨®ä¾†æºï¼Œå„ªå…ˆç´šéæ¸›ï¼‰
        target_doc_ids = []
        
        # å„ªå…ˆç´š1: å¾åˆ†é¡çµæœç²å–ï¼ˆæœ€æº–ç¢ºï¼Œåˆ†é¡æ™‚å·²è­˜åˆ¥ï¼‰
        if classification.target_document_ids:
            target_doc_ids = classification.target_document_ids
            logger.info(f"âœ… å¾åˆ†é¡å™¨ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹ï¼ˆé¿å…é‡è¤‡è­˜åˆ¥ï¼‰")
        
        # å„ªå…ˆç´š2: å¾è«‹æ±‚åƒæ•¸ç²å–ï¼ˆå‰ç«¯å¯èƒ½å‚³å›ï¼‰
        elif hasattr(request, 'document_ids') and request.document_ids:
            target_doc_ids = request.document_ids
            logger.info(f"ğŸ“¥ å¾è«‹æ±‚åƒæ•¸ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹")
        
        # å„ªå…ˆç´š3: å›é€€æ–¹æ¡ˆ - ä½¿ç”¨å‰3å€‹ç·©å­˜æ–‡æª”
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›®æ¨™æ–‡æª”IDï¼Œä½¿ç”¨å‰3å€‹ç·©å­˜æ–‡æª”ä½œç‚ºå›é€€")
            target_doc_ids = cached_doc_ids[:3]
        
        # æ­¥é©Ÿ4: æº–å‚™æ–‡æª” Schema ä¿¡æ¯
        document_schema_info = {
            "description": "MongoDB æ–‡æª” Schema",
            "fields": {
                "filename": "æ–‡ä»¶å",
                "extracted_text": "æ–‡æœ¬å…§å®¹",
                "analysis.ai_analysis_output.key_information": "çµæ§‹åŒ–ä¿¡æ¯ï¼ˆé‡‘é¡ã€æ—¥æœŸã€äººåç­‰ï¼‰",
                "analysis.ai_analysis_output.key_information.dynamic_fields": "å‹•æ…‹æ¬„ä½"
            }
        }
        
        # æ­¥é©Ÿ5: å°é¸å®šçš„æ–‡æª”åŸ·è¡Œ MongoDB è©³ç´°æŸ¥è©¢
        all_detailed_data = []
        document_reference_map = {}  # ç”¨æ–¼ä¿å­˜æ–‡æª”IDåˆ°åƒè€ƒç·¨è™Ÿçš„æ˜ å°„
        
        # æ§‹å»ºæ–‡æª”IDåˆ°åƒè€ƒç·¨è™Ÿçš„æ˜ å°„ï¼ˆå¾ç·©å­˜æ–‡æª”åˆ—è¡¨ï¼‰
        for idx, doc_id in enumerate(cached_doc_ids, 1):
            document_reference_map[doc_id] = idx
        
        documents = await get_documents_by_ids(db, target_doc_ids)
        
        # éæ¿¾æ¬Šé™
        if user_id:
            from uuid import UUID
            user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
            documents = [doc for doc in documents if hasattr(doc, 'owner_id') and doc.owner_id == user_uuid]
        
        for doc in documents:
            logger.info(f"å°æ–‡æª” {doc.filename} åŸ·è¡Œè©³ç´°æŸ¥è©¢")
            
            ai_query_response = await unified_ai_service_simplified.generate_mongodb_detail_query(
                user_question=request.question,
                document_id=str(doc.id),
                document_schema_info=document_schema_info,
                db=db,
                model_preference=request.model_preference,
                user_id=user_id,
                session_id=request.session_id
            )
            api_calls += 1
            
            if ai_query_response.success and isinstance(ai_query_response.output_data, AIMongoDBQueryDetailOutput):
                query_components = ai_query_response.output_data
                
                mongo_filter = {"_id": doc.id}
                mongo_projection = query_components.projection
                
                if query_components.sub_filter:
                    mongo_filter.update(query_components.sub_filter)
                
                if mongo_projection or query_components.sub_filter:
                    safe_projection = remove_projection_path_collisions(mongo_projection) if mongo_projection else None
                    fetched_data = await db.documents.find_one(mongo_filter, projection=safe_projection)
                    
                    if fetched_data:
                        # è³‡æ–™æ¸…ç†
                        def sanitize(data: Any) -> Any:
                            if isinstance(data, dict):
                                return {k: sanitize(v) for k, v in data.items()}
                            if isinstance(data, list):
                                return [sanitize(i) for i in data]
                            if isinstance(data, uuid.UUID):
                                return str(data)
                            return data
                        
                        sanitized_data = sanitize(fetched_data)
                        
                        # æ·»åŠ å…ƒæ•¸æ“šï¼šåŸå§‹çš„åƒè€ƒç·¨è™Ÿï¼ˆæ–‡æª”å¹¾ï¼‰
                        doc_id_str = str(doc.id)
                        if doc_id_str in document_reference_map:
                            sanitized_data['_reference_number'] = document_reference_map[doc_id_str]
                        
                        all_detailed_data.append(sanitized_data)
                        logger.info(f"æˆåŠŸç²å–æ–‡æª” {doc.filename} çš„è©³ç´°æ•¸æ“š")
        
        # æ­¥é©Ÿ5: ä½¿ç”¨è©³ç´°æ•¸æ“šç”Ÿæˆç­”æ¡ˆ
        answer = await self._generate_answer_from_details(
            question=request.question,
            detailed_data=all_detailed_data,
            classification=classification,
            db=db,
            user_id=user_id,
            conversation_id=request.conversation_id,
            context=context
        )
        api_calls += 1
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜å°è©±
        if db is not None:
            await conversation_helper.save_qa_to_conversation(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                question=request.question,
                answer=answer,
                tokens_used=api_calls * 150,
                source_documents=target_doc_ids
            )
        
        logger.info(f"è©³ç´°æŸ¥è©¢å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’, APIèª¿ç”¨: {api_calls}æ¬¡")
        
        return AIQAResponse(
            answer=answer,
            source_documents=target_doc_ids,
            confidence_score=0.90,
            tokens_used=api_calls * 150,
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={
                    "detail_query_count": len(all_detailed_data),
                    "target_document_count": len(target_doc_ids)
                },
                intent_analysis=classification.reasoning
            ),
            semantic_search_contexts=[],
            session_id=request.session_id,
            classification=classification,
            workflow_state={
                "current_step": "completed",
                "strategy_used": "document_detail_query",
                "api_calls": api_calls,
                "documents_queried": len(all_detailed_data),
                "target_documents": target_doc_ids
            },
            detailed_document_data_from_ai_query=all_detailed_data
        )
    
    async def _generate_answer_from_details(
        self,
        question: str,
        detailed_data: List[Dict],
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str],
        context: Optional[dict]
    ) -> str:
        """ä½¿ç”¨è©³ç´°æ•¸æ“šç”Ÿæˆç­”æ¡ˆ"""
        
        # è¼‰å…¥å°è©±æ­·å²
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id,
            limit=5,
            max_content_length=2000
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)
        
        # æ·»åŠ è©³ç´°æ•¸æ“š
        for i, data in enumerate(detailed_data, 1):
            data_str = json.dumps(data, ensure_ascii=False, indent=2)
            
            # ç²å–æ–‡æª”çš„åŸå§‹åƒè€ƒç·¨è™Ÿï¼ˆæ–‡æª”å¹¾ï¼‰
            filename = data.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            reference_number = data.get('_reference_number', i)  # å¦‚æœæœ‰åŸå§‹ç·¨è™Ÿå°±ç”¨ï¼Œæ²’æœ‰å°±ç”¨å¾ªç’°ç·¨è™Ÿ
            
            # æ§‹å»ºæ¸…æ™°çš„æ¨™é¡Œï¼ŒåŒ…å«åƒè€ƒç·¨è™Ÿå’Œæ–‡ä»¶å
            doc_label = f"æ–‡æª”{reference_number} ({filename})"
            context_parts.append(f"=== {doc_label} çš„è©³ç´°æ•¸æ“š ===\n{data_str}\n")
            
            logger.debug(f"æ·»åŠ æ–‡æª”ä¸Šä¸‹æ–‡: {doc_label}")
        
        # èª¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆ
        try:
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=context_parts,
                db=db,
                user_id=user_id
            )
            
            if ai_response.success and ai_response.output_data:
                return ai_response.output_data.answer_text
            else:
                return "æŠ±æ­‰ï¼Œç„¡æ³•å¾æ–‡æª”è©³ç´°æ•¸æ“šä¸­ç”Ÿæˆç­”æ¡ˆã€‚"
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}", exc_info=True)
            return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"


# å‰µå»ºå…¨å±€å¯¦ä¾‹
document_detail_query_handler = DocumentDetailQueryHandler()

