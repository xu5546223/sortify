"""
æ–‡æª”è©³ç´°æŸ¥è©¢è™•ç†å™¨

è™•ç†å°å·²çŸ¥æ–‡æª”çš„è©³ç´°æ•¸æ“šæŸ¥è©¢ï¼Œä½¿ç”¨ MongoDB è©³ç´°æŸ¥è©¢åŠŸèƒ½æå–ç²¾ç¢ºä¿¡æ¯
"""
import time
import logging
import json
import uuid
from typing import Optional, List, Dict, Any
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import (
    AIQARequest,
    AIQAResponse,
    QueryRewriteResult
)
from app.models.question_models import QuestionClassification
from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified
from app.services.qa_workflow.conversation_helper import conversation_helper
from app.crud.crud_documents import get_documents_by_ids

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


def remove_projection_path_collisions(projection: dict) -> dict:
    """ç§»é™¤ MongoDB projection ä¸­çš„çˆ¶å­æ¬„ä½è¡çª"""
    if not projection or not isinstance(projection, dict):
        return projection
    keys = list(projection.keys())
    keys_to_remove = set()
    for k in keys:
        for other in keys:
            if k == other:
                continue
            if k.startswith(other + "."):
                keys_to_remove.add(other)
            elif other.startswith(k + "."):
                keys_to_remove.add(k)
    for k in keys_to_remove:
        projection.pop(k, None)
    return projection


def sanitize_for_json(obj: Any) -> Any:
    """æ¸…ç†æ•¸æ“šä¸­çš„ä¸å¯ JSON åºåˆ—åŒ–çš„å°è±¡ï¼ˆUUIDã€datetime ç­‰ï¼‰"""
    from datetime import datetime
    if isinstance(obj, dict):
        return {k: sanitize_for_json(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [sanitize_for_json(item) for item in obj]
    if isinstance(obj, uuid.UUID):
        return str(obj)
    if isinstance(obj, datetime):
        return obj.isoformat()
    return obj


class DocumentDetailQueryHandler:
    """æ–‡æª”è©³ç´°æŸ¥è©¢è™•ç†å™¨ - å°å·²çŸ¥æ–‡æª”åŸ·è¡Œ MongoDB ç²¾ç¢ºæŸ¥è©¢"""
    
    async def handle(
        self,
        request: AIQARequest,
        classification: QuestionClassification,
        context: Optional[dict],
        db: Optional[AsyncIOMotorDatabase] = None,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        è™•ç†æ–‡æª”è©³ç´°æŸ¥è©¢è«‹æ±‚
        
        æµç¨‹:
        1. å¾å°è©±ä¸Šä¸‹æ–‡ç²å–å·²çŸ¥çš„æ–‡æª”ID
        2. è«‹æ±‚ç”¨æˆ¶æ‰¹å‡†è©³ç´°æŸ¥è©¢
        3. ä½¿ç”¨ AI ç”Ÿæˆ MongoDB æŸ¥è©¢
        4. åŸ·è¡ŒæŸ¥è©¢ç²å–ç²¾ç¢ºæ•¸æ“š
        5. ç”Ÿæˆç­”æ¡ˆ
        """
        start_time = time.time()
        api_calls = 0
        
        logger.info(f"è™•ç†æ–‡æª”è©³ç´°æŸ¥è©¢: {request.question}")
        
        # æª¢æŸ¥å·¥ä½œæµæ“ä½œ
        workflow_action = getattr(request, 'workflow_action', None)
        
        # æ­¥é©Ÿ1: ç²å–å·²çŸ¥çš„æ–‡æª”IDï¼ˆå„ªå…ˆä½¿ç”¨å„ªå…ˆæ–‡æª”ï¼‰
        # å„ªå…ˆä½¿ç”¨çµ±ä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨æä¾›çš„å„ªå…ˆæ–‡æª”
        priority_doc_ids = context.get('priority_document_ids', []) if context else []
        cached_doc_ids = context.get('cached_document_ids', []) if context else []
        
        # å„ªå…ˆæ–‡æª”å„ªå…ˆç´šæ›´é«˜ï¼ˆåŸºæ–¼ç›¸é—œæ€§å’Œè¨ªå•é »ç‡ï¼‰
        available_doc_ids = priority_doc_ids if priority_doc_ids else cached_doc_ids
        
        if priority_doc_ids:
            logger.info(f"ğŸ¯ ä½¿ç”¨å„ªå…ˆæ–‡æª”: {len(priority_doc_ids)} å€‹ï¼ˆä¾†è‡ªæ–‡æª”æ± ï¼‰")
        elif cached_doc_ids:
            logger.info(f"å¾ä¸Šä¸‹æ–‡ç²å– {len(cached_doc_ids)} å€‹å·²çŸ¥æ–‡æª”ï¼ˆèˆŠæ–¹å¼ï¼‰")
        
        if not available_doc_ids:
            # æ²’æœ‰å·²çŸ¥æ–‡æª”ï¼Œé€€åŒ–ç‚º document_search
            logger.warning("æ²’æœ‰æ‰¾åˆ°å·²çŸ¥æ–‡æª”ï¼Œè½‰ç‚ºæ–‡æª”æœç´¢")
            from app.services.intent_handlers.document_search_handler import document_search_handler
            return await document_search_handler.handle(
                request, classification, context, db, user_id, request_id
            )
        
        # å¦‚æœç”¨æˆ¶é¸æ“‡è·³éï¼Œä½¿ç”¨æ‘˜è¦å›ç­”
        if workflow_action == 'skip_detail_query':
            logger.info("ç”¨æˆ¶è·³éè©³ç´°æŸ¥è©¢ï¼Œä½¿ç”¨æ–‡æª”æ‘˜è¦å›ç­”")
            # è½‰ç™¼çµ¦ simple_factual_handler ä½¿ç”¨æ‘˜è¦
            from app.services.intent_handlers.simple_factual_handler import simple_factual_handler
            return await simple_factual_handler.handle(
                request, classification, None, db, user_id, request_id
            )
        
        # æ­¥é©Ÿ2: å¦‚æœç”¨æˆ¶æœªæ‰¹å‡†ï¼Œç²å–ç›®æ¨™æ–‡æª”ç„¶å¾Œè«‹æ±‚æ‰¹å‡†
        if workflow_action != 'approve_detail_query' and not getattr(request, 'skip_classification', False):
            logger.info(f"ç²å–ç›®æ¨™æ–‡æª”ï¼ˆå…± {len(available_doc_ids)} å€‹å€™é¸ï¼‰")
            
            # å„ªå…ˆå¾åˆ†é¡çµæœç²å–ç›®æ¨™æ–‡æª” IDï¼ˆåˆ†é¡å™¨å·²ç¶“è­˜åˆ¥éäº†ï¼‰
            target_doc_ids = []
            if classification.target_document_ids:
                target_doc_ids = classification.target_document_ids
                logger.info(f"âœ… å¾åˆ†é¡å™¨ç›´æ¥ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹ï¼Œè·³éé‡è¤‡è­˜åˆ¥")
                if classification.target_document_reasoning:
                    logger.info(f"åˆ†é¡å™¨æ¨ç†: {classification.target_document_reasoning}")
            else:
                # å›é€€ï¼šå¦‚æœåˆ†é¡å™¨æ²’æœ‰è­˜åˆ¥ï¼Œå„ªå…ˆä½¿ç”¨å„ªå…ˆæ–‡æª”ï¼ˆæœ€å¤š3å€‹ï¼‰
                if priority_doc_ids:
                    logger.info("âœ… ä½¿ç”¨å„ªå…ˆæ–‡æª”ä½œç‚ºç›®æ¨™ï¼ˆæœ€ç›¸é—œçš„æ–‡æª”ï¼‰")
                    target_doc_ids = priority_doc_ids[:3]
                else:
                    logger.warning("âš ï¸ åˆ†é¡å™¨æœªæä¾›ç›®æ¨™æ–‡æª”ï¼Œä½¿ç”¨å‰3å€‹ç·©å­˜æ–‡æª”ä½œç‚ºå›é€€")
                    target_doc_ids = available_doc_ids[:3]
            
            processing_time = time.time() - start_time
            
            # ç²å–ç›®æ¨™æ–‡æª”åç¨±ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰
            doc_names = []
            try:
                documents = await get_documents_by_ids(db, target_doc_ids)
                doc_names = [doc.filename for doc in documents if hasattr(doc, 'filename')]
            except Exception as e:
                logger.warning(f"ç²å–æ–‡æª”åç¨±å¤±æ•—: {e}")
            
            return AIQAResponse(
                answer="",
                source_documents=[],
                confidence_score=0.0,
                tokens_used=0,
                processing_time=processing_time,
                query_rewrite_result=QueryRewriteResult(
                    original_query=request.question,
                    rewritten_queries=[request.question],
                    extracted_parameters={},
                    intent_analysis=classification.reasoning
                ),
                semantic_search_contexts=[],
                session_id=request.session_id,
                classification=classification,
                workflow_state={
                    "current_step": "awaiting_detail_query_approval",
                    "strategy_used": "document_detail_query",
                    "api_calls": 0,
                    "target_documents": target_doc_ids,  # åªé¡¯ç¤ºè­˜åˆ¥å‡ºçš„ç›®æ¨™æ–‡æª”
                    "document_names": doc_names,
                    "query_type": "è©³ç´°æ•¸æ“šæŸ¥è©¢",
                    "estimated_time": "2-4ç§’"
                },
                next_action="approve_detail_query",
                pending_approval="detail_query"
            )
        
        # ç”¨æˆ¶å·²æ‰¹å‡†ï¼ŒåŸ·è¡Œè©³ç´°æŸ¥è©¢
        logger.info("ç”¨æˆ¶å·²æ‰¹å‡†è©³ç´°æŸ¥è©¢ï¼Œé–‹å§‹åŸ·è¡Œ")
        
        # æ­¥é©Ÿ3: ç²å–ç›®æ¨™æ–‡æª”IDï¼ˆä¸‰ç¨®ä¾†æºï¼Œå„ªå…ˆç´šéæ¸›ï¼‰
        target_doc_ids = []
        
        # å„ªå…ˆç´š1: å¾åˆ†é¡çµæœç²å–ï¼ˆæœ€æº–ç¢ºï¼Œåˆ†é¡æ™‚å·²è­˜åˆ¥ï¼‰
        if classification.target_document_ids:
            target_doc_ids = classification.target_document_ids
            logger.info(f"âœ… å¾åˆ†é¡å™¨ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹ï¼ˆé¿å…é‡è¤‡è­˜åˆ¥ï¼‰")
        
        # å„ªå…ˆç´š2: å¾è«‹æ±‚åƒæ•¸ç²å–ï¼ˆå‰ç«¯å¯èƒ½å‚³å›ï¼‰
        elif hasattr(request, 'document_ids') and request.document_ids:
            target_doc_ids = request.document_ids
            logger.info(f"ğŸ“¥ å¾è«‹æ±‚åƒæ•¸ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹")
        
        # å„ªå…ˆç´š3: å›é€€æ–¹æ¡ˆ - ä½¿ç”¨å‰3å€‹å¯ç”¨æ–‡æª”ï¼ˆå„ªå…ˆæ–‡æª”æˆ–ç·©å­˜æ–‡æª”ï¼‰
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›®æ¨™æ–‡æª”IDï¼Œä½¿ç”¨å‰3å€‹å¯ç”¨æ–‡æª”ä½œç‚ºå›é€€")
            target_doc_ids = available_doc_ids[:3]
        
        # æ­¥é©Ÿ4: ä½¿ç”¨å›ºå®šçš„ projection ç›´æ¥æŸ¥è©¢æ–‡æª”ï¼ˆç°¡åŒ–ç­–ç•¥ï¼‰
        logger.info(f"ğŸ“‹ æº–å‚™æŸ¥è©¢ {len(target_doc_ids)} å€‹æ–‡æª”çš„å®Œæ•´æ–‡æœ¬å…§å®¹...")

        # âœ… ç°¡åŒ–æ–¹æ¡ˆï¼šå›ºå®šæŸ¥è©¢æ¬„ä½ï¼Œä¸éœ€è¦ AI ç”ŸæˆæŸ¥è©¢
        # åªæŸ¥è©¢å¿…è¦çš„æ¬„ä½ï¼š_idã€filenameã€extracted_text
        fixed_projection = {
            "_id": 1,
            "filename": 1,
            "extracted_text": 1  # å®Œæ•´çš„æå–æ–‡æœ¬ï¼ˆé©ç”¨æ–¼æ‰€æœ‰æ–‡ä»¶é¡å‹ï¼‰
        }

        logger.info(f"âœ… ä½¿ç”¨å›ºå®š projection æŸ¥è©¢ç­–ç•¥ï¼ˆextracted_textï¼‰")

        # æ­¥é©Ÿ5: å°é¸å®šçš„æ–‡æª”åŸ·è¡Œ MongoDB è©³ç´°æŸ¥è©¢
        all_detailed_data = []
        document_reference_map = {}  # ç”¨æ–¼ä¿å­˜æ–‡æª”IDåˆ°åƒè€ƒç·¨è™Ÿçš„æ˜ å°„
        
        # æ§‹å»ºæ–‡æª”IDåˆ°åƒè€ƒç·¨è™Ÿçš„æ˜ å°„ï¼ˆå¾å¯ç”¨æ–‡æª”åˆ—è¡¨ï¼Œå„ªå…ˆä½¿ç”¨å„ªå…ˆæ–‡æª”ï¼‰
        # ç¢ºä¿ key çµ±ä¸€ç‚ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä»¥ä¾¿å¾ŒçºŒæŸ¥æ‰¾
        for idx, doc_id in enumerate(available_doc_ids, 1):
            document_reference_map[str(doc_id)] = idx
        
        documents = await get_documents_by_ids(db, target_doc_ids)

        # éæ¿¾æ¬Šé™
        if user_id:
            from uuid import UUID
            user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
            documents = [doc for doc in documents if hasattr(doc, 'owner_id') and doc.owner_id == user_uuid]

        # âœ… ç›´æ¥æŸ¥è©¢æ–‡æª”ï¼Œä¸éœ€è¦ AI ç”ŸæˆæŸ¥è©¢ï¼ˆç°¡åŒ–ä¸”å¯é ï¼‰
        for doc in documents:
            logger.info(f"å°æ–‡æª” {doc.filename} åŸ·è¡Œè©³ç´°æŸ¥è©¢ï¼ˆfixed projectionï¼‰")

            try:
                # ç›´æ¥ä½¿ç”¨å›ºå®š projection æŸ¥è©¢
                fetched_data = await db.documents.find_one(
                    {"_id": doc.id},
                    projection=fixed_projection
                )

                if fetched_data:
                    # è³‡æ–™æ¸…ç†
                    sanitized_data = sanitize_for_json(fetched_data)

                    # æ·»åŠ å…ƒæ•¸æ“šï¼šåŸå§‹çš„åƒè€ƒç·¨è™Ÿï¼ˆæ–‡æª”å¹¾ï¼‰
                    doc_id_str = str(doc.id)
                    if doc_id_str in document_reference_map:
                        sanitized_data['_reference_number'] = document_reference_map[doc_id_str]

                    all_detailed_data.append(sanitized_data)
                    logger.info(f"âœ… æˆåŠŸç²å–æ–‡æª” {doc.filename} çš„è©³ç´°æ•¸æ“šï¼ˆ{len(fetched_data.get('extracted_text', ''))} å­—ç¬¦ï¼‰")
                else:
                    logger.warning(f"âš ï¸ æ–‡æª” {doc.filename} æŸ¥è©¢çµæœç‚ºç©º")

            except Exception as e:
                logger.error(f"âŒ æŸ¥è©¢æ–‡æª” {doc.filename} å¤±æ•—: {e}", exc_info=True)

        # æ­¥é©Ÿ6: ä½¿ç”¨è©³ç´°æ•¸æ“šç”Ÿæˆç­”æ¡ˆ
        answer = await self._generate_answer_from_details(
            question=request.question,
            detailed_data=all_detailed_data,
            classification=classification,
            db=db,
            user_id=user_id,
            conversation_id=request.conversation_id,
            context=context
        )
        api_calls += 1
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜å°è©±
        if db is not None:
            # â­ åˆä½µç›®æ¨™æ–‡æª” + ç”¨æˆ¶ @ çš„æ–‡ä»¶ï¼ˆä¿æŒé †åºï¼‰
            # all_detailed_data çš„é †åºå°±æ˜¯ AI çœ‹åˆ°çš„é †åº
            all_doc_ids_ordered = []
            for data in all_detailed_data:
                doc_id = str(data.get('_id', ''))
                if doc_id and doc_id not in all_doc_ids_ordered:
                    all_doc_ids_ordered.append(doc_id)
            
            # æ·»åŠ ç”¨æˆ¶ @ çš„æ–‡ä»¶ï¼ˆå¦‚æœä¸åœ¨åˆ—è¡¨ä¸­ï¼‰
            if request.document_ids:
                for doc_id in request.document_ids:
                    if doc_id not in all_doc_ids_ordered:
                        all_doc_ids_ordered.append(doc_id)
            
            await conversation_helper.save_qa_to_conversation(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                question=request.question,
                answer=answer,
                tokens_used=api_calls * 150,
                source_documents=all_doc_ids_ordered  # â­ ä½¿ç”¨æœ‰åºåˆ—è¡¨
            )
        
        logger.info(f"è©³ç´°æŸ¥è©¢å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’, APIèª¿ç”¨: {api_calls}æ¬¡")
        
        # æ§‹å»ºåŒ…å«è©³ç´°æ•¸æ“šçš„ semantic_search_contexts
        from app.models.vector_models import SemanticContextDocument
        semantic_contexts = []
        for data in all_detailed_data:
            # æå–æ–‡æª”ä¿¡æ¯
            doc_filename = data.get('filename', 'æœªçŸ¥æ–‡æª”')
            reference_num = data.get('_reference_number', 0)
            
            # å‰µå»ºä¸€å€‹åŒ…å«è©³ç´°æ•¸æ“šçš„ context
            context_doc = SemanticContextDocument(
                document_id=str(data.get('_id', '')),
                summary_or_chunk_text=f"MongoDB æŸ¥è©¢çµæœï¼š{json.dumps(data, ensure_ascii=False, indent=2)}",
                similarity_score=1.0,
                metadata={
                    'source': 'mongodb_detail_query',
                    'filename': doc_filename,
                    'reference_number': reference_num,
                    'fields_count': len(data) - 2,  # æ’é™¤ _id å’Œ _reference_number
                    'detailed_data': data  # ä¿å­˜å®Œæ•´çš„è©³ç´°æ•¸æ“š
                }
            )
            semantic_contexts.append(context_doc)
        
        # â­ ä¿®å¾©ï¼šsource_documents çš„é †åºå¿…é ˆèˆ‡ AI çœ‹åˆ°çš„é †åºä¸€è‡´
        # AI çœ‹åˆ°çš„æ˜¯ all_detailed_data çš„é †åºï¼ˆæŒ‰ _reference_number æ’åºï¼‰
        # å¾ all_detailed_data ä¸­æå–æ–‡æª” IDï¼Œä¿æŒèˆ‡ AI çœ‹åˆ°çš„ç›¸åŒé †åº
        source_doc_ids_in_ai_order = []
        for data in all_detailed_data:
            doc_id = str(data.get('_id', ''))
            if doc_id and doc_id not in source_doc_ids_in_ai_order:
                source_doc_ids_in_ai_order.append(doc_id)
        
        # å¦‚æœæ²’æœ‰è©³ç´°æ•¸æ“šï¼Œä½¿ç”¨ target_doc_ids ä½œç‚º fallback
        if not source_doc_ids_in_ai_order:
            source_doc_ids_in_ai_order = target_doc_ids
        
        return AIQAResponse(
            answer=answer,
            source_documents=source_doc_ids_in_ai_order,  # â­ ä¿®å¾©ï¼šä½¿ç”¨èˆ‡ AI çœ‹åˆ°çš„ç›¸åŒé †åº
            confidence_score=0.90,
            tokens_used=api_calls * 150,
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={
                    "detail_query_count": len(all_detailed_data),
                    "target_document_count": len(target_doc_ids),
                    "total_fields": sum(len(data) - 2 for data in all_detailed_data)  # æ’é™¤ _id å’Œ _reference_number
                },
                intent_analysis=classification.reasoning
            ),
            semantic_search_contexts=semantic_contexts,  # åŒ…å«è©³ç´°æ•¸æ“š
            session_id=request.session_id,
            classification=classification,
            workflow_state={
                "current_step": "completed",
                "strategy_used": "document_detail_query",
                "api_calls": api_calls,
                "documents_queried": len(all_detailed_data),
                "target_documents": target_doc_ids,
                "mongodb_results": all_detailed_data  # åŒæ™‚ä¿ç•™åœ¨ workflow_state ä¸­
            },
            detailed_document_data_from_ai_query=all_detailed_data
        )
    
    async def _generate_answer_from_details(
        self,
        question: str,
        detailed_data: List[Dict],
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str],
        context: Optional[dict]
    ) -> str:
        """ä½¿ç”¨è©³ç´°æ•¸æ“šç”Ÿæˆç­”æ¡ˆï¼ˆéæµå¼ï¼‰"""
        
        # è¼‰å…¥å°è©±æ­·å²
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)

        # æ·»åŠ è©³ç´°æ•¸æ“šï¼ˆâœ… ç°¡åŒ–æ–¹æ¡ˆï¼šåªä½¿ç”¨ extracted_textï¼‰
        for i, data in enumerate(detailed_data, 1):
            filename = data.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            extracted_text = data.get('extracted_text', '')

            # å®¹éŒ¯è™•ç†ï¼šå¦‚æœæ²’æœ‰æå–æ–‡æœ¬ï¼Œè¨˜éŒ„è­¦å‘Š
            if not extracted_text or len(extracted_text.strip()) < 10:
                logger.warning(f"âš ï¸ æ–‡æª” {filename} æ²’æœ‰è¶³å¤ çš„æå–æ–‡æœ¬ï¼ˆé•·åº¦: {len(extracted_text)}ï¼‰")
                extracted_text = "[æ­¤æ–‡æª”æ²’æœ‰å¯ç”¨çš„æå–æ–‡æœ¬]"

            # æ§‹å»ºæ¸…æ™°çš„æ¨™é¡Œï¼Œä½¿ç”¨å¾ªç’°ç·¨è™Ÿï¼ˆcitation:iï¼‰
            doc_label = f"æ–‡æª”{i}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{i}ï¼‰"

            # âœ… ç›´æ¥æä¾›å®Œæ•´æ–‡æœ¬ï¼Œä¸ä½¿ç”¨ JSON æ ¼å¼
            context_parts.append(f"=== {doc_label}: {filename} ===\n\n{extracted_text}\n\n")

            logger.debug(f"æ·»åŠ æ–‡æª”ä¸Šä¸‹æ–‡: {doc_label}ï¼Œæ–‡æœ¬é•·åº¦: {len(extracted_text)} å­—ç¬¦")

        # èª¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼‰
        from app.core.config import settings
        try:
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=context_parts,
                db=db,
                user_id=user_id,
                detailed_text_max_length=settings.DETAIL_QUERY_MAX_CONTEXT_LENGTH,
                max_chars_per_doc=settings.DETAIL_QUERY_MAX_CHARS_PER_DOC
            )
            
            if ai_response.success and ai_response.output_data:
                return ai_response.output_data.answer_text
            else:
                return "æŠ±æ­‰ï¼Œç„¡æ³•å¾æ–‡æª”è©³ç´°æ•¸æ“šä¸­ç”Ÿæˆç­”æ¡ˆã€‚"
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}", exc_info=True)
            return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    async def generate_answer_from_details_stream(
        self,
        question: str,
        detailed_data: List[Dict],
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str],
        context: Optional[dict]
    ):
        """ä½¿ç”¨è©³ç´°æ•¸æ“šç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼ï¼‰"""
        
        # è¼‰å…¥å°è©±æ­·å²
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)

        # æ·»åŠ è©³ç´°æ•¸æ“šï¼ˆâœ… ç°¡åŒ–æ–¹æ¡ˆï¼šåªä½¿ç”¨ extracted_textï¼‰
        for i, data in enumerate(detailed_data, 1):
            filename = data.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            extracted_text = data.get('extracted_text', '')

            # å®¹éŒ¯è™•ç†ï¼šå¦‚æœæ²’æœ‰æå–æ–‡æœ¬ï¼Œè¨˜éŒ„è­¦å‘Š
            if not extracted_text or len(extracted_text.strip()) < 10:
                logger.warning(f"âš ï¸ æ–‡æª” {filename} æ²’æœ‰è¶³å¤ çš„æå–æ–‡æœ¬ï¼ˆé•·åº¦: {len(extracted_text)}ï¼‰")
                extracted_text = "[æ­¤æ–‡æª”æ²’æœ‰å¯ç”¨çš„æå–æ–‡æœ¬]"

            # æ§‹å»ºæ¸…æ™°çš„æ¨™é¡Œï¼Œä½¿ç”¨å¾ªç’°ç·¨è™Ÿï¼ˆcitation:iï¼‰
            doc_label = f"æ–‡æª”{i}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{i}ï¼‰"

            # âœ… ç›´æ¥æä¾›å®Œæ•´æ–‡æœ¬ï¼Œä¸ä½¿ç”¨ JSON æ ¼å¼
            context_parts.append(f"=== {doc_label}: {filename} ===\n\n{extracted_text}\n\n")

            logger.debug(f"æ·»åŠ æ–‡æª”ä¸Šä¸‹æ–‡: {doc_label}ï¼Œæ–‡æœ¬é•·åº¦: {len(extracted_text)} å­—ç¬¦")

        # èª¿ç”¨ AI æµå¼ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼‰
        from app.core.config import settings
        try:
            from app.services.ai.unified_ai_service_stream import generate_answer_stream

            async for chunk in generate_answer_stream(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=context_parts,
                model_preference=None,
                user_id=user_id,
                db=db,
                detailed_text_max_length=settings.DETAIL_QUERY_MAX_CONTEXT_LENGTH,
                max_chars_per_doc=settings.DETAIL_QUERY_MAX_CHARS_PER_DOC
            ):
                yield chunk
                
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}", exc_info=True)
            yield "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"


# å‰µå»ºå…¨å±€å¯¦ä¾‹
document_detail_query_handler = DocumentDetailQueryHandler()

