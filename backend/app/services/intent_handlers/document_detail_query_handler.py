"""
æ–‡æª”è©³ç´°æŸ¥è©¢è™•ç†å™¨

è™•ç†å°å·²çŸ¥æ–‡æª”çš„è©³ç´°æ•¸æ“šæŸ¥è©¢ï¼Œä½¿ç”¨ MongoDB è©³ç´°æŸ¥è©¢åŠŸèƒ½æå–ç²¾ç¢ºä¿¡æ¯
"""
import time
import logging
import json
import uuid
from typing import Optional, List, Dict, Any
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import (
    AIQARequest,
    AIQAResponse,
    QueryRewriteResult
)
from app.models.question_models import QuestionClassification
from app.models.ai_models_simplified import AIMongoDBQueryDetailOutput
from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified
from app.services.qa_workflow.conversation_helper import conversation_helper
from app.crud.crud_documents import get_documents_by_ids

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


def remove_projection_path_collisions(projection: dict) -> dict:
    """ç§»é™¤ MongoDB projection ä¸­çš„çˆ¶å­æ¬„ä½è¡çª"""
    if not projection or not isinstance(projection, dict):
        return projection
    keys = list(projection.keys())
    keys_to_remove = set()
    for k in keys:
        for other in keys:
            if k == other:
                continue
            if k.startswith(other + "."):
                keys_to_remove.add(other)
            elif other.startswith(k + "."):
                keys_to_remove.add(k)
    for k in keys_to_remove:
        projection.pop(k, None)
    return projection


def sanitize_for_json(obj: Any) -> Any:
    """æ¸…ç†æ•¸æ“šä¸­çš„ä¸å¯ JSON åºåˆ—åŒ–çš„å°è±¡ï¼ˆUUIDã€datetime ç­‰ï¼‰"""
    from datetime import datetime
    if isinstance(obj, dict):
        return {k: sanitize_for_json(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [sanitize_for_json(item) for item in obj]
    if isinstance(obj, uuid.UUID):
        return str(obj)
    if isinstance(obj, datetime):
        return obj.isoformat()
    return obj


class DocumentDetailQueryHandler:
    """æ–‡æª”è©³ç´°æŸ¥è©¢è™•ç†å™¨ - å°å·²çŸ¥æ–‡æª”åŸ·è¡Œ MongoDB ç²¾ç¢ºæŸ¥è©¢"""
    
    async def handle(
        self,
        request: AIQARequest,
        classification: QuestionClassification,
        context: Optional[dict],
        db: Optional[AsyncIOMotorDatabase] = None,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        è™•ç†æ–‡æª”è©³ç´°æŸ¥è©¢è«‹æ±‚
        
        æµç¨‹:
        1. å¾å°è©±ä¸Šä¸‹æ–‡ç²å–å·²çŸ¥çš„æ–‡æª”ID
        2. è«‹æ±‚ç”¨æˆ¶æ‰¹å‡†è©³ç´°æŸ¥è©¢
        3. ä½¿ç”¨ AI ç”Ÿæˆ MongoDB æŸ¥è©¢
        4. åŸ·è¡ŒæŸ¥è©¢ç²å–ç²¾ç¢ºæ•¸æ“š
        5. ç”Ÿæˆç­”æ¡ˆ
        """
        start_time = time.time()
        api_calls = 0
        
        logger.info(f"è™•ç†æ–‡æª”è©³ç´°æŸ¥è©¢: {request.question}")
        
        # æª¢æŸ¥å·¥ä½œæµæ“ä½œ
        workflow_action = getattr(request, 'workflow_action', None)
        
        # æ­¥é©Ÿ1: ç²å–å·²çŸ¥çš„æ–‡æª”IDï¼ˆå„ªå…ˆä½¿ç”¨å„ªå…ˆæ–‡æª”ï¼‰
        # å„ªå…ˆä½¿ç”¨çµ±ä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨æä¾›çš„å„ªå…ˆæ–‡æª”
        priority_doc_ids = context.get('priority_document_ids', []) if context else []
        cached_doc_ids = context.get('cached_document_ids', []) if context else []
        
        # å„ªå…ˆæ–‡æª”å„ªå…ˆç´šæ›´é«˜ï¼ˆåŸºæ–¼ç›¸é—œæ€§å’Œè¨ªå•é »ç‡ï¼‰
        available_doc_ids = priority_doc_ids if priority_doc_ids else cached_doc_ids
        
        if priority_doc_ids:
            logger.info(f"ğŸ¯ ä½¿ç”¨å„ªå…ˆæ–‡æª”: {len(priority_doc_ids)} å€‹ï¼ˆä¾†è‡ªæ–‡æª”æ± ï¼‰")
        elif cached_doc_ids:
            logger.info(f"å¾ä¸Šä¸‹æ–‡ç²å– {len(cached_doc_ids)} å€‹å·²çŸ¥æ–‡æª”ï¼ˆèˆŠæ–¹å¼ï¼‰")
        
        if not available_doc_ids:
            # æ²’æœ‰å·²çŸ¥æ–‡æª”ï¼Œé€€åŒ–ç‚º document_search
            logger.warning("æ²’æœ‰æ‰¾åˆ°å·²çŸ¥æ–‡æª”ï¼Œè½‰ç‚ºæ–‡æª”æœç´¢")
            from app.services.intent_handlers.document_search_handler import document_search_handler
            return await document_search_handler.handle(
                request, classification, context, db, user_id, request_id
            )
        
        # å¦‚æœç”¨æˆ¶é¸æ“‡è·³éï¼Œä½¿ç”¨æ‘˜è¦å›ç­”
        if workflow_action == 'skip_detail_query':
            logger.info("ç”¨æˆ¶è·³éè©³ç´°æŸ¥è©¢ï¼Œä½¿ç”¨æ–‡æª”æ‘˜è¦å›ç­”")
            # è½‰ç™¼çµ¦ simple_factual_handler ä½¿ç”¨æ‘˜è¦
            from app.services.intent_handlers.simple_factual_handler import simple_factual_handler
            return await simple_factual_handler.handle(
                request, classification, None, db, user_id, request_id
            )
        
        # æ­¥é©Ÿ2: å¦‚æœç”¨æˆ¶æœªæ‰¹å‡†ï¼Œç²å–ç›®æ¨™æ–‡æª”ç„¶å¾Œè«‹æ±‚æ‰¹å‡†
        if workflow_action != 'approve_detail_query' and not getattr(request, 'skip_classification', False):
            logger.info(f"ç²å–ç›®æ¨™æ–‡æª”ï¼ˆå…± {len(available_doc_ids)} å€‹å€™é¸ï¼‰")
            
            # å„ªå…ˆå¾åˆ†é¡çµæœç²å–ç›®æ¨™æ–‡æª” IDï¼ˆåˆ†é¡å™¨å·²ç¶“è­˜åˆ¥éäº†ï¼‰
            target_doc_ids = []
            if classification.target_document_ids:
                target_doc_ids = classification.target_document_ids
                logger.info(f"âœ… å¾åˆ†é¡å™¨ç›´æ¥ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹ï¼Œè·³éé‡è¤‡è­˜åˆ¥")
                if classification.target_document_reasoning:
                    logger.info(f"åˆ†é¡å™¨æ¨ç†: {classification.target_document_reasoning}")
            else:
                # å›é€€ï¼šå¦‚æœåˆ†é¡å™¨æ²’æœ‰è­˜åˆ¥ï¼Œå„ªå…ˆä½¿ç”¨å„ªå…ˆæ–‡æª”ï¼ˆæœ€å¤š3å€‹ï¼‰
                if priority_doc_ids:
                    logger.info("âœ… ä½¿ç”¨å„ªå…ˆæ–‡æª”ä½œç‚ºç›®æ¨™ï¼ˆæœ€ç›¸é—œçš„æ–‡æª”ï¼‰")
                    target_doc_ids = priority_doc_ids[:3]
                else:
                    logger.warning("âš ï¸ åˆ†é¡å™¨æœªæä¾›ç›®æ¨™æ–‡æª”ï¼Œä½¿ç”¨å‰3å€‹ç·©å­˜æ–‡æª”ä½œç‚ºå›é€€")
                    target_doc_ids = available_doc_ids[:3]
            
            processing_time = time.time() - start_time
            
            # ç²å–ç›®æ¨™æ–‡æª”åç¨±ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰
            doc_names = []
            try:
                documents = await get_documents_by_ids(db, target_doc_ids)
                doc_names = [doc.filename for doc in documents if hasattr(doc, 'filename')]
            except Exception as e:
                logger.warning(f"ç²å–æ–‡æª”åç¨±å¤±æ•—: {e}")
            
            return AIQAResponse(
                answer="",
                source_documents=[],
                confidence_score=0.0,
                tokens_used=0,
                processing_time=processing_time,
                query_rewrite_result=QueryRewriteResult(
                    original_query=request.question,
                    rewritten_queries=[request.question],
                    extracted_parameters={},
                    intent_analysis=classification.reasoning
                ),
                semantic_search_contexts=[],
                session_id=request.session_id,
                classification=classification,
                workflow_state={
                    "current_step": "awaiting_detail_query_approval",
                    "strategy_used": "document_detail_query",
                    "api_calls": 0,
                    "target_documents": target_doc_ids,  # åªé¡¯ç¤ºè­˜åˆ¥å‡ºçš„ç›®æ¨™æ–‡æª”
                    "document_names": doc_names,
                    "query_type": "è©³ç´°æ•¸æ“šæŸ¥è©¢",
                    "estimated_time": "2-4ç§’"
                },
                next_action="approve_detail_query",
                pending_approval="detail_query"
            )
        
        # ç”¨æˆ¶å·²æ‰¹å‡†ï¼ŒåŸ·è¡Œè©³ç´°æŸ¥è©¢
        logger.info("ç”¨æˆ¶å·²æ‰¹å‡†è©³ç´°æŸ¥è©¢ï¼Œé–‹å§‹åŸ·è¡Œ")
        
        # æ­¥é©Ÿ3: ç²å–ç›®æ¨™æ–‡æª”IDï¼ˆä¸‰ç¨®ä¾†æºï¼Œå„ªå…ˆç´šéæ¸›ï¼‰
        target_doc_ids = []
        
        # å„ªå…ˆç´š1: å¾åˆ†é¡çµæœç²å–ï¼ˆæœ€æº–ç¢ºï¼Œåˆ†é¡æ™‚å·²è­˜åˆ¥ï¼‰
        if classification.target_document_ids:
            target_doc_ids = classification.target_document_ids
            logger.info(f"âœ… å¾åˆ†é¡å™¨ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹ï¼ˆé¿å…é‡è¤‡è­˜åˆ¥ï¼‰")
        
        # å„ªå…ˆç´š2: å¾è«‹æ±‚åƒæ•¸ç²å–ï¼ˆå‰ç«¯å¯èƒ½å‚³å›ï¼‰
        elif hasattr(request, 'document_ids') and request.document_ids:
            target_doc_ids = request.document_ids
            logger.info(f"ğŸ“¥ å¾è«‹æ±‚åƒæ•¸ç²å–ç›®æ¨™æ–‡æª”: {len(target_doc_ids)} å€‹")
        
        # å„ªå…ˆç´š3: å›é€€æ–¹æ¡ˆ - ä½¿ç”¨å‰3å€‹å¯ç”¨æ–‡æª”ï¼ˆå„ªå…ˆæ–‡æª”æˆ–ç·©å­˜æ–‡æª”ï¼‰
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›®æ¨™æ–‡æª”IDï¼Œä½¿ç”¨å‰3å€‹å¯ç”¨æ–‡æª”ä½œç‚ºå›é€€")
            target_doc_ids = available_doc_ids[:3]
        
        # æ­¥é©Ÿ4: å‹•æ…‹è¼‰å…¥æ–‡æª” Schemaï¼ˆåˆä½µæ‰€æœ‰ç›®æ¨™æ–‡æª”çš„çµæ§‹ï¼‰
        logger.info(f"ğŸ“‹ å‹•æ…‹è¼‰å…¥ {len(target_doc_ids)} å€‹æ–‡æª”çš„ Schema...")
        
        # æ­¥é©Ÿ4.1: ç²å–æ‰€æœ‰ç›®æ¨™æ–‡æª”çš„çµæ§‹ï¼ˆåˆä½µæ¨¡å¼ï¼Œé¿å…éºæ¼ï¼‰
        actual_schema_fields = {}
        schema_by_document = {}  # è¨˜éŒ„æ¯å€‹æ–‡æª”æœ‰å“ªäº›æ¬„ä½
        
        if target_doc_ids:
            try:
                # æ‰¹é‡è¼•é‡ç´šæŸ¥è©¢ï¼šåªç²å–çµæ§‹ï¼Œä¸ç²å–å¤§é‡æ•¸æ“š
                # é™åˆ¶æœ€å¤šåˆ†æ 5 å€‹æ–‡æª”ï¼ˆé¿å…æ€§èƒ½å•é¡Œï¼‰
                sample_doc_ids = target_doc_ids[:5]
                
                cursor = db.documents.find(
                    {"_id": {"$in": sample_doc_ids}},
                    projection={
                        "_id": 1,
                        "filename": 1,
                        "analysis.ai_analysis_output.key_information": 1
                    }
                )
                
                sample_docs = await cursor.to_list(length=5)
                
                for doc in sample_docs:
                    doc_id = str(doc.get("_id"))
                    doc_filename = doc.get("filename", "æœªçŸ¥æ–‡æª”")
                    doc_fields = []
                    
                    if "analysis" in doc:
                        key_info = doc.get("analysis", {}).get("ai_analysis_output", {}).get("key_information", {})
                        
                        # æå– dynamic_fields çš„å¯¦éš›æ¬„ä½
                        if "dynamic_fields" in key_info and isinstance(key_info["dynamic_fields"], dict):
                            dynamic_fields = key_info["dynamic_fields"]
                            for field_name, field_value in dynamic_fields.items():
                                field_type = type(field_value).__name__
                                field_key = f"dynamic_fields.{field_name}"
                                
                                # åˆä½µåˆ°ç¸½ Schemaï¼ˆä½¿ç”¨ set é¿å…é‡è¤‡ï¼‰
                                if field_key not in actual_schema_fields:
                                    actual_schema_fields[field_key] = f"{field_name} ({field_type})"
                                
                                doc_fields.append(field_key)
                                
                        # æå– structured_entities çš„å¯¦éš›æ¬„ä½
                        if "structured_entities" in key_info and isinstance(key_info["structured_entities"], dict):
                            struct_entities = key_info["structured_entities"]
                            for entity_type in struct_entities.keys():
                                field_key = f"structured_entities.{entity_type}"
                                
                                if field_key not in actual_schema_fields:
                                    actual_schema_fields[field_key] = f"{entity_type} å¯¦é«”"
                                
                                doc_fields.append(field_key)
                    
                    # è¨˜éŒ„é€™å€‹æ–‡æª”æœ‰å“ªäº›æ¬„ä½
                    if doc_fields:
                        schema_by_document[doc_filename] = doc_fields
                
                logger.info(f"âœ… åˆä½µè¼‰å…¥äº† {len(actual_schema_fields)} å€‹å¯¦éš›æ¬„ä½ï¼ˆä¾†è‡ª {len(sample_docs)} å€‹æ–‡æª”ï¼‰")
                
                # æ—¥èªŒè¨˜éŒ„æ¯å€‹æ–‡æª”çš„å·®ç•°
                if len(schema_by_document) > 1:
                    logger.info(f"ğŸ“Š æ–‡æª”çµæ§‹å·®ç•°ï¼š{len(schema_by_document)} å€‹æ–‡æª”æœ‰ä¸åŒçš„æ¬„ä½çµ„åˆ")
                    for filename, fields in schema_by_document.items():
                        logger.debug(f"  - {filename}: {len(fields)} å€‹æ¬„ä½")
                        
            except Exception as e:
                logger.warning(f"âš ï¸ å‹•æ…‹ Schema è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é€šç”¨ Schema: {e}")
        
        # æ­¥é©Ÿ4.2: æº–å‚™æ–‡æª” Schema ä¿¡æ¯ï¼ˆçµåˆé€šç”¨ + å‹•æ…‹ï¼‰
        document_schema_info = {
            "description": "MongoDB æ–‡æª” Schema çµæ§‹ï¼ˆåŒ…å«å¯¦éš›æ¬„ä½ï¼‰",
            "required_fields": {
                "_id": "æ–‡æª”å”¯ä¸€ID",
                "filename": "æ–‡ä»¶å"
            },
            "content_fields": {
                "extracted_text": "OCRæå–çš„å®Œæ•´æ–‡æœ¬å…§å®¹"
            },
            "standard_analysis_fields": {
                "analysis.ai_analysis_output.key_information.content_summary": "å…§å®¹æ‘˜è¦",
                "analysis.ai_analysis_output.key_information.content_type": "æ–‡æª”é¡å‹",
                "analysis.ai_analysis_output.key_information.structured_entities": "çµæ§‹åŒ–å¯¦é«”ï¼ˆé‡‘é¡ã€æ—¥æœŸã€äººç‰©ç­‰ï¼‰",
                "analysis.ai_analysis_output.key_information.extracted_entities": "æå–çš„å¯¦é«”",
                "analysis.ai_analysis_output.key_information.auto_title": "è‡ªå‹•ç”Ÿæˆçš„æ¨™é¡Œ"
            },
            "recommendation": "å»ºè­°æŸ¥è©¢ç­–ç•¥ï¼š\n1. æœ€æ¨è–¦ï¼šæŸ¥è©¢å®Œæ•´çš„ analysis.ai_analysis_output.key_informationï¼ˆç¢ºä¿ä¸éºæ¼ï¼‰\n2. å¦‚éœ€ç‰¹å®šæ¬„ä½ï¼šæ ¹æ“šä¸‹é¢çš„å¯¦éš›æ¬„ä½é¸æ“‡"
        }
        
        # æ·»åŠ å¯¦éš›ç™¼ç¾çš„æ¬„ä½ï¼ˆå¦‚æœæœ‰ï¼‰
        if actual_schema_fields:
            document_schema_info["actual_fields_in_document"] = actual_schema_fields
            document_schema_info["recommendation"] += f"\n3. æ­¤æ–‡æª”åŒ…å« {len(actual_schema_fields)} å€‹å¯¦éš›æ¬„ä½ï¼Œå¯ç²¾ç¢ºæŸ¥è©¢"
        
        # æ­¥é©Ÿ5: å°é¸å®šçš„æ–‡æª”åŸ·è¡Œ MongoDB è©³ç´°æŸ¥è©¢
        all_detailed_data = []
        document_reference_map = {}  # ç”¨æ–¼ä¿å­˜æ–‡æª”IDåˆ°åƒè€ƒç·¨è™Ÿçš„æ˜ å°„
        
        # æ§‹å»ºæ–‡æª”IDåˆ°åƒè€ƒç·¨è™Ÿçš„æ˜ å°„ï¼ˆå¾å¯ç”¨æ–‡æª”åˆ—è¡¨ï¼Œå„ªå…ˆä½¿ç”¨å„ªå…ˆæ–‡æª”ï¼‰
        # ç¢ºä¿ key çµ±ä¸€ç‚ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä»¥ä¾¿å¾ŒçºŒæŸ¥æ‰¾
        for idx, doc_id in enumerate(available_doc_ids, 1):
            document_reference_map[str(doc_id)] = idx
        
        documents = await get_documents_by_ids(db, target_doc_ids)
        
        # éæ¿¾æ¬Šé™
        if user_id:
            from uuid import UUID
            user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
            documents = [doc for doc in documents if hasattr(doc, 'owner_id') and doc.owner_id == user_uuid]
        
        for doc in documents:
            logger.info(f"å°æ–‡æª” {doc.filename} åŸ·è¡Œè©³ç´°æŸ¥è©¢")
            
            ai_query_response = await unified_ai_service_simplified.generate_mongodb_detail_query(
                user_question=request.question,
                document_id=str(doc.id),
                document_schema_info=document_schema_info,
                db=db,
                model_preference=request.model_preference,
                user_id=user_id,
                session_id=request.session_id
            )
            api_calls += 1
            
            if ai_query_response.success and isinstance(ai_query_response.output_data, AIMongoDBQueryDetailOutput):
                query_components = ai_query_response.output_data
                
                mongo_filter = {"_id": doc.id}
                mongo_projection = query_components.projection
                
                if query_components.sub_filter:
                    mongo_filter.update(query_components.sub_filter)
                
                if mongo_projection or query_components.sub_filter:
                    safe_projection = remove_projection_path_collisions(mongo_projection) if mongo_projection else None
                    fetched_data = await db.documents.find_one(mongo_filter, projection=safe_projection)
                    
                    if fetched_data:
                        # è³‡æ–™æ¸…ç†
                        def sanitize(data: Any) -> Any:
                            if isinstance(data, dict):
                                return {k: sanitize(v) for k, v in data.items()}
                            if isinstance(data, list):
                                return [sanitize(i) for i in data]
                            if isinstance(data, uuid.UUID):
                                return str(data)
                            return data
                        
                        sanitized_data = sanitize(fetched_data)
                        
                        # æ·»åŠ å…ƒæ•¸æ“šï¼šåŸå§‹çš„åƒè€ƒç·¨è™Ÿï¼ˆæ–‡æª”å¹¾ï¼‰
                        doc_id_str = str(doc.id)
                        if doc_id_str in document_reference_map:
                            sanitized_data['_reference_number'] = document_reference_map[doc_id_str]
                        
                        all_detailed_data.append(sanitized_data)
                        logger.info(f"æˆåŠŸç²å–æ–‡æª” {doc.filename} çš„è©³ç´°æ•¸æ“š")
        
        # æ­¥é©Ÿ5: ä½¿ç”¨è©³ç´°æ•¸æ“šç”Ÿæˆç­”æ¡ˆ
        answer = await self._generate_answer_from_details(
            question=request.question,
            detailed_data=all_detailed_data,
            classification=classification,
            db=db,
            user_id=user_id,
            conversation_id=request.conversation_id,
            context=context
        )
        api_calls += 1
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜å°è©±
        if db is not None:
            # âœ… åˆä½µç›®æ¨™æ–‡æª” + ç”¨æˆ¶ @ çš„æ–‡ä»¶
            all_doc_ids = set(target_doc_ids)
            if request.document_ids:
                all_doc_ids.update(request.document_ids)
            
            await conversation_helper.save_qa_to_conversation(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                question=request.question,
                answer=answer,
                tokens_used=api_calls * 150,
                source_documents=list(all_doc_ids)
            )
        
        logger.info(f"è©³ç´°æŸ¥è©¢å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’, APIèª¿ç”¨: {api_calls}æ¬¡")
        
        # æ§‹å»ºåŒ…å«è©³ç´°æ•¸æ“šçš„ semantic_search_contexts
        from app.models.vector_models import SemanticContextDocument
        semantic_contexts = []
        for data in all_detailed_data:
            # æå–æ–‡æª”ä¿¡æ¯
            doc_filename = data.get('filename', 'æœªçŸ¥æ–‡æª”')
            reference_num = data.get('_reference_number', 0)
            
            # å‰µå»ºä¸€å€‹åŒ…å«è©³ç´°æ•¸æ“šçš„ context
            context_doc = SemanticContextDocument(
                document_id=str(data.get('_id', '')),
                summary_or_chunk_text=f"MongoDB æŸ¥è©¢çµæœï¼š{json.dumps(data, ensure_ascii=False, indent=2)}",
                similarity_score=1.0,
                metadata={
                    'source': 'mongodb_detail_query',
                    'filename': doc_filename,
                    'reference_number': reference_num,
                    'fields_count': len(data) - 2,  # æ’é™¤ _id å’Œ _reference_number
                    'detailed_data': data  # ä¿å­˜å®Œæ•´çš„è©³ç´°æ•¸æ“š
                }
            )
            semantic_contexts.append(context_doc)
        
        # â­ ä¿®å¾©ï¼šsource_documents çš„é †åºå¿…é ˆèˆ‡ AI çœ‹åˆ°çš„é †åºä¸€è‡´
        # AI çœ‹åˆ°çš„æ˜¯ all_detailed_data çš„é †åºï¼ˆæŒ‰ _reference_number æ’åºï¼‰
        # å¾ all_detailed_data ä¸­æå–æ–‡æª” IDï¼Œä¿æŒèˆ‡ AI çœ‹åˆ°çš„ç›¸åŒé †åº
        source_doc_ids_in_ai_order = []
        for data in all_detailed_data:
            doc_id = str(data.get('_id', ''))
            if doc_id and doc_id not in source_doc_ids_in_ai_order:
                source_doc_ids_in_ai_order.append(doc_id)
        
        # å¦‚æœæ²’æœ‰è©³ç´°æ•¸æ“šï¼Œä½¿ç”¨ target_doc_ids ä½œç‚º fallback
        if not source_doc_ids_in_ai_order:
            source_doc_ids_in_ai_order = target_doc_ids
        
        return AIQAResponse(
            answer=answer,
            source_documents=source_doc_ids_in_ai_order,  # â­ ä¿®å¾©ï¼šä½¿ç”¨èˆ‡ AI çœ‹åˆ°çš„ç›¸åŒé †åº
            confidence_score=0.90,
            tokens_used=api_calls * 150,
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={
                    "detail_query_count": len(all_detailed_data),
                    "target_document_count": len(target_doc_ids),
                    "total_fields": sum(len(data) - 2 for data in all_detailed_data)  # æ’é™¤ _id å’Œ _reference_number
                },
                intent_analysis=classification.reasoning
            ),
            semantic_search_contexts=semantic_contexts,  # åŒ…å«è©³ç´°æ•¸æ“š
            session_id=request.session_id,
            classification=classification,
            workflow_state={
                "current_step": "completed",
                "strategy_used": "document_detail_query",
                "api_calls": api_calls,
                "documents_queried": len(all_detailed_data),
                "target_documents": target_doc_ids,
                "mongodb_results": all_detailed_data  # åŒæ™‚ä¿ç•™åœ¨ workflow_state ä¸­
            },
            detailed_document_data_from_ai_query=all_detailed_data
        )
    
    async def _generate_answer_from_details(
        self,
        question: str,
        detailed_data: List[Dict],
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str],
        context: Optional[dict]
    ) -> str:
        """ä½¿ç”¨è©³ç´°æ•¸æ“šç”Ÿæˆç­”æ¡ˆï¼ˆéæµå¼ï¼‰"""
        
        # è¼‰å…¥å°è©±æ­·å²
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)
        
        # æ·»åŠ è©³ç´°æ•¸æ“š
        # â­â­ é—œéµä¿®å¾©ï¼šä½¿ç”¨å¾ªç’°ç·¨è™Ÿ iï¼ˆå¾ 1 é–‹å§‹ï¼‰ï¼Œè€Œä¸æ˜¯æ–‡æª”æ± ä¸­çš„ä½ç½®
        # é€™æ¨£ citation:1 å°±æœƒå°æ‡‰ç•¶å‰æŸ¥è©¢çš„ç¬¬ä¸€å€‹æ–‡æª”ï¼Œè€Œä¸æ˜¯æ–‡æª”æ± ä¸­çš„ç¬¬ N å€‹
        for i, data in enumerate(detailed_data, 1):
            # æ¸…ç†æ•¸æ“šä¸­çš„ UUID å’Œå…¶ä»–ä¸å¯åºåˆ—åŒ–çš„å°è±¡
            sanitized_data = sanitize_for_json(data)
            data_str = json.dumps(sanitized_data, ensure_ascii=False, indent=2)
            
            # â­ ä½¿ç”¨å¾ªç’°ç·¨è™Ÿ iï¼Œç¢ºä¿å¼•ç”¨ç·¨è™Ÿèˆ‡ source_documents é †åºä¸€è‡´
            filename = data.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            
            # æ§‹å»ºæ¸…æ™°çš„æ¨™é¡Œï¼Œä½¿ç”¨å¾ªç’°ç·¨è™Ÿï¼ˆä¸æ˜¯æ–‡æª”æ± ä½ç½®ï¼‰
            doc_label = f"æ–‡æª”{i}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{i}ï¼‰"
            context_parts.append(f"=== {doc_label}: {filename} çš„è©³ç´°æ•¸æ“š ===\n{data_str}\n")
            
            logger.debug(f"æ·»åŠ æ–‡æª”ä¸Šä¸‹æ–‡: {doc_label}")
        
        # èª¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆ
        try:
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=context_parts,
                db=db,
                user_id=user_id
            )
            
            if ai_response.success and ai_response.output_data:
                return ai_response.output_data.answer_text
            else:
                return "æŠ±æ­‰ï¼Œç„¡æ³•å¾æ–‡æª”è©³ç´°æ•¸æ“šä¸­ç”Ÿæˆç­”æ¡ˆã€‚"
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}", exc_info=True)
            return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    async def generate_answer_from_details_stream(
        self,
        question: str,
        detailed_data: List[Dict],
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str],
        context: Optional[dict]
    ):
        """ä½¿ç”¨è©³ç´°æ•¸æ“šç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼ï¼‰"""
        
        # è¼‰å…¥å°è©±æ­·å²
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)
        
        # æ·»åŠ è©³ç´°æ•¸æ“š
        # â­â­ é—œéµä¿®å¾©ï¼šä½¿ç”¨å¾ªç’°ç·¨è™Ÿ iï¼ˆå¾ 1 é–‹å§‹ï¼‰ï¼Œè€Œä¸æ˜¯æ–‡æª”æ± ä¸­çš„ä½ç½®
        # é€™æ¨£ citation:1 å°±æœƒå°æ‡‰ç•¶å‰æŸ¥è©¢çš„ç¬¬ä¸€å€‹æ–‡æª”ï¼Œè€Œä¸æ˜¯æ–‡æª”æ± ä¸­çš„ç¬¬ N å€‹
        for i, data in enumerate(detailed_data, 1):
            # æ¸…ç†æ•¸æ“šä¸­çš„ UUID å’Œå…¶ä»–ä¸å¯åºåˆ—åŒ–çš„å°è±¡
            sanitized_data = sanitize_for_json(data)
            data_str = json.dumps(sanitized_data, ensure_ascii=False, indent=2)
            
            # â­ ä½¿ç”¨å¾ªç’°ç·¨è™Ÿ iï¼Œç¢ºä¿å¼•ç”¨ç·¨è™Ÿèˆ‡ source_documents é †åºä¸€è‡´
            filename = data.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            
            # æ§‹å»ºæ¸…æ™°çš„æ¨™é¡Œï¼Œä½¿ç”¨å¾ªç’°ç·¨è™Ÿï¼ˆä¸æ˜¯æ–‡æª”æ± ä½ç½®ï¼‰
            doc_label = f"æ–‡æª”{i}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{i}ï¼‰"
            context_parts.append(f"=== {doc_label}: {filename} çš„è©³ç´°æ•¸æ“š ===\n{data_str}\n")
            
            logger.debug(f"æ·»åŠ æ–‡æª”ä¸Šä¸‹æ–‡: {doc_label}")
        
        # èª¿ç”¨ AI æµå¼ç”Ÿæˆç­”æ¡ˆ
        try:
            from app.services.ai.unified_ai_service_stream import generate_answer_stream
            
            async for chunk in generate_answer_stream(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=context_parts,
                model_preference=None,
                user_id=user_id,
                db=db
            ):
                yield chunk
                
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}", exc_info=True)
            yield "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"


# å‰µå»ºå…¨å±€å¯¦ä¾‹
document_detail_query_handler = DocumentDetailQueryHandler()

