"""
æ–‡æª”æœç´¢è™•ç†å™¨

è™•ç†æ¨™æº–çš„æ–‡æª”æœç´¢è«‹æ±‚,ä½¿ç”¨å…©éšæ®µæ··åˆæª¢ç´¢
"""
import time
import logging
from typing import Optional, List, Dict, Any
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import (
    AIQARequest,
    AIQAResponse,
    QueryRewriteResult,
    SemanticContextDocument,
    SemanticSearchResult
)
from app.models.question_models import QuestionClassification
from app.services.vector.enhanced_search_service import enhanced_search_service
from app.services.vector.embedding_service import embedding_service
from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified
from app.services.qa_workflow.conversation_helper import conversation_helper
from app.crud.crud_documents import get_documents_by_ids

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


class DocumentSearchHandler:
    """æ–‡æª”æœç´¢è™•ç†å™¨ - æ¨™æº–å…©éšæ®µæª¢ç´¢,2-3æ¬¡APIèª¿ç”¨"""
    
    async def handle(
        self,
        request: AIQARequest,
        classification: QuestionClassification,
        context: Optional[dict],
        db: Optional[AsyncIOMotorDatabase] = None,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        è™•ç†æ–‡æª”æœç´¢è«‹æ±‚
        
        ç­–ç•¥:
        1. å¯é¸è¼•é‡ç´šæŸ¥è©¢é‡å¯«(å¦‚æœç½®ä¿¡åº¦è¼ƒä½)
        2. è«‹æ±‚ç”¨æˆ¶æ‰¹å‡†æœç´¢(æ¼¸é€²å¼äº¤äº’)
        3. åŸ·è¡Œå…©éšæ®µæ··åˆæª¢ç´¢
        4. é¡¯ç¤ºæ‰¾åˆ°çš„æ–‡æª”ä¾›ç”¨æˆ¶ç¢ºèª
        5. ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            request: AI QA è«‹æ±‚
            classification: å•é¡Œåˆ†é¡çµæœ
            context: å°è©±ä¸Šä¸‹æ–‡
            db: æ•¸æ“šåº«é€£æ¥
            user_id: ç”¨æˆ¶ID
            request_id: è«‹æ±‚ID
            
        Returns:
            AIQAResponse: æ–‡æª”æœç´¢çµæœå’Œç­”æ¡ˆ
        """
        start_time = time.time()
        api_calls = 0
        
        logger.info(f"è™•ç†æ–‡æª”æœç´¢: {request.question}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¯©æ‰¹ç‹€æ…‹(å¾ workflow_action åƒæ•¸ç²å–)
        workflow_action = getattr(request, 'workflow_action', None)
        
        # å¦‚æœç”¨æˆ¶é¸æ“‡è·³éæœç´¢,ç›´æ¥ä½¿ç”¨é€šç”¨çŸ¥è­˜å›ç­”
        if workflow_action == 'skip_search':
            logger.info("ç”¨æˆ¶è·³éæ–‡æª”æœç´¢,ä½¿ç”¨é€šç”¨çŸ¥è­˜å›ç­”")
            return await self._handle_skip_search(
                request, classification, db, user_id, request_id, start_time
            )
        
        # Step 1: æª¢æŸ¥æ˜¯å¦éœ€è¦ç”¨æˆ¶æ‰¹å‡†ï¼ˆæ ¹æ“šé…ç½®å’Œç½®ä¿¡åº¦ï¼‰
        # ç­–ç•¥: æ‰€æœ‰document_searchéƒ½éœ€è¦æ‰¹å‡†,é™¤éç½®ä¿¡åº¦éå¸¸é«˜
        needs_approval = True
        auto_approve_threshold = 0.90  # åªæœ‰ç½®ä¿¡åº¦ >= 0.90 æ‰è‡ªå‹•æ‰¹å‡†
        
        if classification.confidence >= auto_approve_threshold:
            logger.info(f"ç½®ä¿¡åº¦{classification.confidence:.2f} >= {auto_approve_threshold},è‡ªå‹•æ‰¹å‡†æœç´¢")
            needs_approval = False
        
        # å¦‚æœéœ€è¦æ‰¹å‡†ä¸”ç”¨æˆ¶æœªæ‰¹å‡†,å…ˆè«‹æ±‚æ‰¹å‡†
        if needs_approval and workflow_action != 'approve_search' and not getattr(request, 'skip_classification', False):
            logger.info(f"è«‹æ±‚ç”¨æˆ¶æ‰¹å‡†æ–‡æª”æœç´¢ï¼ˆç½®ä¿¡åº¦:{classification.confidence:.2f}ï¼‰")
            processing_time = time.time() - start_time
            
            # æ§‹å»ºçµ¦ç”¨æˆ¶çœ‹çš„é è¦½ä¿¡æ¯ï¼ˆä¸ä½¿ç”¨æ­£å‰‡æå–ï¼Œè®“AIé‡å¯«è™•ç†ï¼‰
            # é¡¯ç¤ºAIçš„å®Œæ•´æ¨ç†ï¼Œè®“ç”¨æˆ¶äº†è§£AIçš„ç†è§£
            search_preview = {
                "original_question": request.question,
                "ai_understanding": "å°‡ä½¿ç”¨ AI æŸ¥è©¢é‡å¯«åˆ†æä¸Šä¸‹æ–‡ä¸¦å„ªåŒ–æœç´¢",
                "will_use_rewrite": True,
                "reasoning": classification.reasoning[:200] + "..." if len(classification.reasoning) > 200 else classification.reasoning
            }
            
            return AIQAResponse(
                answer="",  # æš«æ™‚ä¸ç”Ÿæˆç­”æ¡ˆ
                source_documents=[],
                confidence_score=0.0,
                tokens_used=0,
                processing_time=processing_time,
                query_rewrite_result=QueryRewriteResult(
                    original_query=request.question,
                    rewritten_queries=[request.question],
                    extracted_parameters=search_preview,
                    intent_analysis=classification.reasoning
                ),
                semantic_search_contexts=[],
                session_id=request.session_id,
                classification=classification,
                workflow_state={
                    "current_step": "awaiting_search_approval",
                    "strategy_used": "document_search",
                    "api_calls": 0,
                    "classification": classification.model_dump() if hasattr(classification, 'model_dump') else {},
                    "search_preview": search_preview,  # æ–°å¢ï¼šæœç´¢é è¦½ä¿¡æ¯
                    "estimated_documents": "æœªçŸ¥",
                    "estimated_time": "3-5ç§’"
                },
                next_action="approve_search",
                pending_approval="search"
            )
        
        # ç”¨æˆ¶å·²æ‰¹å‡†,ç¹¼çºŒåŸ·è¡Œæœç´¢
        logger.info("ç”¨æˆ¶å·²æ‰¹å‡†æ–‡æª”æœç´¢,é–‹å§‹åŸ·è¡Œ")
        
        # Step 2: æ™ºèƒ½æŸ¥è©¢é‡å¯«ï¼ˆç›´æ¥ä½¿ç”¨AIæ¨ç†å…§å®¹ï¼‰
        # ç­–ç•¥ï¼šè®“ AI æŸ¥è©¢é‡å¯«åŠŸèƒ½åˆ†æåŸå§‹å•é¡Œ+åˆ†é¡æ¨ç†ï¼Œè‡ªå‹•æå–æœ€ä½³æŸ¥è©¢
        
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰é å…ˆé‡å¯«çš„æŸ¥è©¢çµæœï¼ˆæ‰¹å‡†æ“ä½œæ™‚ç”± orchestrator æä¾›ï¼‰
        query_rewrite_result = None
        if context and 'pre_rewritten_query_result' in context:
            query_rewrite_result = context['pre_rewritten_query_result']
            logger.info(f"âœ… ä½¿ç”¨é å…ˆé‡å¯«çš„æŸ¥è©¢çµæœï¼ˆä¾†è‡ªæ‰¹å‡†æ“ä½œï¼‰ï¼ŒåŒ…å« {len(query_rewrite_result.rewritten_queries)} å€‹æŸ¥è©¢")
        else:
            # æ§‹å»ºçµ¦ AI æŸ¥è©¢é‡å¯«çš„è¼¸å…¥ï¼ˆåŒ…å«åŸå§‹å•é¡Œå’Œåˆ†é¡æ¨ç†ï¼‰
            if classification.reasoning and len(classification.reasoning) > 20:
                # æœ‰æ¨ç†å…§å®¹ï¼Œçµ„åˆåŸå§‹å•é¡Œå’ŒAIçš„ç†è§£
                query_for_rewrite = f"{request.question}ã€‚ä¸Šä¸‹æ–‡ç†è§£: {classification.reasoning[:300]}"
                logger.info(f"ğŸ“ æŸ¥è©¢é‡å¯«è¼¸å…¥: åŸå§‹å•é¡Œ + AIæ¨ç†å…§å®¹ï¼ˆ{len(classification.reasoning)}å­—ï¼‰")
            else:
                # æ²’æœ‰æ¨ç†å…§å®¹ï¼Œä½¿ç”¨åŸå§‹å•é¡Œ
                query_for_rewrite = request.question
                logger.info(f"ğŸ“ æŸ¥è©¢é‡å¯«è¼¸å…¥: åŸå§‹å•é¡Œï¼ˆç„¡æ¨ç†å…§å®¹ï¼‰")
            
            # æ­¥é©Ÿ2.2: åŸ·è¡Œæ™ºèƒ½æŸ¥è©¢é‡å¯«ï¼ˆAIæœƒè‡ªå‹•åˆ†ææ¨ç†å…§å®¹ï¼‰
            logger.info(f"ğŸ”„ åŸ·è¡Œæ™ºèƒ½æŸ¥è©¢é‡å¯«")
            
            query_rewrite_result = await self._lightweight_query_rewrite(
                query_for_rewrite,  # åŸå§‹å•é¡Œ + AIæ¨ç†å…§å®¹
                db,
                user_id,
                request.document_ids,  # âœ… ä¼ é€’ @ æ–‡ä»¶
                context  # âœ… ä¼ é€’æ–‡æ¡£æ± è¯¦ç»†ä¿¡æ¯
            )
            api_calls += 1
        
        # æ­¥é©Ÿ2.3: æ§‹å»ºæœ€çµ‚æŸ¥è©¢åˆ—è¡¨
        if query_rewrite_result and query_rewrite_result.rewritten_queries:
            # æŸ¥è©¢é‡å¯«æˆåŠŸï¼ˆä¸»è¦è·¯å¾‘ï¼‰
            queries_to_search = query_rewrite_result.rewritten_queries[:2]
            logger.info(f"âœ… æŸ¥è©¢é‡å¯«æˆåŠŸï¼Œæœ€çµ‚æŸ¥è©¢: {queries_to_search}")
        else:
            # æŸ¥è©¢é‡å¯«å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å•é¡Œï¼ˆé€€è·¯ï¼‰
            queries_to_search = [request.question]
            logger.warning(f"âš ï¸ æŸ¥è©¢é‡å¯«å¤±æ•—ï¼Œé€€è·¯: ä½¿ç”¨åŸå§‹å•é¡Œ: {request.question}")
            
            # æ‰‹å‹•æ§‹å»ºquery_rewrite_result
            query_rewrite_result = QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={"rewrite_failed": True},
                intent_analysis=classification.reasoning
            )
        
        # Step 2: åŸ·è¡Œå…©éšæ®µæ··åˆæª¢ç´¢ï¼ˆæ”¯æŒå„ªå…ˆæ–‡æª”ï¼‰
        # æª¢æŸ¥æ˜¯å¦æœ‰å„ªå…ˆæ–‡æª”ï¼ˆå¾çµ±ä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‚³éï¼‰
        priority_document_ids = context.get('priority_document_ids', []) if context else []
        should_reuse_cached = context.get('should_reuse_cached', False) if context else False
        
        if priority_document_ids and should_reuse_cached:
            logger.info(f"ğŸ¯ å„ªå…ˆå¾æ–‡æª”æ± æª¢ç´¢: {len(priority_document_ids)} å€‹æ–‡æª”")
            # å„ªå…ˆæª¢ç´¢æ–‡æª”æ± ä¸­çš„æ–‡æª”
            semantic_results = await self._perform_hybrid_search(
                db=db,
                queries=queries_to_search,
                top_k=request.context_limit or 5,
                user_id=user_id,
                document_ids=priority_document_ids  # ä½¿ç”¨å„ªå…ˆæ–‡æª”
            )
            
            # å¦‚æœå„ªå…ˆæ–‡æª”çµæœä¸å¤ å¥½ï¼Œå†æ“´å±•æœç´¢
            if not semantic_results or (semantic_results and max(r.similarity_score for r in semantic_results) < 0.6):
                logger.info("ğŸ“š å„ªå…ˆæ–‡æª”ç›¸é—œæ€§ä¸è¶³ï¼Œæ“´å±•åˆ°å…¨å±€æœç´¢")
                semantic_results = await self._perform_hybrid_search(
                    db=db,
                    queries=queries_to_search,
                    top_k=request.context_limit or 5,
                    user_id=user_id,
                    document_ids=request.document_ids  # å…¨å±€æœç´¢
                )
        else:
            # æ­£å¸¸æª¢ç´¢æµç¨‹
            semantic_results = await self._perform_hybrid_search(
                db=db,
                queries=queries_to_search,
                top_k=request.context_limit or 5,
                user_id=user_id,
                document_ids=request.document_ids
            )
        
        # Step 3: æº–å‚™èªç¾©æœç´¢ä¸Šä¸‹æ–‡
        semantic_contexts = []
        for result in semantic_results:
            semantic_contexts.append(
                SemanticContextDocument(
                    document_id=result.document_id,
                    summary_or_chunk_text=result.summary_text,
                    similarity_score=result.similarity_score,
                    metadata=result.metadata
                )
            )
        
        # Step 4: ç²å–æ–‡æª”è©³ç´°ä¿¡æ¯
        if not semantic_results:
            logger.warning("æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”,æä¾›éˆæ´»é¸é …")
            processing_time = time.time() - start_time
            
            # ä½¿ç”¨å·¥ä½œæµå”èª¿å™¨ç”Ÿæˆæ™ºèƒ½å»ºè­°
            from app.services.qa_workflow.workflow_coordinator import workflow_coordinator
            
            return await workflow_coordinator.handle_search_no_results(
                original_request=request,
                classification=classification,
                db=db,
                user_id=user_id,
                request_id=request_id
            )
        
        # å‰µå»ºæ–‡æª”IDåˆ°ç›¸é—œæ€§è©•åˆ†çš„æ˜ å°„
        # âš ï¸ æ³¨æ„ï¼šRRF èåˆæœç´¢æœƒç”¨ RRF åˆ†æ•¸è¦†è“‹ similarity_score
        # çœŸæ­£çš„å‘é‡ç›¸ä¼¼åº¦ä¿å­˜åœ¨ metadata["original_similarity"]
        doc_similarity_map = {}
        for result in semantic_results:
            # å„ªå…ˆä½¿ç”¨åŸå§‹å‘é‡ç›¸ä¼¼åº¦ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ similarity_score
            original_sim = result.metadata.get("original_similarity") if result.metadata else None
            similarity = original_sim if original_sim is not None else result.similarity_score
            doc_similarity_map[result.document_id] = similarity
            
        logger.info(f"ğŸ“Š ç›¸ä¼¼åº¦ä¾†æº: {'åŸå§‹å‘é‡ç›¸ä¼¼åº¦' if any(r.metadata and 'original_similarity' in r.metadata for r in semantic_results) else 'RRFåˆ†æ•¸'}")
        
        document_ids = [result.document_id for result in semantic_results]
        documents = await get_documents_by_ids(db, document_ids)
        
        # éæ¿¾ç”¨æˆ¶æœ‰æ¬Šé™çš„æ–‡æª”
        if user_id:
            from uuid import UUID
            user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
            documents = [
                doc for doc in documents
                if hasattr(doc, 'owner_id') and doc.owner_id == user_uuid
            ]
        
        if not documents:
            logger.warning("æ‰¾åˆ°æ–‡æª”ä½†ç”¨æˆ¶ç„¡æ¬Šè¨ªå•")
            processing_time = time.time() - start_time
            
            no_access_answer = "æ‰¾åˆ°äº†ç›¸é—œæ–‡æª”,ä½†æ‚¨å¯èƒ½æ²’æœ‰è¨ªå•æ¬Šé™ã€‚"
            
            # ä¿å­˜å°è©±è¨˜éŒ„(ç„¡æ¬Šé™æƒ…æ³)
            if db is not None:
                await conversation_helper.save_qa_to_conversation(
                    db=db,
                    conversation_id=request.conversation_id,
                    user_id=str(user_id) if user_id else None,
                    question=request.question,
                    answer=no_access_answer,
                    tokens_used=api_calls * 100,
                    source_documents=[]
                )
            
            return AIQAResponse(
                answer=no_access_answer,
                source_documents=[],
                confidence_score=0.3,
                tokens_used=api_calls * 100,
                processing_time=processing_time,
                query_rewrite_result=query_rewrite_result,
                semantic_search_contexts=semantic_contexts,
                session_id=request.session_id,
                classification=classification
            )
        
        # â­ éæ¿¾ä½ç›¸é—œæ€§æ–‡æª”ï¼ˆé¿å…æ±¡æŸ“æ–‡æª”æ± ï¼‰
        # é™ä½é–¾å€¼ï¼Œé¿å…éæ¿¾æ‰æœ‰ç”¨çš„æ–‡æª”ï¼ˆå¾ 0.55 é™åˆ° 0.45ï¼‰
        RELEVANCE_THRESHOLD = 0.45  # ç›¸é—œæ€§é–¾å€¼
        high_relevance_documents = [
            doc for doc in documents
            if doc_similarity_map.get(str(doc.id), 0) >= RELEVANCE_THRESHOLD
        ]
        
        # è¨˜éŒ„è©³ç´°çš„ç›¸ä¼¼åº¦ä¿¡æ¯
        if documents:
            similarity_scores = [doc_similarity_map.get(str(doc.id), 0) for doc in documents]
            logger.info(f"ğŸ“Š æ–‡æª”ç›¸ä¼¼åº¦åˆ†å¸ƒ: æœ€é«˜={max(similarity_scores):.3f}, æœ€ä½={min(similarity_scores):.3f}, å¹³å‡={sum(similarity_scores)/len(similarity_scores):.3f}")
        
        if high_relevance_documents:
            # ä½¿ç”¨é«˜ç›¸é—œæ€§æ–‡æª”
            logger.info(f"âœ… éæ¿¾å¾Œä¿ç•™ {len(high_relevance_documents)}/{len(documents)} å€‹é«˜ç›¸é—œæ€§æ–‡æª”ï¼ˆé–¾å€¼>={RELEVANCE_THRESHOLD}ï¼‰")
            documents_for_answer = high_relevance_documents
        else:
            # å¦‚æœæ‰€æœ‰æ–‡æª”ç›¸é—œæ€§éƒ½å¤ªä½ï¼Œä½¿ç”¨æœ€å¥½çš„2-3å€‹
            logger.warning(f"âš ï¸ æ‰€æœ‰æ–‡æª”ç›¸é—œæ€§éƒ½ä½æ–¼é–¾å€¼ {RELEVANCE_THRESHOLD}ï¼Œä½¿ç”¨top-3æ–‡æª”")
            documents_for_answer = documents[:3] if len(documents) >= 3 else documents
        
        # Step 5: ç”Ÿæˆç­”æ¡ˆ(ä½¿ç”¨æ‘˜è¦+éƒ¨åˆ†å…§å®¹)
        answer = await self._generate_answer_from_documents(
            question=request.question,
            documents=documents_for_answer,
            semantic_results=semantic_results,
            query_rewrite_result=query_rewrite_result,
            db=db,
            user_id=user_id,
            conversation_id=request.conversation_id,
            context=context
        )
        api_calls += 1
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜å°è©±è¨˜éŒ„ï¼ˆä½¿ç”¨éæ¿¾å¾Œçš„é«˜ç›¸é—œæ€§æ–‡æª”ï¼‰
        if db is not None:
            # âœ… åˆä½µæœç´¢çµæœ + ç”¨æˆ¶ @ çš„æ–‡ä»¶
            all_doc_ids = set()
            if documents_for_answer:
                all_doc_ids.update(str(doc.id) for doc in documents_for_answer)
            if request.document_ids:
                all_doc_ids.update(request.document_ids)
            
            await conversation_helper.save_qa_to_conversation(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                question=request.question,
                answer=answer,
                tokens_used=api_calls * 150,
                source_documents=list(all_doc_ids)
            )
        
        # è¨˜éŒ„æ—¥èªŒ
        if db is not None:
            await log_event(
                db=db,
                level=LogLevel.INFO,
                message="æ–‡æª”æœç´¢è™•ç†å®Œæˆ",
                source="handler.document_search",
                user_id=str(user_id) if user_id else None,
                request_id=request_id,
                details={
                    "question": request.question[:100],
                    "documents_found": len(documents),
                    "api_calls": api_calls,
                    "processing_time": processing_time
                }
            )
        
        logger.info(
            f"æ–‡æª”æœç´¢å®Œæˆ,è€—æ™‚: {processing_time:.2f}ç§’, "
            f"æ‰¾åˆ° {len(documents_for_answer)} å€‹é«˜ç›¸é—œæ€§æ–‡æª”, APIèª¿ç”¨: {api_calls}æ¬¡"
        )
        
        return AIQAResponse(
            answer=answer,
            source_documents=[str(doc.id) for doc in documents_for_answer],
            confidence_score=0.85,
            tokens_used=api_calls * 150,
            processing_time=processing_time,
            query_rewrite_result=query_rewrite_result,
            semantic_search_contexts=semantic_contexts,
            session_id=request.session_id,
            classification=classification,
            workflow_state={
                "current_step": "completed",
                "strategy_used": "document_search_hybrid",
                "api_calls": api_calls,
                "documents_found": len(documents)
            }
        )
    
    async def _lightweight_query_rewrite(
        self,
        question: str,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        document_ids: Optional[List[str]] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Optional[QueryRewriteResult]:
        """è¼•é‡ç´šæŸ¥è©¢é‡å¯«(ç”Ÿæˆ1-2å€‹è®Šé«”å³å¯)"""
        try:
            # âœ… å‡†å¤‡æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬æ–‡æ¡£æ‘˜è¦ä¿¡æ¯ï¼‰
            document_context = None
            if document_ids:
                logger.info(f"ğŸ¯ æŸ¥è¯¢é‡å†™ï¼šç”¨æˆ·é€‰æ‹©äº† {len(document_ids)} ä¸ªæ–‡ä»¶")
                
                # âœ… ä» context ä¸­è·å–æ–‡æ¡£æ± è¯¦ç»†ä¿¡æ¯ï¼ˆæ‘˜è¦ï¼‰
                document_summaries = []
                if context and 'cached_documents' in context:
                    for doc in context['cached_documents']:
                        doc_id = doc.get('document_id')
                        if doc_id in document_ids:
                            document_summaries.append({
                                'document_id': doc_id,
                                'filename': doc.get('filename', ''),
                                'summary': doc.get('summary', ''),
                                'key_concepts': doc.get('key_concepts', [])
                            })
                    logger.info(f"ğŸ“„ è·å–åˆ° {len(document_summaries)} ä¸ªæ–‡æ¡£æ‘˜è¦ç”¨äºæŸ¥è¯¢é‡å†™")
                
                document_context = {
                    "document_ids": document_ids,
                    "document_count": len(document_ids),
                    "document_summaries": document_summaries  # âœ… ä¼ é€’æ–‡æ¡£æ‘˜è¦
                }
            
            ai_response = await unified_ai_service_simplified.rewrite_query(
                original_query=question,
                db=db,
                user_id=user_id,
                document_context=document_context  # âœ… ä¼ é€’å®Œæ•´æ–‡æ¡£ä¸Šä¸‹æ–‡
            )
            
            if ai_response.success and ai_response.output_data:
                output = ai_response.output_data
                return QueryRewriteResult(
                    original_query=question,
                    rewritten_queries=output.rewritten_queries if hasattr(output, 'rewritten_queries') else [question],
                    extracted_parameters=output.extracted_parameters if hasattr(output, 'extracted_parameters') else {},
                    intent_analysis=output.intent_analysis if hasattr(output, 'intent_analysis') else "",
                    query_granularity=output.query_granularity if hasattr(output, 'query_granularity') else None,
                    search_strategy_suggestion=output.search_strategy_suggestion if hasattr(output, 'search_strategy_suggestion') else None,
                    reasoning=output.reasoning if hasattr(output, 'reasoning') else None
                )
        except Exception as e:
            logger.error(f"æŸ¥è©¢é‡å¯«å¤±æ•—: {e}", exc_info=True)
        
        return None
    
    async def _perform_hybrid_search(
        self,
        db: AsyncIOMotorDatabase,
        queries: List[str],
        top_k: int,
        user_id: Optional[str],
        document_ids: Optional[List[str]] = None
    ) -> List[SemanticSearchResult]:
        """åŸ·è¡Œå…©éšæ®µæ··åˆæª¢ç´¢"""
        
        all_results = {}
        
        for query in queries:
            try:
                # ä½¿ç”¨ enhanced_search_service çš„ RRF èåˆæœç´¢
                results = await enhanced_search_service.two_stage_hybrid_search(
                    db=db,
                    query=query,
                    user_id=str(user_id) if user_id else None,
                    search_type="rrf_fusion",
                    stage1_top_k=min(top_k * 2, 15),
                    stage2_top_k=top_k,
                    similarity_threshold=0.3
                )
                
                # åˆä½µçµæœ(å–æœ€é«˜åˆ†)
                for result in results:
                    if result.document_id not in all_results or result.similarity_score > all_results[result.document_id].similarity_score:
                        all_results[result.document_id] = result
                        
            except Exception as e:
                logger.error(f"æ··åˆæœç´¢å¤±æ•—(query: {query}): {e}", exc_info=True)
        
        # æ’åºä¸¦è¿”å›
        sorted_results = sorted(all_results.values(), key=lambda x: x.similarity_score, reverse=True)
        return sorted_results[:top_k]
    
    async def _generate_answer_from_documents(
        self,
        question: str,
        documents: list,
        semantic_results: list,
        query_rewrite_result: QueryRewriteResult,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str] = None,
        context: Optional[dict] = None
    ) -> str:
        """å¾æ–‡æª”ç”Ÿæˆç­”æ¡ˆ(å¸¶å°è©±æ­·å²)"""
        
        # ä½¿ç”¨çµ±ä¸€å·¥å…·è¼‰å…¥å°è©±æ­·å²
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = ""
        
        # å„ªå…ˆä½¿ç”¨å‚³å…¥çš„context
        if context and context.get('recent_messages'):
            # æ‰‹å‹•æ ¼å¼åŒ–contextä¸­çš„æ¶ˆæ¯ï¼ˆä¿ç•™å®Œæ•´å…§å®¹ï¼‰
            conversation_history_text = "=== å°è©±æ­·å² ===\n"
            for msg in context['recent_messages']:
                role_name = "ç”¨æˆ¶" if msg.get("role") == "user" else "åŠ©æ‰‹"
                content = msg.get("content", "")
                # ä¿ç•™å®Œæ•´å…§å®¹ï¼Œæœ€å¤š2000å­—
                if len(content) > 2000:
                    content = content[:2000] + "...[å…§å®¹è¼ƒé•·ï¼Œæ­¤è™•çœç•¥]"
                conversation_history_text += f"{role_name}: {content}\n"
            conversation_history_text += "=== ç•¶å‰å•é¡Œ ===\n"
            logger.info(f"document_searchä½¿ç”¨å‚³å…¥çš„{len(context['recent_messages'])}æ¢æ­·å²")
        else:
            # ä½¿ç”¨çµ±ä¸€å·¥å…·è¼‰å…¥ï¼ˆä¿ç•™å®Œæ•´å…§å®¹ï¼‰
            conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
                db=db,
                conversation_id=conversation_id,
                user_id=user_id,
                limit=5,
                max_content_length=2000  # ä¿ç•™å®Œæ•´å…§å®¹
            )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡(ä½¿ç”¨æ‘˜è¦+é—œéµä¿¡æ¯)
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)
        
        for i, doc in enumerate(documents[:5], 1):  # æœ€å¤š5å€‹æ–‡æª”
            doc_context = []
            doc_context.append(f"=== æ–‡æª”{i}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{i}ï¼‰: {getattr(doc, 'filename', 'Unknown')} ===")
            
            # å˜—è©¦ç²å–AIåˆ†æçµæœ
            if hasattr(doc, 'analysis') and doc.analysis:
                if hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                    key_info = doc.analysis.ai_analysis_output.get('key_information', {})
                    
                    # æ‘˜è¦
                    if key_info.get('content_summary'):
                        doc_context.append(f"æ‘˜è¦: {key_info['content_summary']}")
                    
                    # é—œéµæ¦‚å¿µ
                    if key_info.get('key_concepts'):
                        doc_context.append(f"é—œéµæ¦‚å¿µ: {', '.join(key_info['key_concepts'][:5])}")
                    
                    # ä¸»é¡Œ
                    if key_info.get('main_topics'):
                        doc_context.append(f"ä¸»é¡Œ: {', '.join(key_info['main_topics'][:3])}")
            
            # å¦‚æœæ²’æœ‰AIåˆ†æ,ä½¿ç”¨æå–çš„æ–‡æœ¬ç‰‡æ®µ
            if len(doc_context) == 1:  # åªæœ‰æ¨™é¡Œ
                matching_result = next(
                    (r for r in semantic_results if r.document_id == str(doc.id)),
                    None
                )
                if matching_result:
                    doc_context.append(matching_result.summary_text[:500])
            
            context_parts.append("\n".join(doc_context))
        
        # èª¿ç”¨AIç”Ÿæˆç­”æ¡ˆ(ä½¿ç”¨ç”¨æˆ¶åå¥½çš„æ¨¡å‹)
        try:
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=question,
                intent_analysis=query_rewrite_result.intent_analysis or "",
                document_context=context_parts,
                db=db,
                user_id=user_id,
                model_preference=None  # ä½¿ç”¨ç³»çµ±é…ç½®çš„ç”¨æˆ¶åå¥½æ¨¡å‹
            )
            
            if ai_response.success and ai_response.output_data:
                return ai_response.output_data.answer_text
            else:
                logger.error(f"AIç”Ÿæˆç­”æ¡ˆå¤±æ•—: {ai_response.error_message}")
                return "æŠ±æ­‰,æˆ‘ç„¡æ³•æ ¹æ“šæ‰¾åˆ°çš„æ–‡æª”ç”Ÿæˆæ»¿æ„çš„ç­”æ¡ˆã€‚"
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return "æŠ±æ­‰,ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    async def _handle_skip_search(
        self,
        request: AIQARequest,
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        request_id: Optional[str],
        start_time: float
    ) -> AIQAResponse:
        """è™•ç†ç”¨æˆ¶è·³éæ–‡æª”æœç´¢çš„æƒ…æ³,ä½¿ç”¨é€šç”¨çŸ¥è­˜å›ç­”"""
        
        try:
            # ä½¿ç”¨ AI åŸºæ–¼é€šç”¨çŸ¥è­˜å›ç­”
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=request.question,
                intent_analysis=classification.reasoning or "",
                document_context=[],  # ç©ºä¸Šä¸‹æ–‡
                db=db,
                user_id=user_id,
                model_preference=None
            )
            
            if ai_response.success and ai_response.output_data:
                answer = ai_response.output_data.answer_text
            else:
                answer = "æŠ±æ­‰,æˆ‘ç„¡æ³•åœ¨ä¸æŸ¥æ‰¾æ–‡æª”çš„æƒ…æ³ä¸‹å›ç­”é€™å€‹å•é¡Œã€‚å»ºè­°æ‚¨æ‰¹å‡†æ–‡æª”æœç´¢ä»¥ç²å¾—æ›´æº–ç¢ºçš„ç­”æ¡ˆã€‚"
                
        except Exception as e:
            logger.error(f"è·³éæœç´¢ç”Ÿæˆç­”æ¡ˆå¤±æ•—: {e}", exc_info=True)
            answer = "æŠ±æ­‰,ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜å°è©±è¨˜éŒ„
        if db is not None:
            await conversation_helper.save_qa_to_conversation(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                question=request.question,
                answer=answer,
                tokens_used=200,  # ä¼°ç®—
                source_documents=[]
            )
        
        query_rewrite_result = QueryRewriteResult(
            original_query=request.question,
            rewritten_queries=[request.question],
            extracted_parameters={},
            intent_analysis="ç”¨æˆ¶è·³éæ–‡æª”æœç´¢,ä½¿ç”¨é€šç”¨çŸ¥è­˜å›ç­”"
        )
        
        return AIQAResponse(
            answer=answer,
            source_documents=[],
            confidence_score=0.5,
            tokens_used=200,
            processing_time=processing_time,
            query_rewrite_result=query_rewrite_result,
            semantic_search_contexts=[],
            session_id=request.session_id,
            classification=classification,
            workflow_state={
                "current_step": "completed",
                "strategy_used": "skip_search_general_knowledge",
                "api_calls": 1
            }
        )


# å‰µå»ºå…¨å±€å¯¦ä¾‹
document_search_handler = DocumentSearchHandler()

