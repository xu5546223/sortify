"""
ç°¡å–®äº‹å¯¦æŸ¥è©¢è™•ç†å™¨

è™•ç†ç°¡å–®çš„äº‹å¯¦æŸ¥è©¢,åŸ·è¡Œè¼•é‡ç´šæœç´¢,å¿«é€Ÿå›ç­”
"""
import time
import logging
from typing import Optional
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import (
    AIQARequest, 
    AIQAResponse, 
    QueryRewriteResult,
    SemanticContextDocument
)
from app.models.question_models import QuestionClassification
from app.services.vector.embedding_service import embedding_service
from app.services.vector.vector_db_service import vector_db_service
from app.services.ai.unified_ai_service_simplified import (
    unified_ai_service_simplified,
    AIRequest,
    TaskType
)
from app.services.qa_workflow.conversation_helper import conversation_helper
from app.crud.crud_documents import get_documents_by_ids

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


class SimpleFactualHandler:
    """ç°¡å–®äº‹å¯¦æŸ¥è©¢è™•ç†å™¨ - è¼•é‡ç´šæœç´¢,2-3æ¬¡APIèª¿ç”¨"""
    
    async def handle(
        self,
        request: AIQARequest,
        classification: QuestionClassification,
        context: Optional[dict] = None,
        db: Optional[AsyncIOMotorDatabase] = None,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        è™•ç†ç°¡å–®äº‹å¯¦æŸ¥è©¢
        
        ç­–ç•¥:
        1. è·³éæŸ¥è©¢é‡å¯«(ç¯€çœ1æ¬¡APIèª¿ç”¨)
        2. åŸ·è¡Œå–®æ¬¡æ‘˜è¦å‘é‡æœç´¢
        3. ä½¿ç”¨æ‘˜è¦ç›´æ¥ç”Ÿæˆç­”æ¡ˆ
        4. ä¸åŸ·è¡Œè©³ç´°æ–‡æª”æŸ¥è©¢
        
        Args:
            request: AI QA è«‹æ±‚
            classification: å•é¡Œåˆ†é¡çµæœ
            context: å°è©±ä¸Šä¸‹æ–‡ï¼ˆæœªä½¿ç”¨ï¼‰
            db: æ•¸æ“šåº«é€£æ¥
            user_id: ç”¨æˆ¶ID
            request_id: è«‹æ±‚ID
            
        Returns:
            AIQAResponse: å¿«é€Ÿå›ç­”
        """
        start_time = time.time()
        api_calls = 0
        
        logger.info(f"è™•ç†ç°¡å–®äº‹å¯¦æŸ¥è©¢: {request.question}")
        
        # Step 1: ç”ŸæˆæŸ¥è©¢å‘é‡
        query_embedding = embedding_service.encode_text(request.question)
        if not query_embedding or not any(query_embedding):
            logger.error("ç„¡æ³•ç”ŸæˆæŸ¥è©¢çš„åµŒå…¥å‘é‡")
            return self._create_error_response(
                request, "ç„¡æ³•è™•ç†æ‚¨çš„å•é¡Œ,è«‹ç¨å¾Œå†è©¦",
                time.time() - start_time
            )
        
        # Step 2: åŸ·è¡Œæ‘˜è¦å‘é‡æœç´¢(åªæœç´¢æ‘˜è¦,ä¸æœç´¢æ–‡æœ¬ç‰‡æ®µ)
        try:
            summary_metadata_filter = {"type": "summary"}
            if request.document_ids:
                summary_metadata_filter["document_id"] = {"$in": request.document_ids}
            
            search_results = vector_db_service.search_similar_vectors(
                query_vector=query_embedding,
                top_k=min(5, request.context_limit or 5),  # é™åˆ¶æœç´¢æ•¸é‡
                owner_id_filter=str(user_id) if user_id else None,
                metadata_filter=summary_metadata_filter,
                similarity_threshold=0.4  # ç¨å¾®å¯¬é¬†çš„é–¾å€¼
            )
            
            logger.info(f"æ‘˜è¦æœç´¢æ‰¾åˆ° {len(search_results)} å€‹ç›¸é—œæ–‡æª”")
            
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±æ•—: {e}", exc_info=True)
            search_results = []
        
        # Step 3: æº–å‚™èªç¾©æœç´¢ä¸Šä¸‹æ–‡
        semantic_contexts = []
        for result in search_results:
            semantic_contexts.append(
                SemanticContextDocument(
                    document_id=result.document_id,
                    summary_or_chunk_text=result.summary_text,
                    similarity_score=result.similarity_score,
                    metadata=result.metadata
                )
            )
        
        # Step 4: å¦‚æœæ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡æª”,ç›´æ¥ç”¨AIå›ç­”
        if not search_results:
            logger.info("æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”,ä½¿ç”¨AIé€šç”¨çŸ¥è­˜å›ç­”")
            answer = await self._generate_answer_without_documents(
                request.question,
                classification,
                db,
                user_id,
                request.conversation_id  # å‚³é conversation_id
            )
            api_calls += 1
            
            processing_time = time.time() - start_time
            
            # ä¿å­˜å°è©±è¨˜éŒ„(ç„¡æ–‡æª”æƒ…æ³)
            if db is not None:
                await conversation_helper.save_qa_to_conversation(
                    db=db,
                    conversation_id=request.conversation_id,
                    user_id=str(user_id) if user_id else None,
                    question=request.question,
                    answer=answer,
                    tokens_used=api_calls * 100,
                    source_documents=[]
                )
            
            return AIQAResponse(
                answer=answer,
                source_documents=[],
                confidence_score=0.6,
                tokens_used=api_calls * 100,  # ä¼°ç®—
                processing_time=processing_time,
                query_rewrite_result=QueryRewriteResult(
                    original_query=request.question,
                    rewritten_queries=[request.question],
                    extracted_parameters={},
                    intent_analysis=f"ç°¡å–®äº‹å¯¦æŸ¥è©¢(ç„¡æ–‡æª”): {classification.reasoning}"
                ),
                semantic_search_contexts=semantic_contexts,
                session_id=request.session_id,
                classification=classification,
                workflow_state={
                    "current_step": "completed",
                    "strategy_used": "simple_factual_no_docs",
                    "api_calls": api_calls
                }
            )
        
        # Step 5: ç²å–æ–‡æª”è©³ç´°ä¿¡æ¯
        document_ids = [result.document_id for result in search_results[:3]]  # åªå–å‰3å€‹
        try:
            documents = await get_documents_by_ids(db, document_ids)
            
            # éæ¿¾ç”¨æˆ¶æœ‰æ¬Šé™çš„æ–‡æª”
            if user_id:
                from uuid import UUID
                user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
                documents = [
                    doc for doc in documents 
                    if hasattr(doc, 'owner_id') and doc.owner_id == user_uuid
                ]
            
        except Exception as e:
            logger.error(f"ç²å–æ–‡æª”å¤±æ•—: {e}", exc_info=True)
            documents = []
        
        # Step 6: ä½¿ç”¨æ‘˜è¦ç”Ÿæˆç­”æ¡ˆ(ä¸åšè©³ç´°æŸ¥è©¢)
        if documents:
            answer = await self._generate_answer_with_summaries(
                request.question,
                documents,
                search_results,
                classification,
                db,
                user_id,
                request.conversation_id  # å‚³é conversation_id
            )
            api_calls += 1
        else:
            answer = await self._generate_answer_without_documents(
                request.question,
                classification,
                db,
                user_id
            )
            api_calls += 1
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜å°è©±è¨˜éŒ„
        if db is not None:
            await conversation_helper.save_qa_to_conversation(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                question=request.question,
                answer=answer,
                tokens_used=api_calls * 100,
                source_documents=[str(doc.id) for doc in documents] if documents else []
            )
        
        # è¨˜éŒ„æ—¥èªŒ
        if db is not None:
            await log_event(
                db=db,
                level=LogLevel.INFO,
                message="ç°¡å–®äº‹å¯¦æŸ¥è©¢è™•ç†å®Œæˆ",
                source="handler.simple_factual",
                user_id=str(user_id) if user_id else None,
                request_id=request_id,
                details={
                    "question": request.question[:100],
                    "documents_found": len(documents),
                    "api_calls": api_calls,
                    "processing_time": processing_time
                }
            )
        
        logger.info(
            f"ç°¡å–®äº‹å¯¦æŸ¥è©¢å®Œæˆ,è€—æ™‚: {processing_time:.2f}ç§’, "
            f"APIèª¿ç”¨: {api_calls}æ¬¡"
        )
        
        return AIQAResponse(
            answer=answer,
            source_documents=[str(doc.id) for doc in documents],
            confidence_score=0.8,
            tokens_used=api_calls * 100,  # ä¼°ç®—
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={},
                intent_analysis=f"ç°¡å–®äº‹å¯¦æŸ¥è©¢: {classification.reasoning}"
            ),
            semantic_search_contexts=semantic_contexts,
            session_id=request.session_id,
            classification=classification,
            workflow_state={
                "current_step": "completed",
                "strategy_used": "simple_factual_with_summaries",
                "api_calls": api_calls,
                "documents_used": len(documents)
            }
        )
    
    async def _generate_answer_with_summaries(
        self,
        question: str,
        documents: list,
        search_results: list,
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str] = None
    ) -> str:
        """ä½¿ç”¨æ–‡æª”æ‘˜è¦ç”Ÿæˆç­”æ¡ˆ(å¸¶å°è©±æ­·å²)"""
        
        # ä½¿ç”¨çµ±ä¸€å·¥å…·è¼‰å…¥å°è©±æ­·å²ï¼ˆé‡è¦ï¼šä¿ç•™å®Œæ•´ä¿¡æ¯ï¼‰
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id,
            limit=5,  # å¢åŠ åˆ°5æ¢ï¼Œç¢ºä¿èƒ½çœ‹åˆ°ä¹‹å‰çš„å®Œæ•´å›ç­”
            max_content_length=2000  # ä¿ç•™å®Œæ•´å…§å®¹ï¼ˆç­”æ¡ˆå¯èƒ½åœ¨æ­·å²ä¸­ï¼‰
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡(åªä½¿ç”¨æ‘˜è¦,ä¸æŸ¥è©¢è©³ç´°å…§å®¹)
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)
        
        for i, doc in enumerate(documents[:3], 1):  # æœ€å¤š3å€‹æ–‡æª”
            # å˜—è©¦ç²å–AIåˆ†æçš„æ‘˜è¦
            summary = None
            if hasattr(doc, 'analysis') and doc.analysis:
                if hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                    key_info = doc.analysis.ai_analysis_output.get('key_information', {})
                    summary = key_info.get('content_summary')
            
            # å¦‚æœæ²’æœ‰æ‘˜è¦,ä½¿ç”¨æœç´¢çµæœä¸­çš„æ–‡æœ¬
            if not summary:
                matching_result = next(
                    (r for r in search_results if r.document_id == str(doc.id)),
                    None
                )
                if matching_result:
                    summary = matching_result.summary_text
            
            if summary:
                context_parts.append(
                    f"æ–‡æª”{i} ({getattr(doc, 'filename', 'Unknown')}):\n{summary}"
                )
        
        context_str = "\n\n".join(context_parts) if context_parts else "ç„¡ç›¸é—œæ–‡æª”å…§å®¹"
        
        # èª¿ç”¨AIç”Ÿæˆç­”æ¡ˆ(ä½¿ç”¨ç”¨æˆ¶åå¥½çš„æ¨¡å‹)
        try:
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=[context_str],
                db=db,
                user_id=user_id,
                model_preference=None  # ä½¿ç”¨ç³»çµ±é…ç½®çš„ç”¨æˆ¶åå¥½æ¨¡å‹
            )
            
            if ai_response.success and ai_response.output_data:
                return ai_response.output_data.answer_text
            else:
                logger.error(f"AIç”Ÿæˆç­”æ¡ˆå¤±æ•—: {ai_response.error_message}")
                return "æŠ±æ­‰,æˆ‘ç„¡æ³•æ ¹æ“šæ‰¾åˆ°çš„æ–‡æª”ç”Ÿæˆç­”æ¡ˆã€‚"
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return "æŠ±æ­‰,ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    async def _generate_answer_without_documents(
        self,
        question: str,
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str] = None
    ) -> str:
        """ä¸ä½¿ç”¨æ–‡æª”,ç›´æ¥ç”¨AIå›ç­”(åŸºæ–¼é€šç”¨çŸ¥è­˜,å¸¶å°è©±æ­·å²)"""
        
        # ä½¿ç”¨çµ±ä¸€å·¥å…·è¼‰å…¥å°è©±æ­·å²ï¼ˆé‡è¦ï¼šä¿ç•™å®Œæ•´ä¿¡æ¯ï¼‰
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id,
            limit=5,  # å¢åŠ åˆ°5æ¢
            max_content_length=2000  # ä¿ç•™å®Œæ•´å…§å®¹ï¼ˆç­”æ¡ˆå¯èƒ½åœ¨æ­·å²ä¸­ï¼‰
        )
        
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)
        context_parts.append("æ³¨æ„: ç”¨æˆ¶çš„æ–‡æª”åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œå…§å®¹,è«‹åŸºæ–¼ä½ çš„é€šç”¨çŸ¥è­˜ç°¡æ½”åœ°å›ç­”é€™å€‹å•é¡Œã€‚å¦‚æœå°è©±æ­·å²ä¸­å·²ç¶“åŒ…å«äº†ç­”æ¡ˆ,è«‹ç›´æ¥å¾æ­·å²ä¸­æå–å›ç­”ã€‚")
        
        try:
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=context_parts,
                db=db,
                user_id=user_id,
                model_preference=None  # ä½¿ç”¨ç³»çµ±é…ç½®çš„ç”¨æˆ¶åå¥½æ¨¡å‹
            )
            
            if ai_response.success and ai_response.output_data:
                answer = ai_response.output_data.answer_text
                # æ·»åŠ æç¤ºèªªæ˜é€™æ˜¯åŸºæ–¼é€šç”¨çŸ¥è­˜çš„å›ç­”
                return f"{answer}\n\nğŸ’¡ æç¤º: é€™å€‹å›ç­”åŸºæ–¼AIçš„é€šç”¨çŸ¥è­˜,æœªåœ¨æ‚¨çš„æ–‡æª”ä¸­æ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚"
            else:
                return "æŠ±æ­‰,æˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚æ‚¨å¯ä»¥å˜—è©¦ä¸Šå‚³ç›¸é—œæ–‡æª”æˆ–æ›å€‹æ–¹å¼æå•ã€‚"
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return "æŠ±æ­‰,ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    def _create_error_response(
        self,
        request: AIQARequest,
        error_message: str,
        processing_time: float
    ) -> AIQAResponse:
        """å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰"""
        return AIQAResponse(
            answer=error_message,
            source_documents=[],
            confidence_score=0.0,
            tokens_used=0,
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={},
                intent_analysis="è™•ç†å¤±æ•—"
            ),
            semantic_search_contexts=[],
            session_id=request.session_id,
            error_message=error_message
        )


# å‰µå»ºå…¨å±€å¯¦ä¾‹
simple_factual_handler = SimpleFactualHandler()

