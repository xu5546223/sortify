"""
ç°¡å–®äº‹å¯¦æŸ¥è©¢è™•ç†å™¨

è™•ç†ç°¡å–®çš„äº‹å¯¦æŸ¥è©¢,åŸ·è¡Œè¼•é‡ç´šæœç´¢,å¿«é€Ÿå›ç­”
"""
import time
import logging
from typing import Optional
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import (
    AIQARequest, 
    AIQAResponse, 
    QueryRewriteResult,
    SemanticContextDocument
)
from app.models.question_models import QuestionClassification
from app.services.vector.embedding_service import embedding_service
from app.services.vector.vector_db_service import vector_db_service
from app.services.ai.unified_ai_service_simplified import (
    unified_ai_service_simplified,
    AIRequest,
    TaskType
)
from app.services.qa_workflow.conversation_helper import conversation_helper
from app.crud.crud_documents import get_documents_by_ids

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


class SimpleFactualHandler:
    """ç°¡å–®äº‹å¯¦æŸ¥è©¢è™•ç†å™¨ - è¼•é‡ç´šæœç´¢,2-3æ¬¡APIèª¿ç”¨"""
    
    async def handle(
        self,
        request: AIQARequest,
        classification: QuestionClassification,
        context: Optional[dict] = None,
        db: Optional[AsyncIOMotorDatabase] = None,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        è™•ç†ç°¡å–®äº‹å¯¦æŸ¥è©¢
        
        çµ±ä¸€ç­–ç•¥ï¼ˆ2024å„ªåŒ–ç‰ˆï¼‰:
        1. **ä¸åŸ·è¡Œå‘é‡æœç´¢**ï¼ˆsimple_factual ä¸éœ€è¦æŸ¥æ‰¾æ–‡æª”ï¼‰
        2. **ç¸½æ˜¯è¼‰å…¥å°è©±æ­·å²**ï¼ˆæœ€è¿‘ 5 æ¢æ¶ˆæ¯ï¼‰
        3. **å¦‚æœæœ‰æ–‡æª”æ± ï¼Œæä¾›æ–‡æª”æ± æ‘˜è¦ä¿¡æ¯**
        4. ä½¿ç”¨ AI é€šç”¨çŸ¥è­˜ + å°è©±æ­·å² + æ–‡æª”æ± å›ç­”
        5. è·³éæŸ¥è©¢é‡å¯«ï¼ˆç¯€çœAPIèª¿ç”¨ï¼‰
        
        å„ªå‹¢:
        - AI èƒ½çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆæ­·å² + æ–‡æª”æ± ï¼‰
        - å›ç­”æ›´æº–ç¢ºå’Œç›¸é—œ
        - ç„¡éœ€å‘é‡æœç´¢ï¼Œå¿«é€ŸéŸ¿æ‡‰
        
        Args:
            request: AI QA è«‹æ±‚
            classification: å•é¡Œåˆ†é¡çµæœ
            context: å°è©±ä¸Šä¸‹æ–‡ï¼ˆæœªä½¿ç”¨ï¼‰
            db: æ•¸æ“šåº«é€£æ¥
            user_id: ç”¨æˆ¶ID
            request_id: è«‹æ±‚ID
            
        Returns:
            AIQAResponse: å¿«é€Ÿå›ç­”
        """
        start_time = time.time()
        api_calls = 0
        
        logger.info(f"è™•ç†ç°¡å–®äº‹å¯¦æŸ¥è©¢: {request.question}")
        logger.info("â­ Simple Factual ä¸åŸ·è¡Œå‘é‡æœç´¢ï¼Œä½¿ç”¨å°è©±æ­·å² + æ–‡æª”æ± ï¼ˆå¦‚æœ‰ï¼‰+ AI çŸ¥è­˜å›ç­”")
        
        # çµ±ä¸€ç­–ç•¥ï¼šç¸½æ˜¯è¼‰å…¥å°è©±æ­·å²å’Œæ–‡æª”æ± ä¿¡æ¯
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        # 1. è¼‰å…¥å°è©±æ­·å²ï¼ˆæœ€è¿‘ 5 æ¢ï¼‰
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=request.conversation_id,
            user_id=user_id,
            limit=5,
            max_content_length=2000
        )
        
        # 2. æ§‹å»ºæ–‡æª”æ± ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
        doc_pool_context = None
        cached_doc_data = context.get('cached_documents', []) if context else []  # âœ… ä¿®å¤ï¼šä½¿ç”¨ cached_documents
        
        if cached_doc_data:
            logger.info(f"æ–‡æª”æ± åŒ…å« {len(cached_doc_data)} å€‹æ–‡æª”ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡")
            doc_pool_context = "ğŸ“ ç•¶å‰æ–‡æª”æ± ä¸­çš„æ–‡ä»¶ï¼š\n\n"
            for idx, doc_info in enumerate(cached_doc_data, 1):
                filename = doc_info.get('filename', 'æœªçŸ¥æ–‡ä»¶')
                summary = doc_info.get('summary', 'ç„¡æ‘˜è¦')
                relevance = doc_info.get('relevance_score', 0)
                access_count = doc_info.get('access_count', 0)
                
                # âœ… ä½¿ç”¨ AI æœŸæœ›çš„å¼•ç”¨æ ¼å¼
                doc_pool_context += f"=== æ–‡æª”{idx}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{idx}ï¼‰: {filename} ===\n"
                doc_pool_context += f"ç›¸é—œæ€§: {relevance:.0%} | è¨ªå•æ¬¡æ•¸: {access_count}\n"
                if summary and summary != 'ç„¡æ‘˜è¦':
                    doc_pool_context += f"æ‘˜è¦: {summary}\n"
                doc_pool_context += "\n"
        
        # 3. æ§‹å»ºå®Œæ•´ä¸Šä¸‹æ–‡
        context_parts = []
        if conversation_history_text:
            context_parts.append(f"ğŸ“ å°è©±æ­·å²ï¼š\n{conversation_history_text}")
        if doc_pool_context:
            context_parts.append(doc_pool_context)
        
        # æ·»åŠ ç³»çµ±æç¤º
        if not cached_doc_data:
            context_parts.append("ğŸ’¡ æç¤ºï¼šæ–‡æª”æ± ç‚ºç©ºï¼Œè«‹åŸºæ–¼é€šç”¨çŸ¥è­˜å’Œå°è©±æ­·å²å›ç­”ã€‚")
        else:
            context_parts.append("ğŸ’¡ æç¤ºï¼šå¯ä»¥åƒè€ƒæ–‡æª”æ± ä¸­çš„æ–‡ä»¶ä¿¡æ¯ä¾†å›ç­”å•é¡Œã€‚\nâš ï¸ é‡è¦ï¼šæåŠæ–‡æª”æ™‚ï¼Œå¿…é ˆä½¿ç”¨ [æ–‡æª”å](citation:N) æ ¼å¼å‰µå»ºå¯é»æ“Šå¼•ç”¨ã€‚")
        
        # 4. ä½¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆ
        from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified
        
        ai_response = await unified_ai_service_simplified.generate_answer(
            user_question=request.question,
            intent_analysis="",
            document_context=context_parts,
            db=db,
            user_id=user_id
        )
        api_calls += 1
        
        # æå–ç­”æ¡ˆæ–‡æœ¬
        answer = ai_response.output_data.answer_text if ai_response.success and ai_response.output_data else "æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆç­”æ¡ˆã€‚"
        
        processing_time = time.time() - start_time
        
        # ä¿å­˜å°è©±è¨˜éŒ„ï¼ˆç„¡æ–‡æª”æƒ…æ³ï¼‰
        if db is not None:
            # âœ… å¦‚æœç”¨æˆ¶æä¾›äº† @ æ–‡ä»¶ï¼Œä¹Ÿè¦ä¿å­˜
            source_docs = request.document_ids if request.document_ids else []
            
            await conversation_helper.save_qa_to_conversation(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                question=request.question,
                answer=answer,
                tokens_used=api_calls * 100,
                source_documents=source_docs
            )
        
        logger.info(
            f"ç°¡å–®äº‹å¯¦æŸ¥è©¢å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’ï¼Œ"
            f"ä½¿ç”¨æ–‡æª”æ± : {len(cached_doc_data) > 0}ï¼Œ"
            f"APIèª¿ç”¨: {api_calls}æ¬¡"
        )
        
        return AIQAResponse(
            answer=answer,
            source_documents=[],
            confidence_score=0.85 if cached_doc_data else 0.75,  # æœ‰æ–‡æª”æ± æ™‚ç½®ä¿¡åº¦æ›´é«˜
            tokens_used=api_calls * 100,
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={},
                intent_analysis=f"ç°¡å–®äº‹å¯¦æŸ¥è©¢ï¼ˆçµ±ä¸€ç­–ç•¥ï¼‰: {classification.reasoning}"
            ),
            semantic_search_contexts=[],  # ç„¡å‘é‡æœç´¢
            session_id=request.session_id,
            classification=classification,
            workflow_state={
                "current_step": "completed",
                "strategy_used": "simple_factual_unified",
                "api_calls": api_calls,
                "skipped_vector_search": True,
                "used_conversation_history": bool(conversation_history_text),
                "used_document_pool": len(cached_doc_data) > 0,
                "document_pool_size": len(cached_doc_data)
            }
        )
    
    async def _generate_answer_from_document_pool(
        self,
        request: AIQARequest,
        context: dict,
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        start_time: float,
        api_calls: int
    ) -> AIQAResponse:
        """ç›´æ¥å¾æ–‡æª”æ± ä¿¡æ¯ç”Ÿæˆç­”æ¡ˆï¼ˆä¸åŸ·è¡Œå‘é‡æœç´¢ï¼‰"""
        
        # ç²å–æ–‡æª”æ± æ•¸æ“š
        cached_doc_data = context.get('cached_documents', [])  # âœ… ä¿®å¤ï¼šä½¿ç”¨ cached_documents
        
        logger.info(f"å¾æ–‡æª”æ± è¼‰å…¥äº† {len(cached_doc_data)} å€‹æ–‡æª”")
        
        # è¼‰å…¥å°è©±æ­·å²
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=request.conversation_id,
            user_id=user_id,
            limit=5,
            include_summary=False
        )
        
        # æ§‹å»ºæ–‡æª”æ± ä¸Šä¸‹æ–‡ï¼ˆæ ¼å¼åŒ–ç‚ºæ˜“è®€çš„æ–‡æœ¬ï¼‰
        doc_pool_context = "ç•¶å‰æ–‡æª”æ± ä¸­çš„æ–‡ä»¶ï¼š\n\n"
        for idx, doc_info in enumerate(cached_doc_data, 1):
            filename = doc_info.get('filename', 'æœªçŸ¥æ–‡ä»¶')
            summary = doc_info.get('summary', 'ç„¡æ‘˜è¦')
            relevance = doc_info.get('relevance_score', 0)
            access_count = doc_info.get('access_count', 0)
            
            # âœ… ä½¿ç”¨ AI æœŸæœ›çš„å¼•ç”¨æ ¼å¼
            doc_pool_context += f"=== æ–‡æª”{idx}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{idx}ï¼‰: {filename} ===\n"
            doc_pool_context += f"ç›¸é—œæ€§: {relevance:.0%} | è¨ªå•æ¬¡æ•¸: {access_count}\n"
            if summary and summary != 'ç„¡æ‘˜è¦':
                doc_pool_context += f"æ‘˜è¦: {summary}\n"
            doc_pool_context += "\n"
        
        # ä½¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆ
        from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified
        
        answer = await unified_ai_service_simplified.generate_answer(
            question=request.question,
            document_contexts=[doc_pool_context],
            conversation_history=conversation_history_text,
            user_id=user_id
        )
        
        api_calls += 1
        processing_time = time.time() - start_time
        
        logger.info(f"æ–‡æª”æ± ç¸½è¦½å›ç­”å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
        
        return AIQAResponse(
            answer=answer,
            source_documents=[],  # ä¸å¼•ç”¨ç‰¹å®šæ–‡æª”
            confidence_score=0.9,  # é«˜ç½®ä¿¡åº¦ï¼ˆä¿¡æ¯ä¾†è‡ªæ–‡æª”æ± ï¼‰
            tokens_used=api_calls * 100,
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={},
                intent_analysis=f"æ–‡æª”æ± ç¸½è¦½å•é¡Œ: {classification.reasoning}"
            ),
            semantic_search_contexts=[],
            session_id=request.session_id,
            classification=classification,
            workflow_state={
                "current_step": "completed",
                "strategy_used": "document_pool_overview",
                "api_calls": api_calls,
                "documents_in_pool": len(cached_doc_data),
                "skipped_vector_search": True
            }
        )
    
    async def _generate_answer_with_summaries(
        self,
        question: str,
        documents: list,
        search_results: list,
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str] = None
    ) -> str:
        """ä½¿ç”¨æ–‡æª”æ‘˜è¦ç”Ÿæˆç­”æ¡ˆ(å¸¶å°è©±æ­·å²)"""
        
        # ä½¿ç”¨çµ±ä¸€å·¥å…·è¼‰å…¥å°è©±æ­·å²ï¼ˆé‡è¦ï¼šä¿ç•™å®Œæ•´ä¿¡æ¯ï¼‰
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id,
            limit=5,  # å¢åŠ åˆ°5æ¢ï¼Œç¢ºä¿èƒ½çœ‹åˆ°ä¹‹å‰çš„å®Œæ•´å›ç­”
            max_content_length=2000  # ä¿ç•™å®Œæ•´å…§å®¹ï¼ˆç­”æ¡ˆå¯èƒ½åœ¨æ­·å²ä¸­ï¼‰
        )
        
        # æ§‹å»ºä¸Šä¸‹æ–‡(åªä½¿ç”¨æ‘˜è¦,ä¸æŸ¥è©¢è©³ç´°å…§å®¹)
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)
        
        for i, doc in enumerate(documents[:3], 1):  # æœ€å¤š3å€‹æ–‡æª”
            # å˜—è©¦ç²å–AIåˆ†æçš„æ‘˜è¦
            summary = None
            if hasattr(doc, 'analysis') and doc.analysis:
                if hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                    key_info = doc.analysis.ai_analysis_output.get('key_information', {})
                    summary = key_info.get('content_summary')
            
            # å¦‚æœæ²’æœ‰æ‘˜è¦,ä½¿ç”¨æœç´¢çµæœä¸­çš„æ–‡æœ¬
            if not summary:
                matching_result = next(
                    (r for r in search_results if r.document_id == str(doc.id)),
                    None
                )
                if matching_result:
                    summary = matching_result.summary_text
            
            if summary:
                context_parts.append(
                    f"æ–‡æª”{i} ({getattr(doc, 'filename', 'Unknown')}):\n{summary}"
                )
        
        context_str = "\n\n".join(context_parts) if context_parts else "ç„¡ç›¸é—œæ–‡æª”å…§å®¹"
        
        # èª¿ç”¨AIç”Ÿæˆç­”æ¡ˆ(ä½¿ç”¨ç”¨æˆ¶åå¥½çš„æ¨¡å‹)
        try:
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=[context_str],
                db=db,
                user_id=user_id,
                model_preference=None  # ä½¿ç”¨ç³»çµ±é…ç½®çš„ç”¨æˆ¶åå¥½æ¨¡å‹
            )
            
            if ai_response.success and ai_response.output_data:
                return ai_response.output_data.answer_text
            else:
                logger.error(f"AIç”Ÿæˆç­”æ¡ˆå¤±æ•—: {ai_response.error_message}")
                return "æŠ±æ­‰,æˆ‘ç„¡æ³•æ ¹æ“šæ‰¾åˆ°çš„æ–‡æª”ç”Ÿæˆç­”æ¡ˆã€‚"
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return "æŠ±æ­‰,ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    async def _generate_answer_without_documents(
        self,
        question: str,
        classification: QuestionClassification,
        db: Optional[AsyncIOMotorDatabase],
        user_id: Optional[str],
        conversation_id: Optional[str] = None
    ) -> str:
        """ä¸ä½¿ç”¨æ–‡æª”,ç›´æ¥ç”¨AIå›ç­”(åŸºæ–¼é€šç”¨çŸ¥è­˜,å¸¶å°è©±æ­·å²)"""
        
        # ä½¿ç”¨çµ±ä¸€å·¥å…·è¼‰å…¥å°è©±æ­·å²ï¼ˆé‡è¦ï¼šä¿ç•™å®Œæ•´ä¿¡æ¯ï¼‰
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        
        conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
            db=db,
            conversation_id=conversation_id,
            user_id=user_id,
            limit=5,  # å¢åŠ åˆ°5æ¢
            max_content_length=2000  # ä¿ç•™å®Œæ•´å…§å®¹ï¼ˆç­”æ¡ˆå¯èƒ½åœ¨æ­·å²ä¸­ï¼‰
        )
        
        context_parts = []
        if conversation_history_text:
            context_parts.append(conversation_history_text)
        context_parts.append("æ³¨æ„: ç”¨æˆ¶çš„æ–‡æª”åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œå…§å®¹,è«‹åŸºæ–¼ä½ çš„é€šç”¨çŸ¥è­˜ç°¡æ½”åœ°å›ç­”é€™å€‹å•é¡Œã€‚å¦‚æœå°è©±æ­·å²ä¸­å·²ç¶“åŒ…å«äº†ç­”æ¡ˆ,è«‹ç›´æ¥å¾æ­·å²ä¸­æå–å›ç­”ã€‚")
        
        try:
            ai_response = await unified_ai_service_simplified.generate_answer(
                user_question=question,
                intent_analysis=classification.reasoning,
                document_context=context_parts,
                db=db,
                user_id=user_id,
                model_preference=None  # ä½¿ç”¨ç³»çµ±é…ç½®çš„ç”¨æˆ¶åå¥½æ¨¡å‹
            )
            
            if ai_response.success and ai_response.output_data:
                answer = ai_response.output_data.answer_text
                # æ·»åŠ æç¤ºèªªæ˜é€™æ˜¯åŸºæ–¼é€šç”¨çŸ¥è­˜çš„å›ç­”
                return f"{answer}\n\nğŸ’¡ æç¤º: é€™å€‹å›ç­”åŸºæ–¼AIçš„é€šç”¨çŸ¥è­˜,æœªåœ¨æ‚¨çš„æ–‡æª”ä¸­æ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚"
            else:
                return "æŠ±æ­‰,æˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚æ‚¨å¯ä»¥å˜—è©¦ä¸Šå‚³ç›¸é—œæ–‡æª”æˆ–æ›å€‹æ–¹å¼æå•ã€‚"
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return "æŠ±æ­‰,ç”Ÿæˆç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚"
    
    def _create_error_response(
        self,
        request: AIQARequest,
        error_message: str,
        processing_time: float
    ) -> AIQAResponse:
        """å‰µå»ºéŒ¯èª¤éŸ¿æ‡‰"""
        return AIQAResponse(
            answer=error_message,
            source_documents=[],
            confidence_score=0.0,
            tokens_used=0,
            processing_time=processing_time,
            query_rewrite_result=QueryRewriteResult(
                original_query=request.question,
                rewritten_queries=[request.question],
                extracted_parameters={},
                intent_analysis="è™•ç†å¤±æ•—"
            ),
            semantic_search_contexts=[],
            session_id=request.session_id,
            error_message=error_message
        )


# å‰µå»ºå…¨å±€å¯¦ä¾‹
simple_factual_handler = SimpleFactualHandler()

