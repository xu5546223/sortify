"""
QA ç·¨æ’å™¨æœå‹™

è¼•é‡ç´šç·¨æ’å±¤ï¼Œçµ„åˆç¾æœ‰çš„æ¨¡å¡ŠåŒ–æœå‹™ï¼Œçµ±ä¸€é›»è…¦ç«¯å’Œæ‰‹æ©Ÿç«¯ QA é‚è¼¯ã€‚

è·è²¬ï¼š
- å”èª¿å•é¡Œåˆ†é¡ã€æœç´¢ã€ç­”æ¡ˆç”Ÿæˆç­‰æœå‹™
- å¯¦ç¾æ™ºèƒ½è·¯ç”±ï¼ˆæ ¹æ“šæ„åœ–åˆ†æ´¾åˆ°ä¸åŒè™•ç†å™¨ï¼‰
- å¯¦ç¾æ¨™æº– QA æµç¨‹
- ä¿æŒèˆ‡ç¾æœ‰ API çš„å‘å¾Œå…¼å®¹

é·ç§»è‡ª enhanced_ai_qa_serviceï¼Œä½†æ¡ç”¨çµ„åˆè€Œéç¹¼æ‰¿çš„è¨­è¨ˆæ¨¡å¼ã€‚
"""

import logging
import time
import json
import asyncio
from typing import Optional, List, Dict, Any, Tuple, AsyncGenerator
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import (
    AIQARequest, AIQAResponse, QueryRewriteResult, 
    SemanticSearchResult, SemanticContextDocument, LLMContextDocument
)
from app.models.question_models import QuestionIntent
from app.core.config import settings

# å°å…¥å·²æœ‰æœå‹™
from app.services.qa_core.qa_query_rewriter import qa_query_rewriter
from app.services.qa_core.qa_search_coordinator import qa_search_coordinator
from app.services.qa_core.qa_answer_service import qa_answer_service
from app.services.qa_workflow.question_classifier_service import question_classifier_service
from app.services.qa_workflow.context_loader_service import context_loader_service
from app.services.qa.utils.search_strategy import extract_search_strategy

# å°å…¥çµ±ä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨
from app.services.context.conversation_context_manager import (
    ConversationContextManager,
    ContextPurpose
)

# å°å…¥æ„åœ–è™•ç†å™¨
from app.services.intent_handlers.greeting_handler import greeting_handler
from app.services.intent_handlers.clarification_handler import clarification_handler
from app.services.intent_handlers.simple_factual_handler import simple_factual_handler
from app.services.intent_handlers.document_search_handler import document_search_handler
from app.services.intent_handlers.document_detail_query_handler import document_detail_query_handler
from app.services.intent_handlers.complex_analysis_handler import complex_analysis_handler

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


class StreamEvent:
    """æµå¼äº‹ä»¶ - ç”¨æ–¼æ‰‹æ©Ÿç«¯ SSE è¼¸å‡º"""
    
    def __init__(self, event_type: str, data: dict):
        self.type = event_type
        self.data = data
    
    def to_sse(self) -> str:
        """è½‰æ›ç‚º SSE æ ¼å¼"""
        event_data = {'type': self.type, **self.data}
        return f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"


class QAOrchestrator:
    """
    QA ç·¨æ’å™¨ - è¼•é‡ç´šå”èª¿å±¤
    
    ä½¿ç”¨çµ„åˆæ¨¡å¼æ•´åˆå·²æœ‰çš„æ¨¡å¡ŠåŒ–æœå‹™ï¼Œé¿å…é‡è¤‡å¯¦ç¾æ¥­å‹™é‚è¼¯ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç·¨æ’å™¨ï¼Œæ³¨å…¥å·²æœ‰æœå‹™"""
        # æ ¸å¿ƒæœå‹™ï¼ˆå·²å¯¦ä¾‹åŒ–çš„å…¨å±€æœå‹™ï¼‰
        self.query_rewriter = qa_query_rewriter
        self.search_coordinator = qa_search_coordinator
        self.answer_service = qa_answer_service
        self.classifier = question_classifier_service
        self.context_loader = context_loader_service
        
        # æ„åœ–è™•ç†å™¨æ˜ å°„
        self.intent_handlers = {
            QuestionIntent.GREETING: greeting_handler,
            QuestionIntent.CHITCHAT: greeting_handler,
            QuestionIntent.CLARIFICATION_NEEDED: clarification_handler,
            QuestionIntent.SIMPLE_FACTUAL: simple_factual_handler,
            QuestionIntent.DOCUMENT_SEARCH: document_search_handler,
            QuestionIntent.DOCUMENT_DETAIL_QUERY: document_detail_query_handler,
            QuestionIntent.COMPLEX_ANALYSIS: complex_analysis_handler,
        }
        
        # é…ç½®
        self.enable_intelligent_routing = getattr(settings, 'ENABLE_INTELLIGENT_ROUTING', True)
        
        logger.info(f"QA ç·¨æ’å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ™ºèƒ½è·¯ç”±: {self.enable_intelligent_routing}")
    
    async def process_qa_request_intelligent(
        self,
        db: AsyncIOMotorDatabase,
        request: AIQARequest,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        æ™ºèƒ½å•ç­”è™•ç†å…¥å£ - æ ¹æ“šå•é¡Œæ„åœ–å‹•æ…‹è·¯ç”±
        
        æµç¨‹:
        1. å¿«é€Ÿæ„åœ–åˆ†é¡
        2. æ ¹æ“šæ„åœ–è·¯ç”±åˆ°å°æ‡‰çš„è™•ç†å™¨
        3. å»¶é²è¼‰å…¥å¿…è¦çš„ä¸Šä¸‹æ–‡
        4. è¿”å›å„ªåŒ–çš„å›ç­”
        
        Args:
            db: æ•¸æ“šåº«é€£æ¥
            request: AI QA è«‹æ±‚
            user_id: ç”¨æˆ¶ID
            request_id: è«‹æ±‚ID
            
        Returns:
            AIQAResponse: å•ç­”éŸ¿æ‡‰
        """
        start_time = time.time()
        
        logger.info(f"ğŸš€ [ç·¨æ’å™¨] æ™ºèƒ½å•ç­”è«‹æ±‚: {request.question[:100]}...")
        
        # æª¢æŸ¥æ˜¯å¦è·³éæ™ºèƒ½è·¯ç”±
        if not self.enable_intelligent_routing or getattr(request, 'skip_classification', False):
            logger.info("æ™ºèƒ½è·¯ç”±å·²ç¦ç”¨æˆ–è¢«è·³é,ä½¿ç”¨æ¨™æº–æµç¨‹")
            return await self.process_qa_request(db, request, user_id, request_id)
        
        try:
            # Step 1: è¼‰å…¥å°è©±ä¸Šä¸‹æ–‡ï¼ˆç”¨æ–¼æ„åœ–åˆ†é¡ï¼‰
            from app.services.qa_workflow.unified_context_helper import unified_context_helper
            
            conversation_context = await unified_context_helper.load_conversation_history_list(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                limit=10
            )
            
            # Step 1.5: ç²å–ç·©å­˜æ–‡æª”ä¿¡æ¯ï¼ˆç”¨æ–¼åˆ†é¡ï¼‰
            cached_documents_info_for_classifier = None
            if request.conversation_id and user_id:
                cached_documents_info_for_classifier = await self._get_cached_documents_info(
                    db, request.conversation_id, user_id
                )
            
            # Step 2: å¿«é€Ÿæ„åœ–åˆ†é¡
            classification = await self.classifier.classify_question(
                question=request.question,
                conversation_history=conversation_context,
                has_cached_documents=bool(request.conversation_id),
                cached_documents_info=cached_documents_info_for_classifier,
                db=db,
                user_id=str(user_id) if user_id else None
            )
            
            logger.info(
                f"ğŸ“Š å•é¡Œåˆ†é¡å®Œæˆ: intent={classification.intent}, "
                f"confidence={classification.confidence:.2f}, "
                f"strategy={classification.suggested_strategy}"
            )
            
            # Step 3: æ ¹æ“šæ„åœ–è·¯ç”±åˆ°å°æ‡‰è™•ç†å™¨
            handler = self.intent_handlers.get(classification.intent)
            
            if handler:
                logger.info(f"â†’ è·¯ç”±åˆ°: {handler.__class__.__name__ if hasattr(handler, '__class__') else classification.intent}")
                
                # å»¶é²è¼‰å…¥ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                context = await self._load_context_if_needed(
                    db, request, user_id, classification
                )
                
                return await handler.handle(
                    request, classification, context, db, user_id, request_id
                )
            else:
                logger.warning(f"æœªçŸ¥çš„æ„åœ–é¡å‹: {classification.intent}, ä½¿ç”¨æ¨™æº–æµç¨‹")
                return await self.process_qa_request(db, request, user_id, request_id)
                
        except Exception as e:
            logger.error(f"æ™ºèƒ½è·¯ç”±è™•ç†å¤±æ•—,å›é€€åˆ°æ¨™æº–æµç¨‹: {e}", exc_info=True)
            
            # è¨˜éŒ„éŒ¯èª¤
            await log_event(
                db=db,
                level=LogLevel.ERROR,
                message=f"æ™ºèƒ½è·¯ç”±å¤±æ•—: {str(e)}",
                source="service.qa_orchestrator.intelligent_routing_error",
                user_id=str(user_id) if user_id else None,
                request_id=request_id,
                details={"error": str(e), "question": request.question[:100]}
            )
            
            # å›é€€åˆ°æ¨™æº–æµç¨‹
            return await self.process_qa_request(db, request, user_id, request_id)
    
    async def process_qa_request(
        self,
        db: AsyncIOMotorDatabase,
        request: AIQARequest,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        æ¨™æº– QA æµç¨‹ - ç°¡åŒ–ç‰ˆæœ¬ï¼Œå§”è¨—çµ¦å·²æœ‰æœå‹™
        
        æµç¨‹:
        1. æŸ¥è©¢é‡å¯«
        2. å‘é‡æœç´¢
        3. è™•ç†æ–‡æª”
        4. ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            db: æ•¸æ“šåº«é€£æ¥
            request: AI QA è«‹æ±‚
            user_id: ç”¨æˆ¶ID
            request_id: è«‹æ±‚ID
            
        Returns:
            AIQAResponse: å•ç­”éŸ¿æ‡‰
        """
        user_id_str = str(user_id) if user_id else None
        start_time = time.time()
        total_tokens = 0
        
        logger.info(f"ğŸ“ [ç·¨æ’å™¨] æ¨™æº– QA æµç¨‹: {request.question[:100]}...")
        
        await log_event(
            db=db, level=LogLevel.INFO,
            message="QA Orchestrator: Standard flow started",
            source="service.qa_orchestrator.process_qa_request",
            user_id=user_id_str, request_id=request_id,
            details={"question_length": len(request.question) if request.question else 0}
        )
        
        try:
            # Step 1: æŸ¥è©¢é‡å¯«
            query_rewrite_result, rewrite_tokens = await self.query_rewriter.rewrite_query(
                db=db,
                original_query=request.question,
                user_id=user_id_str,
                request_id=request_id,
                query_rewrite_count=getattr(request, 'query_rewrite_count', 3)
            )
            total_tokens += rewrite_tokens
            
            # Step 2: æ±ºå®šæœç´¢ç­–ç•¥
            search_strategy = extract_search_strategy(query_rewrite_result)
            logger.info(f"ğŸ¯ ä½¿ç”¨æœç´¢ç­–ç•¥: {search_strategy}")
            
            # Step 3: åŸ·è¡Œæœç´¢
            queries = query_rewrite_result.rewritten_queries if query_rewrite_result.rewritten_queries else [request.question]
            
            search_results = await self.search_coordinator.unified_search(
                db=db,
                queries=queries,
                user_id=user_id_str,
                search_strategy=search_strategy,
                top_k=getattr(request, 'max_documents_for_selection', request.context_limit),
                similarity_threshold=getattr(request, 'similarity_threshold', 0.3),
                enable_diversity_optimization=True,
                document_ids=request.document_ids if hasattr(request, 'document_ids') else None
            )
            
            # Step 4: è™•ç†æœç´¢çµæœ
            if not search_results:
                logger.warning("å‘é‡æœç´¢æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”")
                return AIQAResponse(
                    answer="æŠ±æ­‰ï¼Œæˆ‘åœ¨æ‚¨çš„æ–‡æª”åº«ä¸­æ²’æœ‰æ‰¾åˆ°èˆ‡æ‚¨å•é¡Œç›¸é—œçš„å…§å®¹ã€‚",
                    source_documents=[],
                    confidence_score=0.0,
                    tokens_used=total_tokens,
                    processing_time=time.time() - start_time,
                    query_rewrite_result=query_rewrite_result,
                    semantic_search_contexts=[],
                    session_id=request.session_id
                )
            
            # Step 5: æº–å‚™èªç¾©æœç´¢ä¸Šä¸‹æ–‡
            semantic_contexts_for_response: List[SemanticContextDocument] = []
            for res in search_results:
                semantic_contexts_for_response.append(
                    SemanticContextDocument(
                        document_id=res.document_id,
                        summary_or_chunk_text=res.summary_text,
                        similarity_score=res.similarity_score,
                        metadata=res.metadata
                    )
                )
            
            # Step 6: ç²å–å®Œæ•´æ–‡æª”
            from app.crud.crud_documents import get_documents_by_ids
            document_ids = [result.document_id for result in search_results]
            documents = await get_documents_by_ids(db, document_ids)
            
            if not documents:
                logger.warning("ç„¡æ³•ç²å–å®Œæ•´æ–‡æª”")
                return AIQAResponse(
                    answer="æŠ±æ­‰ï¼Œç„¡æ³•ç²å–ç›¸é—œæ–‡æª”çš„è©³ç´°å…§å®¹ã€‚",
                    source_documents=[],
                    confidence_score=0.0,
                    tokens_used=total_tokens,
                    processing_time=time.time() - start_time,
                    query_rewrite_result=query_rewrite_result,
                    semantic_search_contexts=semantic_contexts_for_response,
                    session_id=request.session_id
                )
            
            # Step 7: ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ qa_answer_serviceï¼‰
            # generate_answer è¿”å› Tuple[answer_text, tokens_used, confidence, contexts_used]
            # ğŸš€ å„ªåŒ–ï¼šå‚³éæœç´¢çµæœï¼Œè®“ AI èƒ½çœ‹åˆ°å…·é«”çš„ chunk å…§å®¹
            answer_text, answer_tokens, confidence, llm_contexts = await self.answer_service.generate_answer(
                db=db,
                original_query=request.question,
                documents_for_context=documents,
                query_rewrite_result=query_rewrite_result,
                detailed_document_data=None,  # æ¨™æº–æµç¨‹ä¸ä½¿ç”¨è©³ç´°æ•¸æ“š
                ai_generated_query_reasoning=None,  # æ¨™æº–æµç¨‹ä¸ä½¿ç”¨ AI æŸ¥è©¢æ¨ç†
                user_id=user_id_str,
                request_id=request_id,
                model_preference=request.model_preference,
                ensure_chinese_output=getattr(request, 'ensure_chinese_output', None),
                conversation_history=None,  # å¯ä»¥æ“´å±•æ”¯æŒ
                search_results=search_results  # ğŸš€ å‚³éæœç´¢çµæœ
            )
            
            total_tokens += answer_tokens
            
            # æå–ä¾†æºæ–‡æª” ID
            source_doc_ids = [ctx.document_id for ctx in llm_contexts] if llm_contexts else []
            
            # Step 8: æ§‹å»ºéŸ¿æ‡‰
            processing_time = time.time() - start_time
            
            response = AIQAResponse(
                answer=answer_text,
                source_documents=source_doc_ids,
                confidence_score=confidence,
                tokens_used=total_tokens,
                processing_time=processing_time,
                query_rewrite_result=query_rewrite_result,
                semantic_search_contexts=semantic_contexts_for_response,
                llm_context_documents=llm_contexts,
                session_id=request.session_id
            )
            
            logger.info(f"âœ… [ç·¨æ’å™¨] QA å®Œæˆï¼Œè€—æ™‚ {processing_time:.2f}sï¼Œtokens={total_tokens}")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ [ç·¨æ’å™¨] æ¨™æº–æµç¨‹å¤±æ•—: {e}", exc_info=True)
            
            await log_event(
                db=db, level=LogLevel.ERROR,
                message=f"QA Orchestrator failed: {str(e)}",
                source="service.qa_orchestrator.process_qa_request_error",
                user_id=user_id_str, request_id=request_id,
                details={"error": str(e)}
            )
            
            return AIQAResponse(
                answer=f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
                source_documents=[],
                confidence_score=0.0,
                tokens_used=total_tokens,
                processing_time=time.time() - start_time,
                query_rewrite_result=None,
                semantic_search_contexts=[],
                session_id=request.session_id
            )
    
    async def _get_cached_documents_info(
        self,
        db: AsyncIOMotorDatabase,
        conversation_id: str,
        user_id: str
    ) -> Optional[List[Dict]]:
        """ç²å–ç·©å­˜æ–‡æª”ä¿¡æ¯ç”¨æ–¼åˆ†é¡"""
        try:
            from app.crud import crud_conversations
            from uuid import UUID
            
            conversation_uuid = UUID(conversation_id)
            user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
            
            # ç²å–ç·©å­˜çš„æ–‡æª”ID
            cached_doc_ids, _ = await crud_conversations.get_cached_documents(
                db=db,
                conversation_id=conversation_uuid,
                user_id=user_uuid
            )
            
            if not cached_doc_ids:
                return None
            
            # ç²å–æ–‡æª”è©³ç´°ä¿¡æ¯
            from app.crud.crud_documents import get_documents_by_ids
            documents = await get_documents_by_ids(db, cached_doc_ids)
            
            # æ§‹å»ºæ–‡æª”ä¿¡æ¯åˆ—è¡¨
            cached_documents_info = []
            for idx, doc in enumerate(documents, 1):
                doc_info = {
                    "document_id": str(doc.id),
                    "filename": doc.filename,
                    "reference_number": idx,
                    "summary": ""
                }
                
                # å®‰å…¨ç²å–æ‘˜è¦
                try:
                    enriched_data = getattr(doc, 'enriched_data', None)
                    if enriched_data and isinstance(enriched_data, dict):
                        doc_info["summary"] = enriched_data.get('summary', '')
                    
                    if not doc_info["summary"] and hasattr(doc, 'analysis') and doc.analysis:
                        if hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                            key_info = doc.analysis.ai_analysis_output.get('key_information', {})
                            if isinstance(key_info, dict):
                                doc_info["summary"] = key_info.get('content_summary', '')
                except Exception as e:
                    logger.warning(f"ç²å–æ–‡æª” {idx} æ‘˜è¦å¤±æ•—: {e}")
                
                cached_documents_info.append(doc_info)
            
            logger.info(f"æº–å‚™äº† {len(cached_documents_info)} å€‹ç·©å­˜æ–‡æª”ä¿¡æ¯ç”¨æ–¼åˆ†é¡")
            return cached_documents_info
            
        except Exception as e:
            logger.warning(f"ç²å–ç·©å­˜æ–‡æª”ä¿¡æ¯å¤±æ•—: {e}")
            return None
    
    async def _load_context_if_needed(
        self,
        db: AsyncIOMotorDatabase,
        request: AIQARequest,
        user_id: Optional[str],
        classification
    ) -> Optional[dict]:
        """å»¶é²è¼‰å…¥å°è©±ä¸Šä¸‹æ–‡"""
        try:
            context = await self.context_loader.load_conversation_context_if_needed(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                requires_context=classification.requires_context
            )
            
            if context and context.recent_messages:
                logger.info(f"è¼‰å…¥äº† {len(context.recent_messages)} æ¢æ­·å²æ¶ˆæ¯")
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼(ä¿æŒå…¼å®¹æ€§)
            if context:
                return {
                    "conversation_id": context.conversation_id,
                    "recent_messages": context.recent_messages,
                    "cached_document_ids": context.cached_document_ids,
                    "cached_document_data": context.cached_document_data,
                    "message_count": context.message_count
                }
            
            return None
            
        except Exception as e:
            logger.warning(f"è¼‰å…¥ä¸Šä¸‹æ–‡å¤±æ•—,ç¹¼çºŒè™•ç†: {e}")
            return None


    async def process_qa_request_intelligent_stream(
        self,
        db: AsyncIOMotorDatabase,
        request: AIQARequest,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AsyncGenerator[StreamEvent, None]:
        """
        æ™ºèƒ½å•ç­”è™•ç† - æµå¼ç‰ˆæœ¬ï¼ˆæ‰‹æ©Ÿç«¯ï¼‰
        
        ä¿æŒèˆ‡ç¾æœ‰æ‰‹æ©Ÿç«¯å®Œå…¨ä¸€è‡´çš„äº‹ä»¶æ ¼å¼å’Œæµç¨‹ï¼š
        1. ç™¼é€é€²åº¦äº‹ä»¶ï¼ˆåˆ†é¡ã€æœç´¢ç­‰ï¼‰
        2. åœ¨ç­”æ¡ˆç”Ÿæˆéšæ®µä½¿ç”¨ generate_answer_stream() çœŸå¯¦æµå¼è¼¸å‡º
        3. æ”¯æŒæ‰¹å‡†æµç¨‹ï¼ˆapproval_neededï¼‰
        4. æ”¯æŒæ¾„æ¸…è™•ç†ï¼ˆclarification_textï¼‰
        
        Yields:
            StreamEvent: æµå¼äº‹ä»¶ï¼ˆprogress, chunk, metadata, complete, error, approval_neededï¼‰
        """
        try:
            # æª¢æŸ¥æ˜¯å¦æ˜¯æ‰¹å‡†æ“ä½œï¼ˆæ‰¹å‡†å¾Œä¸ç™¼é€é‡è¤‡çš„é€²åº¦äº‹ä»¶ï¼‰
            is_approval_action = getattr(request, 'workflow_action', None) in [
                'approve_search', 'skip_search', 
                'approve_detail_query', 'skip_detail_query'
            ]
            
            # === ç™¼é€é–‹å§‹äº‹ä»¶ï¼ˆæ‰¹å‡†æ“ä½œè·³éï¼‰===
            if not is_approval_action:
                yield StreamEvent('progress', {
                    'stage': 'start',
                    'message': 'ğŸš€ é–‹å§‹è™•ç†æ‚¨çš„å•é¡Œ...'
                })
                await asyncio.sleep(0.05)
            
            # === æ­¥é©Ÿ 1: å‰µå»ºçµ±ä¸€ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸¦è¼‰å…¥ä¸Šä¸‹æ–‡ ===
            context_manager = None
            conversation_context = None
            cached_documents_info_for_classifier = None
            
            if request.conversation_id and user_id:
                try:
                    # å‰µå»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨
                    context_manager = ConversationContextManager(
                        db=db,
                        conversation_id=request.conversation_id,
                        user_id=str(user_id)
                    )
                    
                    # ç‚ºæ„åœ–åˆ†é¡è¼‰å…¥ä¸Šä¸‹æ–‡
                    classification_context = await context_manager.load_context(
                        purpose=ContextPurpose.CLASSIFICATION,
                        max_history_messages=10
                    )
                    
                    conversation_context = classification_context.conversation_history_list
                    cached_documents_info_for_classifier = classification_context.cached_documents_info
                    
                    if conversation_context:
                        logger.info(f"âœ… çµ±ä¸€ç®¡ç†å™¨è¼‰å…¥: {len(conversation_context)} æ¢æ¶ˆæ¯, {len(cached_documents_info_for_classifier or [])} å€‹æ–‡æª”æ± ")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ çµ±ä¸€ç®¡ç†å™¨åˆå§‹åŒ–å¤±æ•—ï¼Œå›é€€åˆ°èˆŠæ–¹å¼: {e}")
                    # å›é€€åˆ°èˆŠæ–¹å¼
                    from app.services.qa_workflow.unified_context_helper import unified_context_helper
                    
                    conversation_context = await unified_context_helper.load_conversation_history_list(
                        db=db,
                        conversation_id=request.conversation_id,
                        user_id=str(user_id) if user_id else None,
                        limit=10
                    )
                    cached_documents_info_for_classifier = await self._get_cached_documents_info(
                        db, request.conversation_id, user_id
                    )
            
            # === æ­¥é©Ÿ 1.5: è™•ç†æ¾„æ¸…å›ç­” ===
            effective_question = request.question
            clarification_provided = False
            if getattr(request, 'workflow_action', None) == 'provide_clarification' and getattr(request, 'clarification_text', None):
                logger.info(f"ğŸ“ æ”¶åˆ°æ¾„æ¸…å›ç­”: {request.clarification_text}")
                clarification_provided = True
                
                # ä¿å­˜æ¾„æ¸…å›ç­”åˆ°å°è©±
                if request.conversation_id and context_manager:
                    try:
                        # ä½¿ç”¨çµ±ä¸€ç®¡ç†å™¨é‡æ–°è¼‰å…¥ï¼ˆä¿å­˜æ“ä½œæœƒåœ¨å¾Œé¢çµ±ä¸€è™•ç†ï¼‰
                        # é€™è£¡åªæ˜¯é‡æ–°è¼‰å…¥ä¸Šä¸‹æ–‡ä»¥åŒ…å«æ¾„æ¸…å›ç­”
                        from app.crud import crud_conversations
                        from uuid import UUID
                        
                        conversation_uuid = UUID(request.conversation_id)
                        user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
                        
                        await crud_conversations.add_message_to_conversation(
                            db=db,
                            conversation_id=conversation_uuid,
                            user_id=user_uuid,
                            role="user",
                            content=request.clarification_text,
                            tokens_used=None
                        )
                        
                        # ä½¿çµ±ä¸€ç®¡ç†å™¨çš„ç·©å­˜å¤±æ•ˆ
                        context_manager._cache_loaded = False
                        context_manager._message_cache = None
                        
                        # é‡æ–°è¼‰å…¥ä¸Šä¸‹æ–‡
                        classification_context = await context_manager.load_context(
                            purpose=ContextPurpose.CLASSIFICATION,
                            max_history_messages=10
                        )
                        conversation_context = classification_context.conversation_history_list
                        
                        logger.info(f"âœ… æ¾„æ¸…å›ç­”å·²ä¿å­˜ï¼Œé‡æ–°è¼‰å…¥äº† {len(conversation_context or [])} æ¢æ¶ˆæ¯")
                    except Exception as e:
                        logger.error(f"âŒ è™•ç†æ¾„æ¸…å›ç­”å¤±æ•—: {e}")
                
                effective_question = f"{request.question} â†’ {request.clarification_text}"
            
            # === æ­¥é©Ÿ 1.8: ä¿å­˜ @ æ–‡ä»¶åˆ°æ–‡æª”æ± ï¼ˆåœ¨åˆ†é¡ä¹‹å‰ï¼‰===
            if request.document_ids and request.conversation_id and context_manager:
                try:
                    logger.info(f"ğŸ“ åœ¨åˆ†é¡å‰ä¿å­˜ @ æ–‡ä»¶åˆ°æ–‡æª”æ± : {len(request.document_ids)} å€‹")
                    await context_manager._update_document_pool(
                        new_document_ids=request.document_ids
                    )
                    
                    # é‡æ–°è¼‰å…¥ä¸Šä¸‹æ–‡ï¼Œä»¥åŒ…å«æ–°æ·»åŠ çš„æ–‡æª”
                    classification_context = await context_manager.load_context(
                        purpose=ContextPurpose.CLASSIFICATION,
                        max_history_messages=10
                    )
                    cached_documents_info_for_classifier = classification_context.cached_documents_info
                    logger.info(f"âœ… æ–‡æª”æ± å·²æ›´æ–°ï¼Œç¾æœ‰ {len(cached_documents_info_for_classifier or [])} å€‹æ–‡æª”")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜ @ æ–‡ä»¶åˆ°æ–‡æª”æ± å¤±æ•—: {e}")
            
            # === æ­¥é©Ÿ 2: å•é¡Œåˆ†é¡ï¼ˆæ‰¹å‡†æ“ä½œè·³éé€²åº¦ç™¼é€ï¼‰===
            if not is_approval_action:
                yield StreamEvent('progress', {
                    'stage': 'classifying',
                    'message': 'ğŸ¯ AI æ­£åœ¨åˆ†æå•é¡Œæ„åœ–...'
                })
                await asyncio.sleep(0.05)
            
            classification = await self.classifier.classify_question(
                question=effective_question,
                conversation_history=conversation_context,
                has_cached_documents=bool(request.conversation_id),
                cached_documents_info=cached_documents_info_for_classifier,
                db=db,
                user_id=str(user_id) if user_id else None
            )
            
            # ç™¼é€åˆ†é¡çµæœï¼ˆæ‰¹å‡†æ“ä½œè·³éï¼‰
            if not is_approval_action:
                intent_label = {
                    QuestionIntent.GREETING: 'å¯’æš„',
                    QuestionIntent.CHITCHAT: 'é–’èŠ',
                    QuestionIntent.DOCUMENT_SEARCH: 'æ–‡æª”æœç´¢',
                    QuestionIntent.SIMPLE_FACTUAL: 'ç°¡å–®æŸ¥è©¢',
                    QuestionIntent.COMPLEX_ANALYSIS: 'è¤‡é›œåˆ†æ',
                    QuestionIntent.CLARIFICATION_NEEDED: 'éœ€è¦æ¾„æ¸…',
                    QuestionIntent.DOCUMENT_DETAIL_QUERY: 'MongoDB è©³ç´°æŸ¥è©¢'
                }.get(classification.intent, str(classification.intent))
                
                yield StreamEvent('progress', {
                    'stage': 'classified',
                    'message': f'âœ… å•é¡Œåˆ†é¡ï¼š{intent_label}ï¼ˆç½®ä¿¡åº¦ {classification.confidence:.0%}ï¼‰'
                })
                await asyncio.sleep(0.1)
                
                # ç™¼é€æ¨ç†å…§å®¹
                if hasattr(classification, 'reasoning') and classification.reasoning:
                    yield StreamEvent('progress', {
                        'stage': 'reasoning',
                        'message': f'ğŸ’­ AI æ¨ç†',
                        'detail': classification.reasoning
                    })
                    await asyncio.sleep(0.05)
            
            # === æ­¥é©Ÿ 3: è·¯ç”±åˆ°è™•ç†å™¨ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰===
            handler = self.intent_handlers.get(classification.intent)
            
            if not handler:
                yield StreamEvent('error', {'message': f'æœªçŸ¥çš„æ„åœ–é¡å‹: {classification.intent}'})
                return
            
            logger.info(f"â†’ è·¯ç”±åˆ°: {handler.__class__.__name__ if hasattr(handler, '__class__') else classification.intent}")
            
            # å¦‚æœæä¾›äº†æ¾„æ¸…ï¼Œæ›´æ–° request.question ç‚º effective_question
            # é€™æ¨£ handler å¯ä»¥ä½¿ç”¨å®Œæ•´çš„ä¸Šä¸‹æ–‡é€²è¡Œè™•ç†
            if clarification_provided:
                logger.info(f"ğŸ”„ æ›´æ–° request.question: '{request.question}' â†’ '{effective_question}'")
                request.question = effective_question
            
            # === æ­¥é©Ÿ 3.5: ç²å–æª¢ç´¢å„ªå…ˆæ–‡æª”ï¼ˆå¦‚æœæ˜¯æ–‡æª”ç›¸é—œæ„åœ–ï¼‰===
            priority_document_ids = []
            should_reuse_cached = False
            
            if context_manager and classification.intent in [
                QuestionIntent.DOCUMENT_SEARCH,
                QuestionIntent.DOCUMENT_DETAIL_QUERY,
                QuestionIntent.COMPLEX_ANALYSIS
            ]:
                try:
                    search_context = await context_manager.load_context(
                        purpose=ContextPurpose.SEARCH_RETRIEVAL
                    )
                    priority_document_ids = search_context.priority_document_ids or []
                    should_reuse_cached = search_context.should_reuse_cached
                    
                    if priority_document_ids:
                        logger.info(f"ğŸ¯ æª¢ç´¢å„ªå…ˆæ–‡æª”: {len(priority_document_ids)} å€‹, é‡ç”¨ç·©å­˜: {should_reuse_cached}")
                except Exception as e:
                    logger.warning(f"ç²å–å„ªå…ˆæ–‡æª”å¤±æ•—: {e}")
            
            # è¼‰å…¥ä¸Šä¸‹æ–‡
            context = await self._load_context_if_needed(
                db, request, user_id, classification
            )
            
            # å°‡å„ªå…ˆæ–‡æª”ä¿¡æ¯æ·»åŠ åˆ° context
            if context is None:
                context = {}
            if priority_document_ids:
                context['priority_document_ids'] = priority_document_ids
                context['should_reuse_cached'] = should_reuse_cached
            
            # âœ… å°†æ–‡æ¡£æ± ä¿¡æ¯æ·»åŠ åˆ° contextï¼ˆç”¨äºæŸ¥è¯¢é‡å†™çš„æŒ‡ä»£è¯è§£æï¼‰
            if cached_documents_info_for_classifier:
                context['cached_documents'] = cached_documents_info_for_classifier
                logger.info(f"ğŸ“¦ æ·»åŠ  {len(cached_documents_info_for_classifier)} ä¸ªæ–‡æ¡£åˆ° context ç”¨äºæŸ¥è¯¢é‡å†™")
            
            # æª¢æŸ¥ handler æ˜¯å¦æœ‰æµå¼ç‰ˆæœ¬
            if hasattr(handler, 'handle_stream'):
                # ä½¿ç”¨æµå¼è™•ç†å™¨
                async for event in handler.handle_stream(
                    request, classification, context, db, user_id, request_id
                ):
                    yield event
            else:
                # ä½¿ç”¨æ™®é€šè™•ç†å™¨ï¼Œæ ¹æ“šæ„åœ–é¡å‹æ±ºå®šåƒæ•¸
                # ç°¡å–®æ„åœ–è™•ç†
                if classification.intent in [QuestionIntent.GREETING, QuestionIntent.CHITCHAT]:
                    # ç°¡å–®æ„åœ–ç›´æ¥è™•ç†ï¼ˆcontext = Noneï¼‰
                    response = await handler.handle(
                        request, classification, None, db, user_id, request_id
                    )
                    yield StreamEvent('complete', {'answer': response.answer})
                    
                elif classification.intent == QuestionIntent.CLARIFICATION_NEEDED:
                    # éœ€è¦æ¾„æ¸…ï¼ˆå¯ä»¥ä½¿ç”¨ context ç”Ÿæˆæ›´å¥½çš„æ¾„æ¸…å•é¡Œï¼‰
                    response = await handler.handle(
                        request, classification, context, db, user_id, request_id
                    )
                    # æ¾„æ¸…å•é¡Œï¼šç™¼é€å®Œæ•´ç­”æ¡ˆï¼Œä¸¦é™„å¸¶ workflow_state ç”¨æ–¼å‰ç«¯åˆ¤æ–·
                    # å‰ç«¯æœƒæª¢æŸ¥ workflow_state.is_clarification æˆ– current_step === 'need_clarification'
                    yield StreamEvent('complete', {
                        'answer': response.answer,
                        'workflow_state': response.workflow_state,
                        'classification': response.classification.model_dump() if response.classification else None
                    })
                    
                elif classification.intent == QuestionIntent.SIMPLE_FACTUAL:
                    # ç°¡å–®äº‹å¯¦æŸ¥è©¢ï¼ˆéœ€è¦ context ä»¥è®¿é—®æ–‡æ¡£æ± ï¼‰
                    response = await handler.handle(
                        request, classification, context, db, user_id, request_id  # âœ… ä¼ é€’ context
                    )
                    if response.answer:
                        yield StreamEvent('complete', {'answer': response.answer})
                    else:
                        yield StreamEvent('error', {'message': 'è™•ç†å¤±æ•—'})
                else:
                    # å…¶ä»–æ„åœ–ï¼ˆDOCUMENT_SEARCH, DOCUMENT_DETAIL_QUERY, COMPLEX_ANALYSISï¼‰éœ€è¦ context
                    # å¦‚æœ handler æ²’æœ‰å¯¦ç¾ handle_streamï¼Œç›´æ¥èª¿ç”¨ handle
                    
                    # å¦‚æœæ˜¯æ‰¹å‡†æ“ä½œï¼Œå…ˆåŸ·è¡Œé è™•ç†ä¸¦ç«‹å³åé¥‹
                    if is_approval_action and classification.intent == QuestionIntent.DOCUMENT_SEARCH:
                        yield StreamEvent('progress', {
                            'stage': 'query_rewriting',
                            'message': 'ğŸ”„ æ­£åœ¨å„ªåŒ–æŸ¥è©¢èªå¥...'
                        })
                        await asyncio.sleep(0.05)
                        
                        # åŸ·è¡ŒæŸ¥è©¢é‡å¯«
                        from app.services.qa_core.qa_query_rewriter import qa_query_rewriter
                        
                        # âœ… æ„å»º document_contextï¼ˆç”¨äºæŒ‡ä»£è¯è§£æï¼‰
                        document_context = None
                        if request.document_ids or (context and context.get('cached_documents')):
                            document_ids = request.document_ids or []
                            document_summaries = []
                            
                            # ä» context ä¸­æå–æ–‡æ¡£æ‘˜è¦
                            if context and 'cached_documents' in context:
                                for doc in context['cached_documents']:
                                    doc_id = doc.get('document_id')
                                    if not document_ids or doc_id in document_ids:
                                        document_summaries.append({
                                            'document_id': doc_id,
                                            'filename': doc.get('filename', ''),
                                            'summary': doc.get('summary', ''),
                                            'key_concepts': doc.get('key_concepts', [])
                                        })
                                logger.info(f"ğŸ“„ æ‰¹å‡†æ“ä½œçš„æŸ¥è¯¢é‡å†™ï¼šè·å–åˆ° {len(document_summaries)} ä¸ªæ–‡æ¡£æ‘˜è¦")
                            
                            document_context = {
                                "document_ids": document_ids,
                                "document_count": len(document_summaries),
                                "document_summaries": document_summaries
                            }
                        
                        query_rewrite_result, rewrite_tokens = await qa_query_rewriter.rewrite_query(
                            db=db,
                            original_query=request.question,
                            user_id=user_id,
                            request_id=request_id,
                            document_context=document_context  # âœ… ä¼ é€’å®Œæ•´çš„ document_context
                        )
                        
                        # ç«‹å³ç™¼é€æŸ¥è©¢é‡å¯«çµæœ
                        if query_rewrite_result and query_rewrite_result.rewritten_queries:
                            yield StreamEvent('progress', {
                                'stage': 'query_rewriting',
                                'message': f'âœ¨ å·²å„ªåŒ–æŸ¥è©¢ï¼ˆç”Ÿæˆ {len(query_rewrite_result.rewritten_queries)} å€‹ï¼‰',
                                'detail': {
                                    'queries': query_rewrite_result.rewritten_queries,
                                    'count': len(query_rewrite_result.rewritten_queries)
                                }
                            })
                            await asyncio.sleep(0.05)
                            
                            # å°‡æŸ¥è©¢é‡å¯«çµæœå­˜å…¥ contextï¼Œé¿å… handler é‡è¤‡åŸ·è¡Œ
                            if context is None:
                                context = {}
                            context['pre_rewritten_query_result'] = query_rewrite_result
                    
                    # ç§»é™¤é è¨­çš„ MongoDB æŸ¥è©¢é€²åº¦ï¼ˆç”± handler è‡ªå·±æ±ºå®šï¼‰
                    # DocumentDetailQueryHandler å¯èƒ½æœƒé™ç´šç‚º document_search
                    # æ‰€ä»¥ä¸æ‡‰è©²åœ¨é€™è£¡é è¨­ç™¼é€å›ºå®šçš„ progress äº‹ä»¶
                    
                    # èª¿ç”¨ handlerï¼ˆé€™äº› handlers æ¥å— context åƒæ•¸ï¼‰
                    response = await handler.handle(
                        request, classification, context, db, user_id, request_id
                    )
                    
                    # ç™¼é€å®Œæˆé€²åº¦
                    if is_approval_action:
                        # å¦‚æœæ˜¯æ–‡æª”æœç´¢ï¼Œé¡¯ç¤ºæ‰¾åˆ°çš„æ–‡æª”æ•¸å’Œæ–‡æª”åˆ—è¡¨
                        if classification.intent == QuestionIntent.DOCUMENT_SEARCH and response.source_documents:
                            doc_count = len(response.source_documents)
                            
                            # æ§‹å»ºæ–‡æª”åˆ—è¡¨ç”¨æ–¼å‰ç«¯é¡¯ç¤º
                            doc_list = []
                            for idx, doc in enumerate(response.source_documents[:10], 1):  # é™åˆ¶æœ€å¤š10å€‹
                                doc_list.append({
                                    'document_id': idx,
                                    'filename': getattr(doc, 'filename', f'æ–‡æª” {idx}'),
                                    'score': getattr(doc, 'score', 0.0) if hasattr(doc, 'score') else 0.0,
                                    'extracted_text': getattr(doc, 'extracted_text', '')[:200] if hasattr(doc, 'extracted_text') else ''
                                })
                            
                            yield StreamEvent('progress', {
                                'stage': 'vector_search',
                                'message': f'âœ… å·²æœç´¢åˆ° {doc_count} å€‹ç›¸é—œæ–‡æª”',
                                'detail': {
                                    'queries': doc_list,
                                    'count': doc_count
                                }
                            })
                            await asyncio.sleep(0.05)
                        
                        # å¦‚æœæ˜¯è©³ç´°æŸ¥è©¢ï¼Œé¡¯ç¤º MongoDB æŸ¥è©¢çµæœ
                        elif classification.intent == QuestionIntent.DOCUMENT_DETAIL_QUERY:
                            # å¾ response ä¸­æå–è©³ç´°æ•¸æ“šä¿¡æ¯
                            detail_info = {}
                            mongodb_data = []
                            
                            if hasattr(response, 'semantic_search_contexts') and response.semantic_search_contexts:
                                detail_info['queried_documents'] = len(response.semantic_search_contexts)
                                # è¨ˆç®—ç¸½æ¬„ä½æ•¸ä¸¦æ”¶é›†æ•¸æ“š
                                total_fields = 0
                                for ctx in response.semantic_search_contexts:
                                    if ctx.metadata:
                                        if 'fields_count' in ctx.metadata:
                                            total_fields += ctx.metadata['fields_count']
                                        # å°‡å®Œæ•´çš„ context æ•¸æ“šæ·»åŠ åˆ° mongodb_data
                                        mongodb_data.append({
                                            'document_id': ctx.document_id,
                                            'metadata': ctx.metadata
                                        })
                                detail_info['total_fields'] = total_fields
                            
                            if response.source_documents:
                                detail_info['source_documents'] = len(response.source_documents)
                            
                            # åŒ…å«å¯¦éš›çš„ MongoDB æŸ¥è©¢æ•¸æ“š
                            if mongodb_data:
                                detail_info['mongodb_data'] = mongodb_data
                            
                            message = f'âœ… MongoDB æŸ¥è©¢å®Œæˆ'
                            if detail_info.get('queried_documents'):
                                message += f"ï¼ˆæŸ¥è©¢ {detail_info['queried_documents']} å€‹æ–‡æª”"
                                if detail_info.get('total_fields'):
                                    message += f"ï¼Œæå– {detail_info['total_fields']} å€‹æ¬„ä½"
                                message += "ï¼‰"
                            
                            yield StreamEvent('progress', {
                                'stage': 'mongodb_query',
                                'message': message,
                                'detail': detail_info
                            })
                            await asyncio.sleep(0.05)
                    
                    # æª¢æŸ¥æ˜¯å¦éœ€è¦æ‰¹å‡†ï¼ˆpending_approval æ˜¯ response çš„ç›´æ¥å±¬æ€§ï¼‰
                    if response.pending_approval or (response.workflow_state and response.workflow_state.get('pending_approval')):
                        logger.info(f"éœ€è¦æ‰¹å‡†: {response.pending_approval or response.workflow_state.get('pending_approval')}")
                        
                        # âœ… å…³é”®ä¿®å¤ï¼šåœ¨å‘é€ approval_needed å‰ä¿å­˜æ–‡æ¡£æ± åˆ°æ•°æ®åº“
                        # è¿™æ ·æ‰¹å‡†åé‡æ–°åŠ è½½æ—¶å°±èƒ½çœ‹åˆ°æ–‡æ¡£äº†
                        if context_manager and request.document_ids:
                            try:
                                logger.info(f"ğŸ’¾ æ‰¹å‡†å‰ä¿å­˜æ–‡æ¡£æ± åˆ°æ•°æ®åº“: {len(request.document_ids)} ä¸ª")
                                # ç›´æ¥ä¿å­˜å½“å‰çŠ¶æ€åˆ° MongoDB
                                await context_manager._save_document_pool_to_db()
                                logger.info("âœ… æ–‡æ¡£æ± å·²ä¿å­˜ï¼Œæ‰¹å‡†åå¯ä»¥è¯»å–")
                            except Exception as e:
                                logger.error(f"ä¿å­˜æ–‡æ¡£æ± å¤±è´¥: {e}", exc_info=True)
                        
                        # ç™¼é€æ‰¹å‡†è«‹æ±‚ï¼ŒåŒ…å«å®Œæ•´ä¿¡æ¯
                        approval_data = {
                            'workflow_state': response.workflow_state,
                            'query_rewrite_result': response.query_rewrite_result.model_dump() if response.query_rewrite_result else None,
                            'classification': response.classification.model_dump() if response.classification else None,
                            'next_action': response.next_action,
                            'pending_approval': response.pending_approval
                        }
                        yield StreamEvent('approval_needed', approval_data)
                        # ä¸ç¹¼çºŒè™•ç†ï¼Œç­‰å¾…ç”¨æˆ¶æ‰¹å‡†
                    elif response.answer:
                        # æœ‰ç­”æ¡ˆï¼Œç™¼é€æµå¼è¼¸å‡º
                        # ç™¼é€ç”Ÿæˆé€²åº¦
                        yield StreamEvent('progress', {
                            'stage': 'ai_generating',
                            'message': 'ğŸ¤– AI æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...'
                        })
                        await asyncio.sleep(0.05)
                        
                        # æ¨¡æ“¬æµå¼è¼¸å‡ºç­”æ¡ˆ
                        answer = response.answer
                        chunk_size = 50
                        for i in range(0, len(answer), chunk_size):
                            chunk = answer[i:i+chunk_size]
                            yield StreamEvent('chunk', {'text': chunk})
                            await asyncio.sleep(0.01)
                        
                        # ç™¼é€å…ƒæ•¸æ“šï¼ˆåŒ…å«æ–‡æª”æ± ä¿¡æ¯ï¼‰
                        metadata_payload = {
                            'tokens_used': response.tokens_used,
                            'source_documents': response.source_documents if response.source_documents else [],
                            'processing_time': response.processing_time
                        }
                        
                        # æ·»åŠ æ–‡æª”æ± ä¿¡æ¯ï¼ˆé‡æ–°è¼‰å…¥ä»¥ç²å–æœ€æ–°ç‹€æ…‹ï¼‰
                        # âš ï¸ handler å·²ç¶“ä¿å­˜äº†æ–‡æª”æ± ï¼Œéœ€è¦é‡æ–°è¼‰å…¥ä»¥ç²å–æœ€æ–°çš„æ–‡æª”æ± 
                        if request.conversation_id and user_id:
                            try:
                                # ç­‰å¾…ä¸€å°æ®µæ™‚é–“ç¢ºä¿æ•¸æ“šåº«æ›´æ–°å®Œæˆ
                                await asyncio.sleep(0.1)
                                
                                # å‰µå»ºè‡¨æ™‚ context_manager ä¾†è®€å–æ–‡æª”æ± ï¼ˆç¦ç”¨ç·©å­˜ï¼‰
                                temp_ctx_mgr = ConversationContextManager(
                                    db=db,
                                    conversation_id=request.conversation_id,
                                    user_id=str(user_id),
                                    enable_caching=False  # ç¦ç”¨ç·©å­˜ï¼Œå¼·åˆ¶å¾æ•¸æ“šåº«è®€å–
                                )
                                
                                # å¼·åˆ¶é‡æ–°è¼‰å…¥æ–‡æª”æ± 
                                await temp_ctx_mgr._load_document_pool()
                                
                                if temp_ctx_mgr._document_pool:
                                    # â­ é—œéµä¿®å¾©ï¼šæŒ‰ source_documents çš„é †åºæ§‹å»º document_pool
                                    # é€™æ¨£å‰ç«¯çš„å¼•ç”¨ç·¨è™Ÿæ‰èƒ½æ­£ç¢ºå°æ‡‰æ–‡æª”
                                    document_pool_data = {}
                                    source_doc_ids = response.source_documents if response.source_documents else []
                                    
                                    # â­â­ æ–°å¢ï¼šæ§‹å»ºç•¶å‰è¼ªæ¬¡çš„æ–‡æª”å¿«ç…§ï¼ˆåªåŒ…å« AI çœ‹åˆ°çš„æ–‡æª”ï¼‰
                                    # é€™æ˜¯å‰ç«¯ç”¨ä¾†è§£æå¼•ç”¨çš„é—œéµæ•¸æ“š
                                    current_round_snapshot = []
                                    
                                    # 1. å…ˆæŒ‰ source_documents é †åºæ·»åŠ ï¼ˆé€™æ˜¯ AI çœ‹åˆ°çš„é †åºï¼‰
                                    for doc_id in source_doc_ids:
                                        if doc_id in temp_ctx_mgr._document_pool:
                                            doc_ref = temp_ctx_mgr._document_pool[doc_id]
                                            doc_data = {
                                                'document_id': doc_id,
                                                'filename': doc_ref.filename,
                                                'summary': doc_ref.summary,
                                                'relevance_score': doc_ref.relevance_score,
                                                'access_count': doc_ref.access_count,
                                                'first_mentioned_round': doc_ref.first_mentioned_round,
                                                'last_accessed_round': doc_ref.last_accessed_round
                                            }
                                            document_pool_data[doc_id] = doc_data
                                            # â­ åŒæ™‚æ·»åŠ åˆ°ç•¶å‰è¼ªæ¬¡å¿«ç…§ï¼ˆé€™æ˜¯å¼•ç”¨è§£æçš„é—œéµï¼‰
                                            current_round_snapshot.append(doc_data)
                                    
                                    # 2. å†æ·»åŠ å…¶ä»–æ–‡æª”ï¼ˆä¸åœ¨ source_documents ä¸­çš„ï¼‰
                                    for doc_id, doc_ref in temp_ctx_mgr._document_pool.items():
                                        if doc_id not in document_pool_data:
                                            document_pool_data[doc_id] = {
                                                'document_id': doc_id,
                                                'filename': doc_ref.filename,
                                                'summary': doc_ref.summary,
                                                'relevance_score': doc_ref.relevance_score,
                                                'access_count': doc_ref.access_count,
                                                'first_mentioned_round': doc_ref.first_mentioned_round,
                                                'last_accessed_round': doc_ref.last_accessed_round
                                            }
                                    
                                    metadata_payload['document_pool'] = document_pool_data
                                    metadata_payload['document_pool_count'] = len(document_pool_data)
                                    # â­â­ æ–°å¢ï¼šç•¶å‰è¼ªæ¬¡çš„æ–‡æª”å¿«ç…§ï¼ˆç”¨æ–¼å¼•ç”¨è§£æï¼‰
                                    metadata_payload['current_round_documents'] = current_round_snapshot
                                    logger.info(f"ğŸ“š ç™¼é€æ–‡æª”æ± ä¿¡æ¯: {len(document_pool_data)} å€‹æ–‡æª”, ç•¶å‰è¼ªæ¬¡: {len(current_round_snapshot)} å€‹")
                                else:
                                    logger.debug(f"ğŸ“š æ–‡æª”æ± ç‚ºç©º")
                            except Exception as e:
                                logger.warning(f"âš ï¸ è¼‰å…¥æ–‡æª”æ± å¤±æ•—: {e}")
                        
                        yield StreamEvent('metadata', metadata_payload)
                        
                        yield StreamEvent('complete', {'message': 'âœ… è™•ç†å®Œæˆ'})
                    else:
                        # æ²’æœ‰ç­”æ¡ˆä¹Ÿæ²’æœ‰ workflow_stateï¼Œå¯èƒ½æ˜¯éŒ¯èª¤
                        yield StreamEvent('error', {'message': 'è™•ç†å¤±æ•—ï¼Œæœªè¿”å›çµæœ'})
            
        except Exception as e:
            logger.error(f"æµå¼æ™ºèƒ½è·¯ç”±å¤±æ•—: {e}", exc_info=True)
            yield StreamEvent('error', {'message': str(e)})


# å‰µå»ºå…¨å±€å¯¦ä¾‹
qa_orchestrator = QAOrchestrator()
