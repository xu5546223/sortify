from typing import List, Optional, Dict, Any
from motor.motor_asyncio import AsyncIOMotorDatabase
from app.services.vector_db_service import vector_db_service
from app.services.embedding_service import embedding_service
from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.models.vector_models import SemanticSearchResult
from app.core.config import settings
import logging
import asyncio
import math
import uuid

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()

class EnhancedSearchService:
    """
    å…©éšæ®µæ··åˆæª¢ç´¢æœç´¢æœå‹™ (Two-Stage Hybrid Retrieval Search Service)
    
    å¯¦ç¾ç­–ç•¥ï¼š
    1. ç¬¬ä¸€éšæ®µ (ç²—ç¯©é¸): åœ¨æ‘˜è¦å‘é‡ä¸­å¿«é€Ÿæ‰¾å‡ºç›¸é—œæ–‡æª”
    2. ç¬¬äºŒéšæ®µ (ç²¾æ’åº): åœ¨å€™é¸æ–‡æª”çš„å…§å®¹å¡Šå‘é‡ä¸­ç²¾ç¢ºåŒ¹é…
    3. RRFèåˆæª¢ç´¢ (çµ‚æ¥µç­–ç•¥): ä¸¦è¡ŒåŸ·è¡Œæ‘˜è¦å’Œå…§å®¹å¡Šæœç´¢ï¼Œä½¿ç”¨å€’æ•¸æ’åèåˆ
    """
    
    def __init__(self):
        self.stage1_top_k = getattr(settings, 'VECTOR_SEARCH_STAGE1_TOP_K', 10)  # ç¬¬ä¸€éšæ®µè¿”å›å€™é¸æ–‡æª”æ•¸
        self.stage2_top_k = getattr(settings, 'VECTOR_SEARCH_TOP_K', 5)  # ç¬¬äºŒéšæ®µæœ€çµ‚çµæœæ•¸
        self.similarity_threshold = getattr(settings, 'VECTOR_SIMILARITY_THRESHOLD', 0.4)
        # RRF åƒæ•¸
        self.rrf_k = getattr(settings, 'RRF_K_CONSTANT', 60)  # RRF å¸¸æ•¸ kï¼Œé™ä½é«˜æ’åå½±éŸ¿åŠ›
        self.rrf_weights = getattr(settings, 'RRF_WEIGHTS', {"summary": 0.4, "chunks": 0.6})  # æœç´¢æ¬Šé‡
    
    async def two_stage_hybrid_search(
        self,
        db: AsyncIOMotorDatabase,
        query: str,
        user_id: Any,
        search_type: str = "hybrid",
        stage1_top_k: Optional[int] = None,
        stage2_top_k: Optional[int] = None,
        similarity_threshold: Optional[float] = None,
        # æ–°å¢ï¼šRRF å‹•æ…‹æ¬Šé‡é…ç½®
        rrf_weights: Optional[Dict[str, float]] = None,
        rrf_k_constant: Optional[int] = None
    ) -> List[SemanticSearchResult]:
        """
        åŸ·è¡Œå…©éšæ®µæ··åˆæª¢ç´¢æœç´¢
        
        Args:
            db: è³‡æ–™åº«é€£æ¥
            query: æœç´¢æŸ¥è©¢
            user_id: ç”¨æˆ¶ID
            search_type: æœç´¢é¡å‹ ("hybrid", "summary_only", "chunks_only", "rrf_fusion")
            stage1_top_k: ç¬¬ä¸€éšæ®µå€™é¸æ–‡æª”æ•¸é‡
            stage2_top_k: ç¬¬äºŒéšæ®µæœ€çµ‚çµæœæ•¸é‡
            similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            æ’åºå¾Œçš„æœç´¢çµæœåˆ—è¡¨
        """
        # ç‚ºäº†ChromaDBçš„ç›¸å®¹æ€§ï¼Œå¦‚æœuser_idæ˜¯UUIDç‰©ä»¶ï¼Œå‰‡å°‡å…¶è½‰æ›ç‚ºå­—ä¸²
        if isinstance(user_id, uuid.UUID):
            user_id = str(user_id)
            
        # ä½¿ç”¨åƒæ•¸æˆ–é è¨­å€¼
        stage1_k = stage1_top_k or self.stage1_top_k
        stage2_k = stage2_top_k or self.stage2_top_k
        sim_threshold = similarity_threshold or self.similarity_threshold
        
        log_details = {
            "user_id": user_id, 
            "query": query[:100],  # é™åˆ¶æŸ¥è©¢é•·åº¦ä»¥ç¯€çœæ—¥èªŒç©ºé–“
            "search_type": search_type,
            "stage1_top_k": stage1_k,
            "stage2_top_k": stage2_k,
            "similarity_threshold": sim_threshold
        }
        
        logger.info(f"é–‹å§‹å…©éšæ®µæ··åˆæª¢ç´¢æœç´¢: {query[:50]}...")
        await log_event(db, LogLevel.INFO, "é–‹å§‹å…©éšæ®µæ··åˆæª¢ç´¢æœç´¢", 
                       "service.enhanced_search.two_stage_search_start", details=log_details)
        
        try:
            # å‘é‡åŒ–æŸ¥è©¢
            query_vector = embedding_service.encode_text(query)
            if not query_vector:
                logger.error("æŸ¥è©¢å‘é‡åŒ–å¤±æ•—")
                await log_event(db, LogLevel.ERROR, "æŸ¥è©¢å‘é‡åŒ–å¤±æ•—", 
                               "service.enhanced_search.query_vectorization_failed", details=log_details)
                return []
            
            # æ ¹æ“šæœç´¢é¡å‹é¸æ“‡ç­–ç•¥
            if search_type == "summary_only":
                return await self._search_summary_vectors_only(
                    db, query_vector, user_id, stage2_k, sim_threshold, log_details
                )
            elif search_type == "chunks_only":
                return await self._search_chunk_vectors_only(
                    db, query_vector, user_id, stage2_k, sim_threshold, log_details
                )
            elif search_type == "rrf_fusion":
                # ğŸš€ æ–°å¢ï¼šRRF èåˆæª¢ç´¢ç­–ç•¥
                return await self._execute_rrf_fusion_search(
                    db, query_vector, user_id, stage2_k, sim_threshold, log_details,
                    rrf_weights, rrf_k_constant
                )
            else:  # "hybrid" é è¨­
                return await self._execute_two_stage_search(
                    db, query_vector, user_id, stage1_k, stage2_k, sim_threshold, log_details
                )
                
        except ValueError as ve:
            logger.error(f"æŸ¥è©¢è™•ç†å¤±æ•—: {str(ve)}", exc_info=True)
            await log_event(db, LogLevel.WARNING, f"æŸ¥è©¢è™•ç†å¤±æ•—: {str(ve)}", 
                           "service.enhanced_search.query_processing_error", 
                           details={**log_details, "error": str(ve)})
            return []
        except Exception as e:
            logger.error(f"å…©éšæ®µæ··åˆæª¢ç´¢æœç´¢å¤±æ•—: {str(e)}", exc_info=True)
            await log_event(db, LogLevel.ERROR, f"å…©éšæ®µæ··åˆæª¢ç´¢æœç´¢å¤±æ•—: {str(e)}", 
                           "service.enhanced_search.two_stage_search_error", 
                           details={**log_details, "error": str(e)})
            return []
    
    async def _execute_two_stage_search(
        self,
        db: AsyncIOMotorDatabase,
        query_vector: List[float],
        user_id: str,
        stage1_k: int,
        stage2_k: int,
        sim_threshold: float,
        log_details: Dict[str, Any]
    ) -> List[SemanticSearchResult]:
        """åŸ·è¡Œå®Œæ•´çš„å…©éšæ®µæ··åˆæª¢ç´¢"""
        
        # ===== ç¬¬ä¸€éšæ®µï¼šç²—ç¯©é¸ (åœ¨æ‘˜è¦å‘é‡ä¸­æœç´¢) =====
        logger.info(f"ç¬¬ä¸€éšæ®µï¼šåœ¨æ‘˜è¦å‘é‡ä¸­æœç´¢ï¼Œç›®æ¨™ {stage1_k} å€‹å€™é¸æ–‡æª”")
        
        stage1_results = vector_db_service.search_similar_vectors(
            query_vector=query_vector,
            top_k=stage1_k,
            owner_id_filter=user_id,
            similarity_threshold=sim_threshold,
            metadata_filter={"type": "summary"}  # åªæœç´¢æ‘˜è¦å‘é‡
        )
        
        if not stage1_results:
            logger.warning("ç¬¬ä¸€éšæ®µæœªæ‰¾åˆ°ç›¸é—œçš„æ‘˜è¦å‘é‡")
            await log_event(db, LogLevel.WARNING, "ç¬¬ä¸€éšæ®µæœªæ‰¾åˆ°ç›¸é—œæ‘˜è¦å‘é‡", 
                           "service.enhanced_search.stage1_no_results", details=log_details)
            return []
        
        # æå–å€™é¸æ–‡æª”IDs
        candidate_doc_ids = [result.document_id for result in stage1_results]
        stage1_details = {
            **log_details,
            "stage1_candidates": len(candidate_doc_ids),
            "stage1_doc_ids": candidate_doc_ids[:5]  # åªè¨˜éŒ„å‰5å€‹ä»¥ç¯€çœç©ºé–“
        }
        
        logger.info(f"ç¬¬ä¸€éšæ®µå®Œæˆï¼šæ‰¾åˆ° {len(candidate_doc_ids)} å€‹å€™é¸æ–‡æª”")
        await log_event(db, LogLevel.INFO, f"ç¬¬ä¸€éšæ®µå®Œæˆï¼š{len(candidate_doc_ids)} å€‹å€™é¸æ–‡æª”", 
                       "service.enhanced_search.stage1_completed", details=stage1_details)
        
        # ===== ç¬¬äºŒéšæ®µï¼šç²¾æ’åº (åœ¨å€™é¸æ–‡æª”çš„å…§å®¹å¡Šä¸­æœç´¢) =====
        logger.info(f"ç¬¬äºŒéšæ®µï¼šåœ¨å€™é¸æ–‡æª”çš„å…§å®¹å¡Šä¸­ç²¾ç¢ºæœç´¢ï¼Œç›®æ¨™ {stage2_k} å€‹çµæœ")
        
        stage2_results = vector_db_service.search_similar_vectors(
            query_vector=query_vector,
            top_k=stage2_k * 2,  # æœç´¢æ›´å¤šçµæœä»¥ä¾¿å¾ŒçºŒéæ¿¾å’Œæ’åº
            owner_id_filter=user_id,
            similarity_threshold=sim_threshold,
            metadata_filter={
                "type": "chunk",
                "document_id": {"$in": candidate_doc_ids}  # é™åˆ¶åœ¨å€™é¸æ–‡æª”çš„å¡Šä¸­æœç´¢
            }
        )
        
        if not stage2_results:
            logger.warning("ç¬¬äºŒéšæ®µæœªæ‰¾åˆ°ç›¸é—œçš„å…§å®¹å¡Šï¼Œé™ç´šä½¿ç”¨ç¬¬ä¸€éšæ®µçµæœ")
            await log_event(db, LogLevel.WARNING, "ç¬¬äºŒéšæ®µç„¡çµæœï¼Œé™ç´šä½¿ç”¨ç¬¬ä¸€éšæ®µçµæœ", 
                           "service.enhanced_search.stage2_fallback", details=stage1_details)
            # é™ç´šç­–ç•¥ï¼šè¿”å›ç¬¬ä¸€éšæ®µçš„æ‘˜è¦çµæœ
            return stage1_results[:stage2_k]
        
        # å°ç¬¬äºŒéšæ®µçµæœé€²è¡Œé‡æ’åºå’Œå»é‡
        final_results = await self._rerank_and_deduplicate_results(
            stage2_results, stage1_results, stage2_k
        )
        
        final_details = {
            **stage1_details,
            "stage2_raw_results": len(stage2_results),
            "final_results": len(final_results)
        }
        
        logger.info(f"å…©éšæ®µæ··åˆæª¢ç´¢å®Œæˆï¼šæœ€çµ‚è¿”å› {len(final_results)} å€‹ç²¾ç¢ºçµæœ")
        await log_event(db, LogLevel.INFO, f"å…©éšæ®µæ··åˆæª¢ç´¢å®Œæˆï¼š{len(final_results)} å€‹çµæœ", 
                       "service.enhanced_search.two_stage_completed", details=final_details)
        
        return final_results
    
    async def _search_summary_vectors_only(
        self,
        db: AsyncIOMotorDatabase,
        query_vector: List[float],
        user_id: str,
        top_k: int,
        sim_threshold: float,
        log_details: Dict[str, Any]
    ) -> List[SemanticSearchResult]:
        """åƒ…åœ¨æ‘˜è¦å‘é‡ä¸­æœç´¢ï¼ˆå¿«é€Ÿæ–‡æª”ç´šåˆ¥æœç´¢ï¼‰"""
        
        logger.info("åŸ·è¡Œæ‘˜è¦å‘é‡å°ˆç”¨æœç´¢")
        
        results = vector_db_service.search_similar_vectors(
            query_vector=query_vector,
            top_k=top_k,
            owner_id_filter=user_id,
            similarity_threshold=sim_threshold,
            metadata_filter={"type": "summary"}
        )
        
        await log_event(db, LogLevel.INFO, f"æ‘˜è¦å‘é‡æœç´¢å®Œæˆï¼š{len(results)} å€‹çµæœ", 
                       "service.enhanced_search.summary_only_completed", 
                       details={**log_details, "results_count": len(results)})
        
        return results
    
    async def _search_chunk_vectors_only(
        self,
        db: AsyncIOMotorDatabase,
        query_vector: List[float],
        user_id: str,
        top_k: int,
        sim_threshold: float,
        log_details: Dict[str, Any]
    ) -> List[SemanticSearchResult]:
        """åƒ…åœ¨å…§å®¹å¡Šå‘é‡ä¸­æœç´¢ï¼ˆç²¾ç¢ºå…§å®¹ç´šåˆ¥æœç´¢ï¼‰"""
        
        logger.info("åŸ·è¡Œå…§å®¹å¡Šå‘é‡å°ˆç”¨æœç´¢")
        
        results = vector_db_service.search_similar_vectors(
            query_vector=query_vector,
            top_k=top_k,
            owner_id_filter=user_id,
            similarity_threshold=sim_threshold,
            metadata_filter={"type": "chunk"}
        )
        
        await log_event(db, LogLevel.INFO, f"å…§å®¹å¡Šæœç´¢å®Œæˆï¼š{len(results)} å€‹çµæœ", 
                       "service.enhanced_search.chunks_only_completed", 
                       details={**log_details, "results_count": len(results)})
        
        return results
    
    async def _execute_rrf_fusion_search(
        self,
        db: AsyncIOMotorDatabase,
        query_vector: List[float],
        user_id: str,
        top_k: int,
        sim_threshold: float,
        log_details: Dict[str, Any],
        rrf_weights: Optional[Dict[str, float]] = None,
        rrf_k_constant: Optional[int] = None
    ) -> List[SemanticSearchResult]:
        """
        ğŸš€ åŸ·è¡Œ RRF (Reciprocal Rank Fusion) èåˆæª¢ç´¢
        
        ä¸¦è¡ŒåŸ·è¡Œå…©ç¨®æœç´¢ï¼š
        1. æœç´¢ A: æ‘˜è¦å‘é‡æœç´¢ (Summary Search)
        2. æœç´¢ B: å…§å®¹å¡Šæœç´¢ (Chunks Search)
        3. ä½¿ç”¨ RRF ç®—æ³•èåˆå…©å€‹æ’ååˆ—è¡¨
        
        RRF å…¬å¼: score(d) = Î£(w_i / (k + rank_i(d)))
        å…¶ä¸­ï¼š
        - w_i: æœç´¢ i çš„æ¬Šé‡
        - k: RRF å¸¸æ•¸ (é€šå¸¸ç‚º 60)
        - rank_i(d): æ–‡æª” d åœ¨æœç´¢ i ä¸­çš„æ’å
        """
        
        # ä½¿ç”¨å‹•æ…‹æ¬Šé‡æˆ–é è¨­æ¬Šé‡
        effective_rrf_weights = rrf_weights or self.rrf_weights
        effective_rrf_k = rrf_k_constant or self.rrf_k
        
        logger.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œ RRF èåˆæª¢ç´¢ï¼Œç›®æ¨™ {top_k} å€‹çµæœ")
        logger.info(f"ğŸ¯ RRF åƒæ•¸ï¼šæ¬Šé‡ {effective_rrf_weights}, kå¸¸æ•¸ {effective_rrf_k}")
        
        # ä¸¦è¡ŒåŸ·è¡Œå…©ç¨®æœç´¢
        search_tasks = [
            # æœç´¢ A: æ‘˜è¦å‘é‡æœç´¢
            self._parallel_search_summary_vectors(query_vector, user_id, top_k * 2, sim_threshold),
            # æœç´¢ B: å…§å®¹å¡Šæœç´¢
            self._parallel_search_chunk_vectors(query_vector, user_id, top_k * 2, sim_threshold)
        ]
        
        try:
            summary_results, chunk_results = await asyncio.gather(*search_tasks)
            
            parallel_search_details = {
                **log_details,
                "summary_results_count": len(summary_results),
                "chunk_results_count": len(chunk_results)
            }
            
            logger.info(f"ä¸¦è¡Œæœç´¢å®Œæˆï¼šæ‘˜è¦ {len(summary_results)} å€‹ï¼Œå…§å®¹å¡Š {len(chunk_results)} å€‹")
            await log_event(db, LogLevel.INFO, "RRF ä¸¦è¡Œæœç´¢å®Œæˆ", 
                           "service.enhanced_search.rrf_parallel_completed", details=parallel_search_details)
            
            if not summary_results and not chunk_results:
                logger.warning("RRF èåˆæª¢ç´¢ï¼šå…©ç¨®æœç´¢éƒ½æ²’æœ‰æ‰¾åˆ°çµæœ")
                await log_event(db, LogLevel.WARNING, "RRF èåˆæª¢ç´¢ç„¡çµæœ", 
                               "service.enhanced_search.rrf_no_results", details=log_details)
                return []
            
            # ğŸ¯ æ‡‰ç”¨ RRF ç®—æ³•é€²è¡Œæ’åèåˆ
            fused_results = await self._apply_rrf_algorithm(
                summary_results, chunk_results, top_k, log_details,
                effective_rrf_weights, effective_rrf_k
            )
            
            final_details = {
                **parallel_search_details,
                "fused_results_count": len(fused_results),
                "rrf_k": effective_rrf_k,
                "rrf_weights": effective_rrf_weights
            }
            
            logger.info(f"ğŸ¯ RRF èåˆæª¢ç´¢å®Œæˆï¼šæœ€çµ‚è¿”å› {len(fused_results)} å€‹çµæœ")
            await log_event(db, LogLevel.INFO, f"RRF èåˆæª¢ç´¢å®Œæˆï¼š{len(fused_results)} å€‹çµæœ", 
                           "service.enhanced_search.rrf_fusion_completed", details=final_details)
            
            return fused_results
            
        except Exception as e:
            logger.error(f"RRF èåˆæª¢ç´¢å¤±æ•—: {str(e)}", exc_info=True)
            await log_event(db, LogLevel.ERROR, f"RRF èåˆæª¢ç´¢å¤±æ•—: {str(e)}", 
                           "service.enhanced_search.rrf_fusion_error", 
                           details={**log_details, "error": str(e)})
            return []
    
    async def _parallel_search_summary_vectors(
        self,
        query_vector: List[float],
        user_id: str,
        top_k: int,
        sim_threshold: float
    ) -> List[SemanticSearchResult]:
        """ä¸¦è¡ŒåŸ·è¡Œæ‘˜è¦å‘é‡æœç´¢"""
        
        return vector_db_service.search_similar_vectors(
            query_vector=query_vector,
            top_k=top_k,
            owner_id_filter=user_id,
            similarity_threshold=sim_threshold,
            metadata_filter={"type": "summary"}
        )
    
    async def _parallel_search_chunk_vectors(
        self,
        query_vector: List[float],
        user_id: str,
        top_k: int,
        sim_threshold: float
    ) -> List[SemanticSearchResult]:
        """ä¸¦è¡ŒåŸ·è¡Œå…§å®¹å¡Šå‘é‡æœç´¢"""
        
        return vector_db_service.search_similar_vectors(
            query_vector=query_vector,
            top_k=top_k,
            owner_id_filter=user_id,
            similarity_threshold=sim_threshold,
            metadata_filter={"type": "chunk"}
        )
    
    async def _apply_rrf_algorithm(
        self,
        summary_results: List[SemanticSearchResult],
        chunk_results: List[SemanticSearchResult],
        target_count: int,
        log_details: Dict[str, Any],
        rrf_weights: Dict[str, float],
        rrf_k_constant: int
    ) -> List[SemanticSearchResult]:
        """
        ğŸ¯ æ‡‰ç”¨ RRF (Reciprocal Rank Fusion) ç®—æ³•
        
        RRF å…¬å¼ï¼š
        score(document_d) = w_summary / (k + rank_summary(d)) + w_chunks / (k + rank_chunks(d))
        
        å…¶ä¸­ï¼š
        - w_summary, w_chunks: æ‘˜è¦å’Œå…§å®¹å¡Šæœç´¢çš„æ¬Šé‡
        - k: RRF å¸¸æ•¸ (é è¨­ 60)
        - rank_*(d): æ–‡æª” d åœ¨ç›¸æ‡‰æœç´¢ä¸­çš„æ’å (å¾ 1 é–‹å§‹)
        """
        
        logger.info("ğŸ¯ é–‹å§‹æ‡‰ç”¨ RRF ç®—æ³•é€²è¡Œæ’åèåˆ")
        
        # å»ºç«‹æ–‡æª”æ’åæ˜ å°„
        summary_ranks = {result.document_id: idx + 1 for idx, result in enumerate(summary_results)}
        chunk_ranks = {}
        
        # å°æ–¼å…§å®¹å¡Šçµæœï¼Œéœ€è¦æŒ‰æ–‡æª”åˆ†çµ„ä¸¦å–æœ€é«˜åˆ†æ•¸çš„å¡Šä½œç‚ºè©²æ–‡æª”çš„ä»£è¡¨
        chunk_doc_best = {}
        for idx, result in enumerate(chunk_results):
            doc_id = result.document_id
            if doc_id not in chunk_doc_best or result.similarity_score > chunk_doc_best[doc_id]["score"]:
                chunk_doc_best[doc_id] = {
                    "result": result,
                    "rank": idx + 1,
                    "score": result.similarity_score
                }
        
        # å»ºç«‹å…§å®¹å¡Šæ–‡æª”æ’å (æŒ‰æœ€é«˜åˆ†æ•¸é‡æ–°æ’åº)
        sorted_chunk_docs = sorted(chunk_doc_best.items(), key=lambda x: x[1]["score"], reverse=True)
        chunk_ranks = {doc_id: idx + 1 for idx, (doc_id, _) in enumerate(sorted_chunk_docs)}
        
        # æ”¶é›†æ‰€æœ‰åƒèˆ‡èåˆçš„æ–‡æª”
        all_documents = set(summary_ranks.keys()) | set(chunk_ranks.keys())
        
        # è¨ˆç®—æ¯å€‹æ–‡æª”çš„ RRF åˆ†æ•¸
        rrf_scores = {}
        detailed_scores = {}  # ç”¨æ–¼æ—¥èªŒè¨˜éŒ„
        
        for doc_id in all_documents:
            rrf_score = 0.0
            score_details = {"doc_id": doc_id, "components": []}
            
            # æ‘˜è¦æœç´¢è²¢ç»
            if doc_id in summary_ranks:
                summary_contribution = rrf_weights["summary"] / (rrf_k_constant + summary_ranks[doc_id])
                rrf_score += summary_contribution
                score_details["components"].append({
                    "type": "summary",
                    "rank": summary_ranks[doc_id],
                    "weight": rrf_weights["summary"],
                    "contribution": summary_contribution
                })
            
            # å…§å®¹å¡Šæœç´¢è²¢ç»
            if doc_id in chunk_ranks:
                chunk_contribution = rrf_weights["chunks"] / (rrf_k_constant + chunk_ranks[doc_id])
                rrf_score += chunk_contribution
                score_details["components"].append({
                    "type": "chunks",
                    "rank": chunk_ranks[doc_id],
                    "weight": rrf_weights["chunks"],
                    "contribution": chunk_contribution
                })
            
            rrf_scores[doc_id] = rrf_score
            score_details["final_rrf_score"] = rrf_score
            detailed_scores[doc_id] = score_details
        
        # æŒ‰ RRF åˆ†æ•¸æ’åº
        sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)
        
        # æ§‹å»ºæœ€çµ‚çµæœ
        final_results = []
        for doc_id, rrf_score in sorted_docs[:target_count]:
            # å„ªå…ˆä½¿ç”¨å…§å®¹å¡Šçµæœï¼ˆæ›´ç²¾ç¢ºï¼‰ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨æ‘˜è¦çµæœ
            if doc_id in chunk_doc_best:
                result = chunk_doc_best[doc_id]["result"]
            else:
                # åœ¨æ‘˜è¦çµæœä¸­æ‰¾åˆ°è©²æ–‡æª”
                result = next((r for r in summary_results if r.document_id == doc_id), None)
                if not result:
                    continue
            
            # å‰µå»ºæ–°çš„çµæœå°è±¡ï¼Œä½¿ç”¨ RRF åˆ†æ•¸æ›¿æ›åŸå§‹ç›¸ä¼¼åº¦åˆ†æ•¸
            fused_result = SemanticSearchResult(
                document_id=result.document_id,
                similarity_score=rrf_score,  # ä½¿ç”¨ RRF åˆ†æ•¸
                summary_text=result.summary_text,
                metadata={
                    **(result.metadata or {}),
                    "rrf_score": rrf_score,
                    "original_similarity": result.similarity_score,
                    "fusion_method": "rrf",
                    "rrf_details": detailed_scores.get(doc_id, {}),
                    "search_strategy": "rrf_fusion"
                }
            )
            
            final_results.append(fused_result)
        
        # è¨˜éŒ„è©³ç´°çš„èåˆçµ±è¨ˆ
        fusion_stats = {
            "total_candidates": len(all_documents),
            "summary_only": len(summary_ranks - chunk_ranks.keys()),
            "chunks_only": len(chunk_ranks.keys() - summary_ranks),
            "both_rankings": len(summary_ranks.keys() & chunk_ranks.keys()),
            "final_count": len(final_results),
            "rrf_k": rrf_k_constant,
            "weights": rrf_weights,
            "top_scores": [{"doc_id": r.document_id, "rrf_score": r.similarity_score} 
                          for r in final_results[:3]]  # å‰3å€‹çµæœçš„åˆ†æ•¸
        }
        
        logger.info(f"RRF ç®—æ³•å®Œæˆï¼š{len(all_documents)} å€‹å€™é¸æ–‡æª” â†’ {len(final_results)} å€‹èåˆçµæœ")
        logger.debug(f"RRF èåˆçµ±è¨ˆï¼š{fusion_stats}")
        
        return final_results
    
    async def _rerank_and_deduplicate_results(
        self,
        stage2_results: List[SemanticSearchResult],
        stage1_results: List[SemanticSearchResult],
        target_count: int
    ) -> List[SemanticSearchResult]:
        """
        é‡æ’åºå’Œå»é‡ç¬¬äºŒéšæ®µçµæœ
        
        ç­–ç•¥ï¼š
        1. æŒ‰ç›¸ä¼¼åº¦åˆ†æ•¸æ’åº
        2. æŒ‰æ–‡æª”å»é‡ï¼ˆæ¯å€‹æ–‡æª”åªä¿ç•™æœ€é«˜åˆ†çš„å¡Šï¼‰
        3. å¦‚æœç¬¬äºŒéšæ®µçµæœä¸è¶³ï¼Œç”¨ç¬¬ä¸€éšæ®µçµæœè£œå……
        """
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•¸æ’åº
        stage2_results.sort(key=lambda x: x.similarity_score, reverse=True)
        
        # æŒ‰æ–‡æª”å»é‡ï¼Œæ¯å€‹æ–‡æª”åªä¿ç•™ç›¸ä¼¼åº¦æœ€é«˜çš„å¡Š
        seen_documents = set()
        deduplicated_results = []
        
        for result in stage2_results:
            if result.document_id not in seen_documents:
                seen_documents.add(result.document_id)
                deduplicated_results.append(result)
                
                if len(deduplicated_results) >= target_count:
                    break
        
        # å¦‚æœç¬¬äºŒéšæ®µçµæœä¸è¶³ï¼Œç”¨ç¬¬ä¸€éšæ®µçµæœè£œå……
        if len(deduplicated_results) < target_count:
            for stage1_result in stage1_results:
                if (stage1_result.document_id not in seen_documents and 
                    len(deduplicated_results) < target_count):
                    deduplicated_results.append(stage1_result)
                    seen_documents.add(stage1_result.document_id)
        
        logger.info(f"é‡æ’åºå®Œæˆï¼š{len(stage2_results)} â†’ {len(deduplicated_results)} å€‹å»é‡çµæœ")
        
        return deduplicated_results[:target_count]

# å‰µå»ºå…¨å±€å¯¦ä¾‹
enhanced_search_service = EnhancedSearchService() 