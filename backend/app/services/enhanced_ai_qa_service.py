from typing import List, Optional, Dict, Any, Tuple
import time
import json
import uuid
import traceback
from motor.motor_asyncio import AsyncIOMotorDatabase
from pydantic import ValidationError
import logging
import asyncio


from app.core.logging_utils import AppLogger, log_event, LogLevel
from app.services.ai_cache_manager import ai_cache_manager
from app.services.unified_ai_service_simplified import unified_ai_service_simplified, AIResponse as UnifiedAIResponse
from app.services.embedding_service import embedding_service
from app.services.vector_db_service import vector_db_service
from app.models.vector_models import (
    AIQARequest, AIQAResponse, QueryRewriteResult, LLMContextDocument,
    SemanticSearchResult, SemanticContextDocument
)
from app.models.ai_models_simplified import (
    AIQueryRewriteOutput,
    AIMongoDBQueryDetailOutput,
    AIDocumentAnalysisOutputDetail,
    AIGeneratedAnswerOutput,
    AIDocumentSelectionOutput
)
from app.models.document_models import Document
from app.crud.crud_documents import get_documents_by_ids
from app.services.vector_db_service import vector_db_service
from app.services.enhanced_search_service import enhanced_search_service

logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


class SearchWeightConfig:
    """æœç´¢æ¬Šé‡é…ç½®é¡ - çµ±ä¸€ç®¡ç†æ‰€æœ‰æ¬Šé‡ç›¸é—œé‚è¼¯"""
    
    QUERY_TYPE_WEIGHTS = {
        0: 1.3,  # é¡å‹Aï¼šè‡ªç„¶èªè¨€æ‘˜è¦é¢¨æ ¼ - æœ€é«˜æ¬Šé‡
        1: 1.1,  # é¡å‹Bï¼šé—œéµè©å¯†é›†æŸ¥è©¢ - ä¸­ç­‰æ¬Šé‡  
        2: 1.0   # é¡å‹Cï¼šé ˜åŸŸå°ˆæ¥­æŸ¥è©¢ - æ¨™æº–æ¬Šé‡
    }
    
    @classmethod
    def apply_query_weights(cls, results: List[SemanticSearchResult], query_index: int) -> List[SemanticSearchResult]:
        """æ‡‰ç”¨æŸ¥è©¢æ¬Šé‡åˆ°æœç´¢çµæœ"""
        weight = cls.QUERY_TYPE_WEIGHTS.get(query_index, 1.0)
        for result in results:
            result.similarity_score *= weight
        return results
    
    @classmethod
    def get_query_weight(cls, query_index: int) -> float:
        """ç²å–ç‰¹å®šæŸ¥è©¢ç´¢å¼•çš„æ¬Šé‡"""
        return cls.QUERY_TYPE_WEIGHTS.get(query_index, 1.0)
    
    @classmethod
    def merge_weighted_results(cls, all_results_map: Dict[str, SemanticSearchResult], new_results: List[SemanticSearchResult], query_index: int) -> None:
        """åˆä½µåŠ æ¬Šçµæœåˆ°ç¸½çµæœé›†"""
        weight = cls.get_query_weight(query_index)
        
        for result in new_results:
            weighted_score = result.similarity_score * weight
            
            if result.document_id not in all_results_map:
                # å‰µå»ºæ–°çµæœ
                all_results_map[result.document_id] = SemanticSearchResult(
                    document_id=result.document_id,
                    similarity_score=weighted_score,
                    summary_text=result.summary_text,
                    metadata=result.metadata
                )
            else:
                # å·²å­˜åœ¨çš„æ–‡æª”ï¼Œå–æœ€é«˜åˆ†æ•¸
                if weighted_score > all_results_map[result.document_id].similarity_score:
                    all_results_map[result.document_id].similarity_score = weighted_score
                    all_results_map[result.document_id].summary_text = result.summary_text


def remove_projection_path_collisions(projection: dict) -> dict:
    """
    ç§»é™¤ MongoDB projection ä¸­çš„çˆ¶å­æ¬„ä½è¡çªï¼Œåªä¿ç•™æœ€åº•å±¤æ¬„ä½ã€‚
    """
    if not projection or not isinstance(projection, dict):
        return projection
    keys = list(projection.keys())
    keys_to_remove = set()
    for k in keys:
        for other in keys:
            if k == other:
                continue
            # k æ˜¯ other çš„å­æ¬„ä½ï¼Œå‰‡ç§»é™¤çˆ¶æ¬„ä½ other
            if k.startswith(other + "."):
                keys_to_remove.add(other)
            # other æ˜¯ k çš„å­æ¬„ä½ï¼Œå‰‡ç§»é™¤çˆ¶æ¬„ä½ k
            elif other.startswith(k + "."):
                keys_to_remove.add(k)
    for k in keys_to_remove:
        projection.pop(k, None)
    return projection


class EnhancedAIQAService:
    """å¢å¼·çš„AIå•ç­”æœå‹™ - ä½¿ç”¨çµ±ä¸€AIç®¡ç†æ¶æ§‹å’Œå°ˆé–€çš„ç·©å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        # ä½¿ç”¨å°ˆé–€çš„ç·©å­˜ç®¡ç†å™¨
        self.cache_manager = ai_cache_manager
        
        # é…ç½®é¸é …ï¼šæ˜¯å¦ç‚ºAIQAå•Ÿç”¨å…©éšæ®µæ··åˆæª¢ç´¢
        # è¨­ç‚º True ä»¥ç²å¾—æ›´é«˜æº–ç¢ºåº¦ï¼Œè¨­ç‚º False ä¿æŒå‘å¾Œå…¼å®¹
        self.enable_hybrid_search_for_aiqa = True
        
        logger.info("EnhancedAIQAService åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨å°ˆé–€çš„ AI ç·©å­˜ç®¡ç†å™¨")
    
    async def _get_query_vectors(self, queries: List[str]) -> Dict[str, List[float]]:
        """çµ±ä¸€å‘é‡ç²å–æ¥å£ - é€šéç·©å­˜ç®¡ç†å™¨çµ±ä¸€è™•ç†"""
        vectors = {}
        uncached_queries = [q for q in queries if self.cache_manager.get_query_embedding(q) is None]
        
        # æ‰¹æ¬¡è™•ç†æœªç·©å­˜çš„æŸ¥è©¢
        if uncached_queries:
            try:
                query_vectors = embedding_service.encode_batch(uncached_queries)
                self.cache_manager.batch_set_query_embeddings(uncached_queries, query_vectors)
                logger.debug(f"æ‰¹æ¬¡ç”Ÿæˆä¸¦ç·©å­˜äº† {len(uncached_queries)} å€‹æŸ¥è©¢çš„å‘é‡")
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡å‘é‡ç”Ÿæˆå¤±æ•—: {str(e)}")
                # å›é€€åˆ°å–®å€‹è™•ç†
                for query in uncached_queries:
                    try:
                        single_vector = embedding_service.encode_text(query)
                        if single_vector:
                            self.cache_manager.set_query_embedding(query, single_vector)
                    except Exception as single_e:
                        logger.error(f"å–®å€‹æŸ¥è©¢å‘é‡ç”Ÿæˆå¤±æ•— '{query[:30]}...': {str(single_e)}")
        
        # æ”¶é›†æ‰€æœ‰å‘é‡
        for query in queries:
            vector = self.cache_manager.get_query_embedding(query)
            if vector:
                vectors[query] = vector
            else:
                logger.warning(f"ç„¡æ³•ç²å–æŸ¥è©¢å‘é‡: '{query[:30]}...'")
        
        return vectors

    def _extract_search_strategy(self, query_rewrite_result: Optional[QueryRewriteResult]) -> str:
        """æå–æœç´¢ç­–ç•¥ - çµ±ä¸€ç­–ç•¥æ±ºç­–é‚è¼¯"""
        if not query_rewrite_result:
            return "hybrid"
        
        suggested = getattr(query_rewrite_result, 'search_strategy_suggestion', None)
        granularity = getattr(query_rewrite_result, 'query_granularity', None)
        
        # ç­–ç•¥æ˜ å°„é‚è¼¯
        strategy_map = {
            "summary_only": "summary_only",
            "rrf_fusion": "rrf_fusion", 
            "keyword_enhanced_rrf": "rrf_fusion"
        }
        
        # æ ¹æ“šç²’åº¦è‡ªå‹•é¸æ“‡
        if granularity == "thematic":
            return "summary_only"
        elif granularity in ["detailed", "unknown"]:
            return "rrf_fusion"
        
        return strategy_map.get(suggested, "hybrid")

    async def _unified_search(
        self,
        db: AsyncIOMotorDatabase,
        queries: List[str],
        search_strategy: str,
        top_k: int,
        user_id: Optional[str],
        request_id: Optional[str],
        similarity_threshold: float,
        **kwargs
    ) -> List[SemanticSearchResult]:
        """çµ±ä¸€æœç´¢æ¥å£ - èª¿ç”¨ enhanced_search_service æˆ–å›é€€åˆ°åŸºç¤æœç´¢"""
        
        if not queries:
            return []
        
        primary_query = queries[0]
        
        try:
            if search_strategy == "summary_only":
                # æ‘˜è¦å°ˆç”¨æœç´¢ç­–ç•¥
                return await self._summary_only_search_optimized(
                    db, queries, top_k, user_id, request_id, similarity_threshold, **kwargs
                )
            elif search_strategy in ["rrf_fusion", "hybrid"]:
                # ä½¿ç”¨å…©éšæ®µæ··åˆæª¢ç´¢
                if self.enable_hybrid_search_for_aiqa:
                    return await self._hybrid_search_optimized(
                        db, queries, top_k, user_id, request_id, similarity_threshold, **kwargs
                    )
                else:
                    # å›é€€åˆ°å‚³çµ±æœç´¢
                    return await self._legacy_search_optimized(
                        db, queries, top_k, user_id, request_id, similarity_threshold, **kwargs
                    )
            else:
                # æœªçŸ¥ç­–ç•¥ï¼Œä½¿ç”¨æ··åˆæœç´¢
                logger.warning(f"æœªçŸ¥æœç´¢ç­–ç•¥ '{search_strategy}'ï¼Œå›é€€åˆ°æ··åˆæœç´¢")
                return await self._hybrid_search_optimized(
                    db, queries, top_k, user_id, request_id, similarity_threshold, **kwargs
                )
                
        except Exception as e:
            logger.error(f"çµ±ä¸€æœç´¢å¤±æ•—ï¼Œä½¿ç”¨åŸºç¤å›é€€æœç´¢: {str(e)}")
            # æœ€çµ‚å›é€€åˆ°æœ€åŸºç¤çš„æœç´¢
            return await self._basic_fallback_search(
                db, primary_query, top_k, user_id, similarity_threshold
            )

    async def _summary_only_search_optimized(
        self,
        db: AsyncIOMotorDatabase,
        queries: List[str],
        top_k: int,
        user_id: Optional[str],
        request_id: Optional[str],
        similarity_threshold: float,
        **kwargs
    ) -> List[SemanticSearchResult]:
        """å„ªåŒ–çš„æ‘˜è¦å°ˆç”¨æœç´¢"""
        logger.info(f"ğŸ¯ åŸ·è¡Œæ‘˜è¦å°ˆç”¨æœç´¢ï¼ŒæŸ¥è©¢æ•¸é‡: {len(queries)}")
        
        # ç²å–æŸ¥è©¢å‘é‡
        query_vectors = await self._get_query_vectors(queries)
        all_results_map: Dict[str, SemanticSearchResult] = {}
        
        for i, query in enumerate(queries):
            if query not in query_vectors:
                logger.warning(f"è·³éç„¡å‘é‡çš„æŸ¥è©¢: {query[:30]}...")
                continue
            
            query_embedding = query_vectors[query]
            
            # æº–å‚™æ‘˜è¦æœç´¢çš„éæ¿¾å™¨
            summary_metadata_filter = {"type": "summary"}
            document_ids = kwargs.get('document_ids')
            if document_ids:
                summary_metadata_filter["document_id"] = {"$in": document_ids}
            
            # åŸ·è¡Œæ‘˜è¦å‘é‡æœç´¢
            summary_results = await asyncio.to_thread(
                vector_db_service.search_similar_vectors,
                query_vector=query_embedding,
                top_k=top_k * 2,  # å¤šå–ä¸€äº›å€™é¸
                owner_id_filter=user_id,
                metadata_filter=summary_metadata_filter,
                similarity_threshold=similarity_threshold * 0.9  # ç¨å¾®é™ä½é–¾å€¼
            )
            
            # æ‡‰ç”¨æŸ¥è©¢æ¬Šé‡ä¸¦åˆä½µçµæœ
            SearchWeightConfig.merge_weighted_results(all_results_map, summary_results, i)
        
        # æ’åºä¸¦è¿”å›çµæœ
        final_results = list(all_results_map.values())
        final_results.sort(key=lambda x: x.similarity_score, reverse=True)
        
        logger.info(f"æ‘˜è¦å°ˆç”¨æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_results)} å€‹çµæœ")
        return final_results[:top_k]

    async def _hybrid_search_optimized(
        self,
        db: AsyncIOMotorDatabase,
        queries: List[str],
        top_k: int,
        user_id: Optional[str],
        request_id: Optional[str],
        similarity_threshold: float,
        **kwargs
    ) -> List[SemanticSearchResult]:
        """å„ªåŒ–çš„æ··åˆæœç´¢ - ä½¿ç”¨ enhanced_search_service"""
        
        try:
            all_results_map: Dict[str, SemanticSearchResult] = {}
            
            # ç‚ºæ¯å€‹é‡å¯«æŸ¥è©¢åŸ·è¡Œå…©éšæ®µæœç´¢
            for i, query in enumerate(queries):
                logger.debug(f"åŸ·è¡Œç¬¬ {i+1}/{len(queries)} å€‹æŸ¥è©¢çš„æ··åˆæœç´¢: {query[:50]}...")
                
                # åŸ·è¡Œ RRF èåˆæœç´¢
                stage_results = await enhanced_search_service.two_stage_hybrid_search(
                    db=db,
                    query=query,
                    user_id=user_id,
                    search_type="rrf_fusion",
                    stage1_top_k=min(top_k * 2, 15),
                    stage2_top_k=top_k,
                    similarity_threshold=similarity_threshold * 0.8
                )
                
                # æ‡‰ç”¨æŸ¥è©¢æ¬Šé‡ä¸¦åˆä½µçµæœ
                SearchWeightConfig.merge_weighted_results(all_results_map, stage_results, i)
                
                logger.debug(f"æŸ¥è©¢ {i+1} å®Œæˆï¼Œæœ¬æ¬¡æ‰¾åˆ° {len(stage_results)} å€‹çµæœ")
            
            # æœ€çµ‚çµæœæ’åºå’Œå¤šæ¨£æ€§å„ªåŒ–
            final_results = list(all_results_map.values())
            final_results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            # å¤šæ¨£æ€§å„ªåŒ–
            if len(final_results) > top_k:
                final_results = self._apply_diversity_optimization(final_results, top_k)
            
            result_list = final_results[:top_k]
            
            logger.info(f"æ··åˆæœç´¢å®Œæˆ: {len(queries)} å€‹æŸ¥è©¢ â†’ {len(result_list)} å€‹æœ€çµ‚çµæœ")
            
            return result_list
            
        except Exception as e:
            logger.error(f"æ··åˆæœç´¢å¤±æ•—ï¼Œå›é€€åˆ°å‚³çµ±æœç´¢: {str(e)}", exc_info=True)
            return await self._legacy_search_optimized(
                db, queries, top_k, user_id, request_id, similarity_threshold, **kwargs
            )

    async def _legacy_search_optimized(
        self,
        db: AsyncIOMotorDatabase,
        queries: List[str],
        top_k: int,
        user_id: Optional[str],
        request_id: Optional[str],
        similarity_threshold: float,
        **kwargs
    ) -> List[SemanticSearchResult]:
        """å„ªåŒ–çš„å‚³çµ±æœç´¢"""
        
        # ç²å–æŸ¥è©¢å‘é‡
        query_vectors = await self._get_query_vectors(queries)
        all_results_map: Dict[str, SemanticSearchResult] = {}
        
        # æº–å‚™å…ƒæ•¸æ“šéæ¿¾å™¨
        chroma_metadata_filter: Dict[str, Any] = {}
        # é€™è£¡å¯ä»¥æ ¹æ“š kwargs æ·»åŠ æ›´å¤šéæ¿¾é‚è¼¯
        
        for i, query in enumerate(queries):
            if query not in query_vectors:
                continue
                
            query_vector = query_vectors[query]
            adjusted_top_k = min(top_k * 2, 20) if len(queries) > 1 else top_k
            
            # åŸ·è¡Œå‘é‡æœç´¢
            results = vector_db_service.search_similar_vectors(
                query_vector=query_vector, 
                top_k=adjusted_top_k, 
                owner_id_filter=user_id, 
                metadata_filter=chroma_metadata_filter,
                similarity_threshold=similarity_threshold * 0.8
            )
            
            # å¦‚æœå¸¶éæ¿¾æ¢ä»¶çš„æœç´¢æ²’æœ‰çµæœï¼Œå˜—è©¦å›é€€æœç´¢
            if not results and chroma_metadata_filter:
                logger.warning(f"å¸¶ metadata_filter çš„æœç´¢æ²’æœ‰çµæœï¼Œå˜—è©¦å›é€€æœç´¢")
                results = vector_db_service.search_similar_vectors(
                    query_vector=query_vector, 
                    top_k=adjusted_top_k, 
                    owner_id_filter=user_id, 
                    metadata_filter=None,
                    similarity_threshold=similarity_threshold * 0.8
                )
            
            # æ‡‰ç”¨æŸ¥è©¢æ¬Šé‡ä¸¦åˆä½µçµæœ
            SearchWeightConfig.merge_weighted_results(all_results_map, results, i)
        
        # æ’åºå’Œå¤šæ¨£æ€§å„ªåŒ–
        final_results = list(all_results_map.values())
        final_results.sort(key=lambda x: x.similarity_score, reverse=True)
        
        if len(final_results) > top_k:
            final_results = self._apply_diversity_optimization(final_results, top_k)
        
        # æœ€çµ‚éæ¿¾
        final_threshold = similarity_threshold
        final_results = [r for r in final_results if r.similarity_score >= final_threshold]
        
        return final_results[:top_k]

    async def _basic_fallback_search(
        self,
        db: AsyncIOMotorDatabase,
        query: str,
        top_k: int,
        user_id: Optional[str],
        similarity_threshold: float
    ) -> List[SemanticSearchResult]:
        """æœ€åŸºç¤çš„å›é€€æœç´¢"""
        try:
            query_embedding = embedding_service.encode_text(query)
            if not query_embedding or not any(query_embedding):
                logger.error("ç„¡æ³•ç”ŸæˆæŸ¥è©¢çš„åµŒå…¥å‘é‡")
                return []
            
            results = vector_db_service.search_similar_vectors(
                query_vector=query_embedding,
                top_k=top_k,
                owner_id_filter=user_id,
                metadata_filter=None,
                similarity_threshold=similarity_threshold * 0.7  # æ›´å¯¬é¬†çš„é–¾å€¼
            )
            
            logger.info(f"åŸºç¤å›é€€æœç´¢æ‰¾åˆ° {len(results)} å€‹çµæœ")
            return results
            
        except Exception as e:
            logger.error(f"åŸºç¤å›é€€æœç´¢ä¹Ÿå¤±æ•—: {str(e)}")
            return []

    def _apply_diversity_optimization(self, results: List[SemanticSearchResult], top_k: int) -> List[SemanticSearchResult]:
        """æ‡‰ç”¨å¤šæ¨£æ€§å„ªåŒ–ç®—æ³•"""
        diversified_results = []
        seen_summary_keywords = set()
        
        for result in results:
            # æå–æ‘˜è¦çš„é—œéµè©é€²è¡Œå»é‡åˆ¤æ–·
            summary_words = set(result.summary_text.lower().split()[:10])
            overlap = len(summary_words.intersection(seen_summary_keywords))
            
            # å¦‚æœé‡ç–Šåº¦ä¸é«˜ï¼Œæˆ–è€…é‚„æ²’æœ‰è¶³å¤ çš„çµæœï¼Œå‰‡åŠ å…¥
            if overlap < 5 or len(diversified_results) < max(3, top_k // 2):
                diversified_results.append(result)
                seen_summary_keywords.update(summary_words)
                
                if len(diversified_results) >= top_k:
                    break
        
        return diversified_results

    async def process_qa_request(
        self, 
        db: AsyncIOMotorDatabase,
        request: AIQARequest,
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> AIQAResponse:
        """
        è™•ç†AIå•ç­”è«‹æ±‚çš„ä¸»è¦æµç¨‹ - æ”¯æŒç”¨æˆ¶èªè­‰
        """
        user_id_str = str(user_id) if user_id else None
        start_time = time.time()
        total_tokens = 0
        detailed_document_data: Optional[List[Dict[str, Any]]] = None
        ai_generated_query_reasoning: Optional[str] = None
        
        log_details_initial = {
            "question_length": len(request.question) if request.question else 0,
            "document_ids_count": len(request.document_ids) if request.document_ids else 0,
            "model_preference": request.model_preference,
            "use_structured_filter": request.use_structured_filter,
            "use_semantic_search": request.use_semantic_search,
            "session_id": request.session_id
        }
        await log_event(db=db, level=LogLevel.INFO,
                        message="Enhanced AI QA request received.",
                        source="service.enhanced_ai_qa.request", user_id=user_id_str, request_id=request_id,
                        details=log_details_initial)

        query_rewrite_result: Optional[QueryRewriteResult] = None
        semantic_contexts_for_response: List[SemanticContextDocument] = []
        conversation_history_context = ""

        try:
            # === å°è©±è¨˜æ†¶å’Œæ–‡æª”ç·©å­˜è™•ç† ===
            cached_doc_ids = []
            cached_doc_data = None
            
            if request.conversation_id and user_id:
                try:
                    from uuid import UUID
                    from app.services.conversation_cache_service import conversation_cache_service
                    from app.crud import crud_conversations
                    
                    conversation_uuid = UUID(request.conversation_id)
                    # è™•ç† user_id å¯èƒ½æ˜¯ UUID æˆ–å­—ç¬¦ä¸²
                    if isinstance(user_id, UUID):
                        user_uuid = user_id
                    else:
                        user_uuid = UUID(str(user_id)) if user_id else None
                    
                    if user_uuid:
                        # å…ˆå˜—è©¦å¾ Redis ç·©å­˜ç²å–æœ€è¿‘æ¶ˆæ¯
                        recent_messages = await conversation_cache_service.get_recent_messages(
                            user_id=user_uuid,
                            conversation_id=conversation_uuid,
                            limit=5
                        )
                        
                        # å¦‚æœç·©å­˜æœªå‘½ä¸­ï¼Œå¾ MongoDB ç²å–
                        if not recent_messages:
                            recent_messages = await crud_conversations.get_recent_messages(
                                db=db,
                                conversation_id=conversation_uuid,
                                user_id=user_uuid,
                                limit=5
                            )
                        
                        # ç²å–å·²ç·©å­˜çš„æ–‡æª”IDå’Œæ•¸æ“š
                        cached_doc_ids, cached_doc_data = await crud_conversations.get_cached_documents(
                            db=db,
                            conversation_id=conversation_uuid,
                            user_id=user_uuid
                        )
                        
                        if cached_doc_ids:
                            logger.info(f"å°è©±å·²ç·©å­˜ {len(cached_doc_ids)} å€‹æ–‡æª”ï¼Œå°‡å„ªå…ˆä½¿ç”¨")
                        
                        # æ ¼å¼åŒ–å°è©±æ­·å²ç‚ºä¸Šä¸‹æ–‡
                        if recent_messages and len(recent_messages) > 0:
                            conversation_history_context = "\n\n=== å°è©±æ­·å²ï¼ˆæœ€è¿‘ 5 æ¢ï¼‰===\n"
                            for msg in recent_messages:
                                role_name = "ç”¨æˆ¶" if msg.role == "user" else "åŠ©æ‰‹"
                                conversation_history_context += f"{role_name}: {msg.content[:200]}...\n" if len(msg.content) > 200 else f"{role_name}: {msg.content}\n"
                            conversation_history_context += "=== ç•¶å‰å•é¡Œ ===\n"
                            
                            logger.info(f"å·²è¼‰å…¥ {len(recent_messages)} æ¢æ­·å²æ¶ˆæ¯ä½œç‚ºä¸Šä¸‹æ–‡")
                            await log_event(db=db, level=LogLevel.INFO,
                                          message=f"è¼‰å…¥å°è©±è¨˜æ†¶: {len(recent_messages)} æ¢æ­·å²æ¶ˆæ¯, ç·©å­˜æ–‡æª”: {len(cached_doc_ids)}",
                                          source="service.enhanced_ai_qa.conversation_memory",
                                          user_id=user_id_str, request_id=request_id,
                                          details={"conversation_id": request.conversation_id, "messages_count": len(recent_messages), "cached_docs_count": len(cached_doc_ids)})
                except Exception as e:
                    logger.warning(f"è¼‰å…¥å°è©±æ­·å²å¤±æ•—ï¼Œå°‡ç¹¼çºŒè™•ç†: {e}")
                    await log_event(db=db, level=LogLevel.WARNING,
                                  message=f"è¼‰å…¥å°è©±æ­·å²å¤±æ•—: {str(e)}",
                                  source="service.enhanced_ai_qa.conversation_memory_error",
                                  user_id=user_id_str, request_id=request_id)
            
            # === æ™ºèƒ½è§¸ç™¼æµç¨‹ï¼šåŸºæ–¼å‚³çµ±å–®éšæ®µæœç´¢çš„çœŸå¯¦ç›¸ä¼¼åº¦åˆ†æ•¸ ===
            # æ ¹æ“šå¯¦é©—æ•¸æ“šï¼Œå‚³çµ±å–®éšæ®µæœç´¢ï¼ˆæ‘˜è¦+æ–‡æœ¬ç‰‡æ®µï¼‰æ•ˆæœæœ€å¥½ï¼Œæˆ‘å€‘å°‡å…¶ä½œç‚ºç¬¬ä¸€é“é˜²ç·šã€‚
            confidence_threshold = 0.75  # å¯èª¿è¶…åƒæ•¸

            # ç¬¬ä¸€æ­¥ï¼šåŸ·è¡Œå‚³çµ±å–®éšæ®µæœç´¢ (Traditional Single-Stage Search)
            logger.info(f"ğŸ”¬ æ™ºèƒ½è§¸ç™¼ - æ­¥é©Ÿ 1: åŸ·è¡Œå‚³çµ±å–®éšæ®µæœç´¢ (é–€æª»: {confidence_threshold})")
            initial_search_results = await self._perform_traditional_single_stage_search(
                db, 
                original_query=request.question, 
                top_k=getattr(request, 'max_documents_for_selection', request.context_limit),
                user_id=user_id_str, 
                request_id=request_id,
                similarity_threshold=0.3, # åˆå§‹æœç´¢çš„é–¾å€¼å¯ä»¥å¯¬é¬†ä¸€äº›
                document_ids=request.document_ids
            )

            # ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½è§¸ç™¼æ±ºç­–
            use_full_rewrite_flow = True
            top_initial_score = 0.0
            
            if initial_search_results:
                top_initial_score = initial_search_results[0].similarity_score
                logger.info(f"å‚³çµ±å–®éšæ®µæœç´¢æœ€é«˜åˆ† (raw similarity): {top_initial_score:.4f}")
                
                if top_initial_score > confidence_threshold:
                    use_full_rewrite_flow = False
                    logger.info(f"âœ… ç½®ä¿¡åº¦è¶³å¤  ({top_initial_score:.4f} > {confidence_threshold})ï¼Œè·³éAIé‡å¯«å’ŒRRFï¼Œç›´æ¥ä½¿ç”¨åˆå§‹æœç´¢çµæœã€‚")
                else:
                    logger.info(f"ğŸ”„ ç½®ä¿¡åº¦ä¸è¶³ ({top_initial_score:.4f} <= {confidence_threshold})ï¼Œè§¸ç™¼å®Œæ•´AIæŸ¥è©¢é‡å¯«å’ŒRRFèåˆæµç¨‹ã€‚")
            else:
                logger.info("åˆå§‹æœç´¢ç„¡çµæœï¼Œè§¸ç™¼å®Œæ•´AIæŸ¥è©¢é‡å¯«å’ŒRRFèåˆæµç¨‹ã€‚")
            
            # ç¬¬ä¸‰æ­¥ï¼šæ ¹æ“šæ±ºç­–åŸ·è¡Œç›¸æ‡‰æµç¨‹
            if use_full_rewrite_flow:
                # ELSE (top_initial_score <= confidence_threshold) -> Trigger full flow
                logger.info("æ™ºèƒ½è§¸ç™¼ - æ­¥é©Ÿ 2: åŸ·è¡Œå®Œæ•´å„ªåŒ–æµç¨‹ (AIé‡å¯« + RRFæª¢ç´¢)")
                await log_event(db=db, level=LogLevel.INFO,
                                message=f"æ™ºèƒ½è§¸ç™¼ï¼šåŸ·è¡ŒAIé‡å¯«å’ŒRRF (åˆå§‹åˆ†æ•¸: {top_initial_score:.4f})",
                                source="service.enhanced_ai_qa.smart_trigger_full_flow",
                                user_id=user_id_str, request_id=request_id,
                                details={"initial_score": top_initial_score, "confidence_threshold": confidence_threshold, "decision": "TRIGGER_FULL_FLOW"})
                
                query_rewrite_result, rewrite_tokens = await self._rewrite_query_unified(
                    db, request.question, user_id_str, request_id, 
                    query_rewrite_count=getattr(request, 'query_rewrite_count', 3)
                )
                total_tokens += rewrite_tokens
                
                # ä½¿ç”¨é‡å¯«æŸ¥è©¢é€²è¡Œå„ªåŒ–æª¢ç´¢ (RRF)
                semantic_results_raw = await self._perform_optimized_search_direct(
                    db, query_rewrite_result.rewritten_queries if query_rewrite_result.rewritten_queries else [request.question],
                    getattr(request, 'max_documents_for_selection', request.context_limit),
                    user_id_str, request_id, query_rewrite_result,
                    getattr(request, 'similarity_threshold', 0.3),
                    getattr(request, 'enable_query_expansion', True)
                )
            else:
                # IF top_initial_score > confidence_threshold -> Skip full flow, use initial search results
                logger.info("æ™ºèƒ½è§¸ç™¼ - æ­¥é©Ÿ 2: è·³éå®Œæ•´å„ªåŒ–æµç¨‹ï¼Œç›´æ¥ä½¿ç”¨åˆå§‹æœç´¢çµæœ")
                await log_event(db=db, level=LogLevel.INFO,
                                message=f"æ™ºèƒ½è§¸ç™¼ï¼šè·³éAIé‡å¯«å’ŒRRF (åˆå§‹åˆ†æ•¸: {top_initial_score:.4f})",
                                source="service.enhanced_ai_qa.smart_trigger_probe_skip",
                                user_id=user_id_str, request_id=request_id,
                                details={"initial_score": top_initial_score, "confidence_threshold": confidence_threshold, "decision": "SKIP_FULL_FLOW", "cost_saving": "YES"})
                
                # å‰µå»ºä¸€å€‹ç°¡å–®çš„æŸ¥è©¢é‡å¯«çµæœä»¥ä¿æŒå…¼å®¹æ€§
                query_rewrite_result = QueryRewriteResult(
                    original_query=request.question,
                    rewritten_queries=[request.question],
                    extracted_parameters={},
                    intent_analysis="æ™ºèƒ½è§¸ç™¼è·³éé‡å¯«ï¼Œä½¿ç”¨å‚³çµ±å–®éšæ®µæœç´¢çµæœ"
                )
                semantic_results_raw = initial_search_results
                document_ids = []

            if semantic_results_raw:
                for res in semantic_results_raw:
                    semantic_contexts_for_response.append(
                        SemanticContextDocument(
                            document_id=res.document_id,
                            summary_or_chunk_text=res.summary_text,
                            similarity_score=res.similarity_score,
                            metadata=res.metadata
                        )
                    )
            
            if not semantic_results_raw:
                logger.warning("å‘é‡æœç´¢æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”")
                return AIQAResponse(
                    answer="æŠ±æ­‰ï¼Œæˆ‘åœ¨æ‚¨çš„æ–‡æª”åº«ä¸­æ²’æœ‰æ‰¾åˆ°èˆ‡æ‚¨å•é¡Œç›¸é—œçš„å…§å®¹ã€‚",
                    source_documents=[],
                    confidence_score=0.0,
                    tokens_used=total_tokens,
                    processing_time=time.time() - start_time,
                    query_rewrite_result=query_rewrite_result,
                    semantic_search_contexts=semantic_contexts_for_response,
                    session_id=request.session_id
                )
            
            document_ids = [result.document_id for result in semantic_results_raw]
            
            # å„ªå…ˆä½¿ç”¨å°è©±ä¸­å·²ç·©å­˜çš„æ–‡æª”
            if cached_doc_ids and len(cached_doc_ids) > 0:
                # åˆä½µç·©å­˜çš„æ–‡æª”IDå’Œæ–°æœç´¢åˆ°çš„æ–‡æª”ID
                all_doc_ids = list(set(cached_doc_ids + document_ids))
                logger.info(f"åˆä½µç·©å­˜æ–‡æª” ({len(cached_doc_ids)}) å’Œæ–°æœç´¢æ–‡æª” ({len(document_ids)})ï¼Œç¸½è¨ˆ {len(all_doc_ids)} å€‹æ–‡æª”")
                document_ids = all_doc_ids
            
            if not isinstance(document_ids, list):
                logger.error(f"Before get_documents_by_ids, document_ids is not a list, but {type(document_ids)}. Defaulting to empty list.")
                document_ids = []

            full_documents = await get_documents_by_ids(db, document_ids)
            
            if user_id:
                full_documents = await self._filter_accessible_documents(db, full_documents, user_id_str, request_id)
            
            if not full_documents:
                logger.warning("ç”¨æˆ¶ç„¡æ¬Šé™è¨ªå•ç›¸é—œæ–‡æª”æˆ–ç²å–æ–‡æª”å…§å®¹å¤±æ•—")
                return AIQAResponse(
                    answer="æ‰¾åˆ°äº†ç›¸é—œæ–‡æª”ï¼Œä½†æ‚¨å¯èƒ½æ²’æœ‰è¨ªå•æ¬Šé™ï¼Œæˆ–ç²å–è©³ç´°å…§å®¹æ™‚å‡ºç¾å•é¡Œã€‚",
                    source_documents=[],
                    confidence_score=0.3,
                    tokens_used=total_tokens,
                    processing_time=time.time() - start_time,
                    query_rewrite_result=query_rewrite_result,
                    semantic_search_contexts=semantic_contexts_for_response,
                    session_id=request.session_id,
                    ai_generated_query_reasoning=ai_generated_query_reasoning,
                    detailed_document_data_from_ai_query=detailed_document_data
                )

            # --- Refactored: Two-Stage Smart Context Generation ---
            all_detailed_data: List[Dict[str, Any]] = []
            if full_documents:
                # Stage 1: AI æ™ºæ…§ç¯©é¸æœ€ä½³æ–‡ä»¶ï¼ˆä½¿ç”¨ç”¨æˆ¶è¨­å®šçš„åƒæ•¸ï¼‰
                selected_doc_ids_for_detail = await self._select_documents_for_detailed_query(
                    db, request.question, semantic_contexts_for_response, 
                    user_id_str, request_id,
                    ai_selection_limit=getattr(request, 'ai_selection_limit', 3),
                    similarity_threshold=getattr(request, 'similarity_threshold', 0.3)
                )

                logger.info(f"AI é¸æ“‡äº† {len(selected_doc_ids_for_detail)} å€‹æ–‡ä»¶é€²è¡Œè©³ç´°æŸ¥è©¢: {selected_doc_ids_for_detail}")

                if selected_doc_ids_for_detail:
                    full_documents_map = {str(doc.id): doc for doc in full_documents}
                    
                    document_schema_info = {
                        "description": "é€™æ˜¯å„²å­˜åœ¨ MongoDB ä¸­çš„å–®ä¸€æ–‡ä»¶ Schemaã€‚æ‚¨çš„æŸ¥è©¢å°‡é‡å° 'Target Document ID' æ‰€æŒ‡å®šçš„å–®ä¸€æ–‡ä»¶é€²è¡Œæ“ä½œã€‚",
                        "fields": {
                            "id": "UUID (å­—ä¸²), æ–‡ä»¶çš„å”¯ä¸€æ¨™è­˜ç¬¦ã€‚é€™å€‹ ID å·²ç¶“è¢«ç”¨ä¾†å®šä½æ–‡ä»¶ï¼Œæ‚¨çš„æŸ¥è©¢ä¸éœ€è¦å†éæ¿¾ `_id`ã€‚",
                            "filename": "å­—ä¸², åŸå§‹æ–‡ä»¶åã€‚",
                            "file_type": "å­—ä¸², æ–‡ä»¶çš„ MIME é¡å‹ã€‚",
                            "content_type_human_readable": "å­—ä¸², äººé¡å¯è®€çš„æ–‡ä»¶é¡å‹ï¼Œä¾‹å¦‚ 'PDF document', 'Word document', 'Email'ã€‚",
                            "extracted_text": "å­—ä¸², å¾æ–‡ä»¶ä¸­æå–çš„å®Œæ•´æ–‡å­—å…§å®¹ã€‚å¯èƒ½éå¸¸é•·ï¼Œå¦‚æœ‰éœ€è¦ï¼Œè«‹ä½¿ç”¨æ­£å‰‡è¡¨é”å¼é€²è¡Œéƒ¨åˆ†åŒ¹é…ã€‚",
                            "analysis": {
                                "type": "object",
                                "description": "åŒ…å«å°æ–‡ä»¶é€²è¡Œ AI åˆ†æå¾Œç”¢ç”Ÿçš„çµæœã€‚",
                                "properties": {
                                    "ai_analysis_output": {
                                        "type": "object",
                                        "description": "é€™æ˜¯å…ˆå‰ AI åˆ†æä»»å‹™ï¼ˆåŸºæ–¼ AITextAnalysisOutput æˆ– AIImageAnalysisOutput æ¨¡å‹ï¼‰ç”¢ç”Ÿçš„æ ¸å¿ƒçµæ§‹åŒ–è¼¸å‡ºã€‚é€™æ˜¯æœ€è©³ç´°ã€æœ€æœ‰åƒ¹å€¼çš„è³‡æ–™ä¾†æºã€‚",
                                        "properties": {
                                            "initial_summary": "å­—ä¸², å°æ–‡ä»¶çš„åˆæ­¥æ‘˜è¦ã€‚",
                                            "content_type": "å­—ä¸², AI è­˜åˆ¥çš„å…§å®¹é¡å‹ã€‚",
                                            "key_information": {
                                                "type": "object",
                                                "description": "é€™æ˜¯åŸºæ–¼ `FlexibleKeyInformation` æ¨¡å‹æå–çš„æœ€é‡è¦çš„çµæ§‹åŒ–è³‡è¨Šã€‚è©³ç´°æŸ¥è©¢æ‡‰å„ªå…ˆé‡å°æ­¤å°è±¡ã€‚",
                                                "properties": {
                                                    "content_summary": "å­—ä¸², ç´„ 2-3 å¥è©±çš„å…§å®¹æ‘˜è¦ï¼Œéå¸¸é©åˆå›ç­”ç¸½çµæ€§å•é¡Œã€‚",
                                                    "semantic_tags": "å­—ä¸²åˆ—è¡¨, ç”¨æ–¼èªç¾©æœç´¢çš„æ¨™ç±¤ã€‚",
                                                    "main_topics": "å­—ä¸²åˆ—è¡¨, æ–‡ä»¶è¨è«–çš„ä¸»è¦ä¸»é¡Œã€‚",
                                                    "key_concepts": "å­—ä¸²åˆ—è¡¨, æåˆ°çš„æ ¸å¿ƒæ¦‚å¿µã€‚",
                                                    "action_items": "å­—ä¸²åˆ—è¡¨, æ–‡ä»¶ä¸­æåˆ°çš„å¾…è¾¦äº‹é …ã€‚",
                                                    "dates_mentioned": "å­—ä¸²åˆ—è¡¨, æåŠçš„æ—¥æœŸã€‚",
                                                    "dynamic_fields": {
                                                        "type": "object",
                                                        "description": "ç”± AI æ ¹æ“šæ–‡ä»¶å…§å®¹å‹•æ…‹ç”Ÿæˆçš„æ¬„ä½å­—å…¸ã€‚å¦‚æœç”¨æˆ¶çš„å•é¡Œæš—ç¤ºäº†ç‰¹å®šçš„è³‡è¨Šï¼ˆä¾‹å¦‚ã€å°ˆæ¡ˆç¶“ç†æ˜¯èª°ï¼Ÿã€ï¼‰ï¼Œæ‚¨å¯ä»¥å˜—è©¦æŸ¥è©¢é€™è£¡çš„éµï¼Œä¾‹å¦‚ `analysis.ai_analysis_output.key_information.dynamic_fields.project_manager`ã€‚"
                                                    }
                                                }
                                            }
                                        }
                                    },
                                    "summary": "å­—ä¸², ä¸€å€‹è¼ƒèˆŠçš„æˆ–ç”±ç”¨æˆ¶æä¾›çš„æ‘˜è¦ã€‚",
                                    "key_terms": "å­—ä¸²åˆ—è¡¨, å¾æ–‡ä»¶ä¸­æå–çš„é—œéµè¡“èªã€‚"
                                }
                            },
                            "tags": "å­—ä¸²åˆ—è¡¨, ç”¨æˆ¶è‡ªå®šç¾©çš„æ¨™ç±¤ã€‚",
                            "metadata": "ç‰©ä»¶, å…¶ä»–å…ƒæ•¸æ“šï¼ˆä¾‹å¦‚ï¼Œä¾†æº URLã€ä½œè€…ï¼‰ã€‚çµæ§‹å¯èƒ½ä¸åŒã€‚",
                            "created_at": "æ—¥æœŸæ™‚é–“å­—ä¸² (ISO æ ¼å¼), æ–‡ä»¶è¨˜éŒ„çš„å‰µå»ºæ™‚é–“ã€‚",
                            "updated_at": "æ—¥æœŸæ™‚é–“å­—ä¸² (ISO æ ¼å¼), æ–‡ä»¶è¨˜éŒ„çš„æœ€å¾Œæ›´æ–°æ™‚é–“ã€‚"
                        },
                        "query_notes": "æ‚¨çš„ç›®æ¨™æ˜¯ç”Ÿæˆ 'projection' ä¾†é¸æ“‡ç‰¹å®šæ¬„ä½ï¼Œå’Œ/æˆ– 'sub_filter' ä¾†å°æ–‡ä»¶å…§çš„æ¬„ä½æ–½åŠ æ¢ä»¶ã€‚ä¾‹å¦‚ï¼Œåœ¨ 'extracted_text' ä¸Šä½¿ç”¨æ­£å‰‡è¡¨é”å¼ï¼Œæˆ–åŒ¹é… 'analysis.ai_analysis_output.key_information.semantic_tags' é™£åˆ—ä¸­çš„å…ƒç´ ã€‚ä¸è¦ç‚º `_id` ç”Ÿæˆéæ¿¾å™¨ï¼Œå› ç‚ºé€™å·²ç¶“è™•ç†å¥½äº†ã€‚"
                    }

                    # Stage 2: å°æ¯å€‹è¢«é¸ä¸­çš„æ–‡ä»¶åŸ·è¡Œè©³ç´°æŸ¥è©¢
                    for doc_id in selected_doc_ids_for_detail:
                        if doc_id in full_documents_map:
                            target_document: Document = full_documents_map[doc_id]
                            logger.info(f"å°æ–‡ä»¶ {doc_id} ({target_document.filename if hasattr(target_document, 'filename') else 'Unknown'}) åŸ·è¡Œè©³ç´°æŸ¥è©¢")
                            
                            ai_query_response = await unified_ai_service_simplified.generate_mongodb_detail_query(
                                user_question=request.question,
                                document_id=str(target_document.id),
                                document_schema_info=document_schema_info,
                                db=db,
                                model_preference=request.model_preference,
                                user_id=user_id_str,
                                session_id=request.session_id
                            )

                            if ai_query_response.success and ai_query_response.output_data and isinstance(ai_query_response.output_data, AIMongoDBQueryDetailOutput):
                                query_components = ai_query_response.output_data
                                if not ai_generated_query_reasoning:  # å–ç¬¬ä¸€å€‹æ–‡ä»¶çš„æ¨ç†ä½œç‚ºç¤ºä¾‹
                                    ai_generated_query_reasoning = query_components.reasoning

                                mongo_filter = {"_id": target_document.id}
                                mongo_projection = query_components.projection

                                if query_components.sub_filter:
                                    mongo_filter.update(query_components.sub_filter)

                                if mongo_projection or query_components.sub_filter:
                                    # å˜—è©¦ AI ç”Ÿæˆçš„è©³ç´°æŸ¥è©¢
                                    logger.debug(f"åŸ·è¡ŒAIæŸ¥è©¢ - Filter: {mongo_filter}, Projection: {mongo_projection}")
                                    safe_projection = remove_projection_path_collisions(mongo_projection) if mongo_projection else None
                                    fetched_data = await db.documents.find_one(mongo_filter, projection=safe_projection)
                                    
                                    if fetched_data:
                                        def sanitize(data: Any) -> Any:
                                            if isinstance(data, dict): return {k: sanitize(v) for k, v in data.items()}
                                            if isinstance(data, list): return [sanitize(i) for i in data]
                                            if isinstance(data, uuid.UUID): return str(data)
                                            return data
                                        all_detailed_data.append(sanitize(fetched_data))
                                        logger.info(f"æˆåŠŸç²å–æ–‡ä»¶ {doc_id} çš„è©³ç´°è³‡æ–™")
                                    else:
                                        # å›é€€ç­–ç•¥ï¼šä½¿ç”¨åŸºæœ¬æŸ¥è©¢
                                        logger.warning(f"æ–‡ä»¶ {doc_id} çš„AIè©³ç´°æŸ¥è©¢æ²’æœ‰è¿”å›è³‡æ–™ï¼Œå˜—è©¦å›é€€æŸ¥è©¢")
                                        fallback_filter = {"_id": target_document.id}
                                        fallback_projection = {
                                            "_id": 1,
                                            "filename": 1,
                                            "extracted_text": 1,
                                            "analysis.ai_analysis_output.key_information.content_summary": 1,
                                            "analysis.ai_analysis_output.key_information.semantic_tags": 1,
                                            "analysis.ai_analysis_output.key_information.key_concepts": 1
                                        }
                                        safe_fallback_projection = remove_projection_path_collisions(fallback_projection)
                                        fallback_data = await db.documents.find_one(fallback_filter, projection=safe_fallback_projection)
                                        if fallback_data:
                                            def sanitize(data: Any) -> Any:
                                                if isinstance(data, dict): return {k: sanitize(v) for k, v in data.items()}
                                                if isinstance(data, list): return [sanitize(i) for i in data]
                                                if isinstance(data, uuid.UUID): return str(data)
                                                return data
                                            all_detailed_data.append(sanitize(fallback_data))
                                            logger.info(f"å›é€€æŸ¥è©¢æˆåŠŸç²å–æ–‡ä»¶ {doc_id} çš„åŸºæœ¬è³‡æ–™")
                                        else:
                                            logger.error(f"æ–‡ä»¶ {doc_id} é€£åŸºæœ¬æŸ¥è©¢éƒ½å¤±æ•—ï¼Œå¯èƒ½æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ¬Šé™å•é¡Œ")
                                else:
                                    logger.info(f"æ–‡ä»¶ {doc_id} çš„æŸ¥è©¢çµ„ä»¶ç‚ºç©ºï¼Œè·³éè©³ç´°æŸ¥è©¢")
                            elif ai_query_response.error_message:
                                logger.error(f"æ–‡ä»¶ {doc_id} çš„ AI è©³ç´°æŸ¥è©¢å¤±æ•—: {ai_query_response.error_message}")
                        else:
                            logger.warning(f"é¸æ“‡çš„æ–‡ä»¶ ID {doc_id} åœ¨å¯è¨ªå•æ–‡ä»¶ä¸­æœªæ‰¾åˆ°")
                else:
                    logger.info("AI æ²’æœ‰é¸æ“‡ä»»ä½•æ–‡ä»¶é€²è¡Œè©³ç´°æŸ¥è©¢ï¼Œå°‡ä½¿ç”¨é€šç”¨ä¸Šä¸‹æ–‡")
            
            detailed_document_data = all_detailed_data if all_detailed_data else None
            logger.info(f"ç¸½å…±ç²å¾— {len(all_detailed_data) if all_detailed_data else 0} å€‹æ–‡ä»¶çš„è©³ç´°è³‡æ–™")
            # --- End of Smart Context Generation ---

            answer, answer_tokens, confidence, actual_contexts_for_llm = await self._generate_answer_unified(
                db,
                request.question,
                full_documents,
                query_rewrite_result,
                detailed_document_data, # Pass the list of detailed data
                ai_generated_query_reasoning,
                user_id_str,
                request_id,
                request.model_preference,
                ensure_chinese_output=getattr(request, 'ensure_chinese_output', None),
                detailed_text_max_length=getattr(request, 'detailed_text_max_length', 8000),  # å‚³éç”¨æˆ¶è¨­å®šçš„æ–‡æœ¬é•·åº¦é™åˆ¶
                max_chars_per_doc=getattr(request, 'max_chars_per_doc', None),  # å‚³éç”¨æˆ¶è¨­å®šçš„å–®æ–‡æª”é™åˆ¶
                conversation_history=conversation_history_context  # å‚³éå°è©±æ­·å²ä¸Šä¸‹æ–‡
            )
            total_tokens += answer_tokens
            
            processing_time = time.time() - start_time
            
            # === ä¿å­˜å•ç­”å°åˆ°å°è©±ä¸­ ===
            if request.conversation_id and user_id:
                try:
                    from uuid import UUID
                    from app.crud import crud_conversations
                    from app.services.conversation_cache_service import conversation_cache_service
                    
                    logger.info(f"é–‹å§‹ä¿å­˜å°è©±: conversation_id={request.conversation_id}, user_id={user_id}")
                    
                    conversation_uuid = UUID(request.conversation_id)
                    # user_id å¯èƒ½å·²ç¶“æ˜¯ UUID å°è±¡
                    if isinstance(user_id, UUID):
                        user_uuid = user_id
                    else:
                        user_uuid = UUID(str(user_id)) if user_id else None
                    
                    logger.info(f"UUID è½‰æ›æˆåŠŸ: conversation_uuid={conversation_uuid}, user_uuid={user_uuid}")
                    
                    if user_uuid:
                        # æ·»åŠ ç”¨æˆ¶å•é¡Œ
                        logger.info(f"æº–å‚™æ·»åŠ ç”¨æˆ¶å•é¡Œ: {request.question[:50]}...")
                        result1 = await crud_conversations.add_message_to_conversation(
                            db=db,
                            conversation_id=conversation_uuid,
                            user_id=user_uuid,
                            role="user",
                            content=request.question,
                            tokens_used=None
                        )
                        logger.info(f"ç”¨æˆ¶å•é¡Œæ·»åŠ çµæœ: {result1}")
                        
                        # æ·»åŠ  AI å›ç­”
                        logger.info(f"æº–å‚™æ·»åŠ  AI å›ç­”: {answer[:50]}...")
                        result2 = await crud_conversations.add_message_to_conversation(
                            db=db,
                            conversation_id=conversation_uuid,
                            user_id=user_uuid,
                            role="assistant",
                            content=answer,
                            tokens_used=answer_tokens
                        )
                        logger.info(f"AI å›ç­”æ·»åŠ çµæœ: {result2}")
                        
                        # æ›´æ–°å°è©±çš„æ–‡æª”ç·©å­˜
                        if full_documents:
                            new_doc_ids = [str(doc.id) for doc in full_documents]
                            logger.info(f"æº–å‚™æ›´æ–°æ–‡æª”ç·©å­˜: {len(new_doc_ids)} å€‹æ–‡æª”ï¼ŒIDs: {new_doc_ids}")
                            cache_result = await crud_conversations.update_cached_documents(
                                db=db,
                                conversation_id=conversation_uuid,
                                user_id=user_uuid,
                                document_ids=new_doc_ids,
                                document_data=None
                            )
                            logger.info(f"æ–‡æª”ç·©å­˜æ›´æ–°çµæœ: {cache_result}")
                        else:
                            logger.warning("æ²’æœ‰ full_documents å¯ä»¥ç·©å­˜")
                        
                        # ä½¿ç·©å­˜å¤±æ•ˆï¼Œä¸‹æ¬¡æœƒå¾ MongoDB é‡æ–°è¼‰å…¥
                        await conversation_cache_service.invalidate_conversation(
                            user_id=user_uuid,
                            conversation_id=conversation_uuid
                        )
                        
                        logger.info(f"âœ… æˆåŠŸä¿å­˜å•ç­”å°åˆ°å°è©± {request.conversation_id}")
                    else:
                        logger.error(f"user_uuid ç‚º Noneï¼Œç„¡æ³•ä¿å­˜å°è©±")
                except Exception as e:
                    import traceback
                    logger.error(f"âŒ ä¿å­˜å°è©±å¤±æ•—: {e}")
                    logger.error(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
                    # ä¸å½±éŸ¿ä¸»æµç¨‹ï¼Œç¹¼çºŒè¿”å›çµæœ
            else:
                logger.warning(f"è·³éä¿å­˜å°è©±: conversation_id={request.conversation_id}, user_id={user_id}")
            
            logger.info(f"AIå•ç­”è™•ç†å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’ï¼ŒToken: {total_tokens} (ç”¨æˆ¶: {user_id})")
            
            return AIQAResponse(
                answer=answer,
                source_documents=[str(doc.id) for doc in full_documents],
                confidence_score=confidence,
                tokens_used=total_tokens,
                processing_time=processing_time,
                query_rewrite_result=query_rewrite_result,
                semantic_search_contexts=semantic_contexts_for_response,
                session_id=request.session_id,
                llm_context_documents=actual_contexts_for_llm,
                ai_generated_query_reasoning=ai_generated_query_reasoning,
                detailed_document_data_from_ai_query=detailed_document_data
            )
            
        except Exception as e:
            processing_time_on_error = time.time() - start_time
            error_trace = traceback.format_exc()
            await log_event(db=db, level=LogLevel.ERROR, message=f"Enhanced AI QA failed: {str(e)}",
                            source="service.enhanced_ai_qa.process_request_error", user_id=user_id_str, request_id=request_id,
                            details={"error": str(e), "error_type": type(e).__name__, "traceback": error_trace, **log_details_initial})
            
            current_total_tokens = total_tokens if isinstance(total_tokens, int) else 0
            current_qrr = query_rewrite_result if isinstance(query_rewrite_result, QueryRewriteResult) else QueryRewriteResult(original_query=request.question, rewritten_queries=[request.question], extracted_parameters={}, intent_analysis="Error before QRR.")
            current_semantic_contexts = semantic_contexts_for_response if isinstance(semantic_contexts_for_response, list) else []

            return AIQAResponse(
                answer=f"An error occurred: {str(e)}", source_documents=[], confidence_score=0.0, tokens_used=current_total_tokens,
                processing_time=processing_time_on_error, query_rewrite_result=current_qrr,
                semantic_search_contexts=current_semantic_contexts, session_id=request.session_id,
                llm_context_documents=[], ai_generated_query_reasoning=ai_generated_query_reasoning,
                detailed_document_data_from_ai_query=detailed_document_data, error_message=str(e) 
            )

    async def _filter_accessible_documents(self, db: AsyncIOMotorDatabase, full_documents: List[Any], user_id_str: Optional[str], request_id: Optional[str]) -> List[Any]:
        if not user_id_str: return full_documents
        try:
            user_uuid = uuid.UUID(user_id_str)
            accessible_documents = [doc for doc in full_documents if hasattr(doc, 'owner_id') and doc.owner_id == user_uuid]
            if not accessible_documents:
                logger.warning("ç”¨æˆ¶ç„¡æ¬Šé™è¨ªå•ç›¸é—œæ–‡æª”æˆ–ç²å–æ–‡æª”å…§å®¹å¤±æ•—")
                return []
            return accessible_documents
        except ValueError:
            await log_event(db=db, level=LogLevel.ERROR, message=f"Invalid user_id format for access filtering: {user_id_str}", source="service.enhanced_ai_qa._filter_accessible_documents", user_id=user_id_str, request_id=request_id)
            return []

    async def _select_documents_for_detailed_query(
        self,
        db: AsyncIOMotorDatabase,
        user_question: str,
        semantic_contexts: List[SemanticContextDocument],
        user_id: Optional[str],
        request_id: Optional[str],
        ai_selection_limit: int,
        similarity_threshold: float
    ) -> List[str]:
        """
        ä½¿ç”¨ AI å¾å€™é¸æ–‡ä»¶ä¸­æ™ºæ…§é¸æ“‡æœ€ç›¸é—œçš„æ–‡ä»¶é€²è¡Œè©³ç´°æŸ¥è©¢ï¼Œ
        åŒ…å«å»é‡é‚è¼¯å’Œå‹•æ…‹æ•¸é‡æ±ºç­–
        """
        if not semantic_contexts:
            return []

        # ç¬¬ä¸€æ­¥ï¼šæ ¹æ“šåˆ†æ•¸é€²è¡Œå»é‡
        # RRFèåˆæœç´¢çš„çµæœå·²ç¶“éæ’åºå’Œåˆæ­¥ç¯©é¸ï¼Œæ­¤è™•ä¸»è¦é€²è¡Œå»é‡
        filtered_contexts = {}
        
        for ctx in semantic_contexts:
            # ç›¸ä¼¼åº¦ç¯©é¸ - ç”±æ–¼RRFåˆ†æ•¸èˆ‡ç›¸ä¼¼åº¦é–¾å€¼ä¸å¯æ¯”ï¼Œç§»é™¤æ­¤éæ¿¾
            # if ctx.similarity_score < similarity_threshold:
            #     continue
                
            # å»é‡ï¼šå¦‚æœåŒä¸€å€‹æ–‡ä»¶æœ‰å¤šå€‹ç‰‡æ®µï¼Œé¸æ“‡ç›¸ä¼¼åº¦ï¼ˆæˆ–RRFåˆ†æ•¸ï¼‰æœ€é«˜çš„
            if ctx.document_id not in filtered_contexts or ctx.similarity_score > filtered_contexts[ctx.document_id].similarity_score:
                filtered_contexts[ctx.document_id] = ctx
        
        # æŒ‰åˆ†æ•¸æ’åºï¼ˆRRFåˆ†æ•¸è¶Šé«˜è¶Šå¥½ï¼‰
        unique_contexts = sorted(filtered_contexts.values(), key=lambda x: x.similarity_score, reverse=True)
        
        # ç¬¬äºŒæ­¥ï¼šå‹•æ…‹æ±ºå®šè¦æä¾›çµ¦AIçš„å€™é¸æ•¸é‡
        max_candidates = min(ai_selection_limit * 2, len(unique_contexts))  # æä¾›çµ¦AIçš„å€™é¸æ•¸æ˜¯é¸æ“‡é™åˆ¶çš„2å€
        candidates_for_ai = unique_contexts[:max_candidates]
        
        if len(candidates_for_ai) < 2:
            logger.info(f"å€™é¸æ–‡ä»¶æ•¸é‡ä¸è¶³ï¼ˆ{len(candidates_for_ai)}ï¼‰ï¼Œè·³éAIé¸æ“‡ï¼Œç›´æ¥è¿”å›æ‰€æœ‰å€™é¸")
            return [ctx.document_id for ctx in candidates_for_ai]

        # æº–å‚™å€™é¸æ–‡ä»¶è³‡æ–™çµ¦AIåˆ†æ
        candidate_docs_for_ai = [
            {
                "document_id": ctx.document_id, 
                "summary": ctx.summary_or_chunk_text,
                "similarity_score": ctx.similarity_score
            }
            for ctx in candidates_for_ai
        ]

        await log_event(db=db, level=LogLevel.INFO,
                        message=f"ç¶“éå»é‡å’Œç¯©é¸å¾Œï¼Œæº–å‚™è«‹AIå¾ {len(candidate_docs_for_ai)} å€‹å€™é¸æ–‡ä»¶ä¸­é¸æ“‡æœ€ç›¸é—œçš„é€²è¡Œè©³ç´°æŸ¥è©¢ï¼ˆç”¨æˆ¶é™åˆ¶ï¼š{ai_selection_limit}å€‹ï¼‰",
                        source="service.enhanced_ai_qa._select_documents_for_detailed_query",
                        user_id=user_id, request_id=request_id,
                        details={"candidates": [{"id": doc["document_id"], "score": doc["similarity_score"]} for doc in candidate_docs_for_ai], "user_selection_limit": ai_selection_limit})

        selection_response = await unified_ai_service_simplified.select_documents_for_detailed_query(
            user_question=user_question,
            candidate_documents=candidate_docs_for_ai,
            db=db,
            user_id=user_id,
            session_id=request_id,
            max_selections=ai_selection_limit  # å‚³éç”¨æˆ¶çš„é¸æ“‡é™åˆ¶
        )

        if selection_response.success and isinstance(selection_response.output_data, AIDocumentSelectionOutput):
            selected_ids = selection_response.output_data.selected_document_ids
            reasoning = selection_response.output_data.reasoning
            
            # é©—è­‰é¸æ“‡çš„æ–‡ä»¶IDæ˜¯å¦æœ‰æ•ˆ
            valid_candidate_ids = {doc["document_id"] for doc in candidate_docs_for_ai}
            validated_selected_ids = [doc_id for doc_id in selected_ids if doc_id in valid_candidate_ids]
            
            if len(validated_selected_ids) != len(selected_ids):
                dropped_ids = set(selected_ids) - set(validated_selected_ids)
                logger.warning(f"AIé¸æ“‡äº†ä¸€äº›ç„¡æ•ˆçš„æ–‡ä»¶IDï¼Œå·²éæ¿¾æ‰: {dropped_ids}")
            
            await log_event(db=db, level=LogLevel.INFO,
                            message=f"AIæ™ºæ…§é¸æ“‡äº† {len(validated_selected_ids)} å€‹æ–‡ä»¶é€²è¡Œè©³ç´°æŸ¥è©¢",
                            source="service.enhanced_ai_qa._select_documents_for_detailed_query",
                            user_id=user_id, request_id=request_id,
                            details={
                                "selected_ids": validated_selected_ids, 
                                "reasoning": reasoning,
                                "original_candidates": len(semantic_contexts),
                                "after_dedup_filter": len(candidates_for_ai),
                                "final_selected": len(validated_selected_ids)
                            })
            
            return validated_selected_ids
        else:
            await log_event(db=db, level=LogLevel.WARNING,
                            message=f"AIæ–‡ä»¶é¸æ“‡å¤±æ•—ï¼Œå›é€€ç­–ç•¥ï¼šé¸æ“‡ç›¸ä¼¼åº¦æœ€é«˜çš„å‰{min(ai_selection_limit, len(candidates_for_ai))}å€‹æ–‡ä»¶",
                            source="service.enhanced_ai_qa._select_documents_for_detailed_query",
                            user_id=user_id, request_id=request_id,
                            details={"error": selection_response.error_message, "fallback_count": min(ai_selection_limit, len(candidates_for_ai))})
            
            # å›é€€ç­–ç•¥ï¼šæ ¹æ“šç”¨æˆ¶è¨­å®šé¸æ“‡ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡ä»¶
            fallback_count = min(ai_selection_limit, len(candidates_for_ai))
            fallback_selection = [ctx.document_id for ctx in candidates_for_ai[:fallback_count]]
            return fallback_selection

    async def _rewrite_query_unified(self, db: AsyncIOMotorDatabase, original_query: str, user_id: Optional[str], request_id: Optional[str], query_rewrite_count: int) -> Tuple[QueryRewriteResult, int]:
        """çµ±ä¸€çš„æŸ¥è©¢é‡å¯«æ–¹æ³• - æ”¯æŒæ–°çš„æ™ºèƒ½æ„åœ–åˆ†æå’Œå‹•æ…‹ç­–ç•¥è·¯ç”±"""
        ai_response = await unified_ai_service_simplified.rewrite_query(original_query=original_query, db=db)
        tokens = ai_response.token_usage.total_tokens if ai_response.token_usage else 0
        
        if ai_response.success and ai_response.output_data:
            # å˜—è©¦è§£ææ–°çš„ AIQueryRewriteOutput æ ¼å¼
            if isinstance(ai_response.output_data, AIQueryRewriteOutput):
                output = ai_response.output_data
                logger.info(f"ğŸ§  AIæ„åœ–åˆ†æï¼š{output.reasoning}")
                logger.info(f"ğŸ“Š å•é¡Œç²’åº¦ï¼š{output.query_granularity}")
                logger.info(f"ğŸ¯ å»ºè­°ç­–ç•¥ï¼š{output.search_strategy_suggestion}")
                logger.info(f"ğŸ“ é‡å¯«æŸ¥è©¢æ•¸é‡ï¼š{len(output.rewritten_queries)}")
                
                # å‰µå»ºæ“´å±•çš„ QueryRewriteResultï¼ŒåŒ…å«æ–°çš„ç­–ç•¥ä¿¡æ¯
                return QueryRewriteResult(
                    original_query=original_query, 
                    rewritten_queries=output.rewritten_queries, 
                    extracted_parameters=output.extracted_parameters, 
                    intent_analysis=output.intent_analysis,
                    # æ·»åŠ æ–°çš„ç­–ç•¥ä¿¡æ¯åˆ° extracted_parameters
                    query_granularity=output.query_granularity,
                    search_strategy_suggestion=output.search_strategy_suggestion,
                    reasoning=output.reasoning
                ), tokens
            
            # å‘å¾Œå…¼å®¹ï¼šè™•ç†èˆŠçš„ AIQueryRewriteOutputLegacy æ ¼å¼
            elif hasattr(ai_response.output_data, 'rewritten_queries'):
                output = ai_response.output_data
                logger.warning("ä½¿ç”¨èˆŠç‰ˆæŸ¥è©¢é‡å¯«æ ¼å¼ï¼Œç¼ºå°‘æ™ºèƒ½ç­–ç•¥è·¯ç”±ä¿¡æ¯")
                return QueryRewriteResult(
                    original_query=original_query, 
                    rewritten_queries=output.rewritten_queries if hasattr(output, 'rewritten_queries') else [original_query], 
                    extracted_parameters=output.extracted_parameters if hasattr(output, 'extracted_parameters') else {}, 
                    intent_analysis=output.intent_analysis if hasattr(output, 'intent_analysis') else "Legacy format - no intent analysis"
                ), tokens
        
        # å¤±æ•—å›é€€
        logger.error("æŸ¥è©¢é‡å¯«å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹æŸ¥è©¢")
        return QueryRewriteResult(
            original_query=original_query, 
            rewritten_queries=[original_query], 
            extracted_parameters={}, 
            intent_analysis="Query rewrite failed."
        ), tokens

    async def _perform_traditional_single_stage_search(
        self,
        db: AsyncIOMotorDatabase,
        original_query: str,
        top_k: int,
        user_id: Optional[str],
        request_id: Optional[str],
        similarity_threshold: float,
        document_ids: Optional[List[str]] = None
    ) -> List[SemanticSearchResult]:
        """
        åŸ·è¡Œå‚³çµ±çš„å–®éšæ®µå‘é‡æœç´¢ï¼ŒåŒæ™‚æœç´¢æ–‡æª”æ‘˜è¦å’Œæ–‡æœ¬ç‰‡æ®µã€‚
        """
        logger.info(f"åŸ·è¡Œå‚³çµ±å–®éšæ®µæœç´¢ (æ‘˜è¦+æ–‡æœ¬ç‰‡æ®µ) for query: '{original_query[:100]}...'")

        # 1. ç”ŸæˆæŸ¥è©¢å‘é‡
        # ä¿®æ­£: ä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•åç¨± encode_text
        query_embedding = embedding_service.encode_text(original_query)
        if not query_embedding or not any(query_embedding): # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºæˆ–é›¶å‘é‡
            logger.error("ç„¡æ³•ç”ŸæˆæŸ¥è©¢çš„åµŒå…¥å‘é‡æˆ–ç”Ÿæˆäº†é›¶å‘é‡ã€‚")
            return []

        # 2. ä¸¦è¡ŒåŸ·è¡Œå°æ‘˜è¦å’Œæ–‡æœ¬ç‰‡æ®µçš„æœç´¢
        # ä¿®æ­£: ä½¿ç”¨å–®ä¸€ collection ä¸¦é€šé metadata filter å€åˆ† 'summary' å’Œ 'chunk'
        # ä¿®æ­£: ä½¿ç”¨ asyncio.to_thread ä¾†èª¿ç”¨åŒæ­¥çš„ aiohttp æ–¹æ³•

        # æº–å‚™æ‘˜è¦æœç´¢çš„éæ¿¾å™¨
        summary_metadata_filter = {"type": "summary"}
        if document_ids:
            summary_metadata_filter["document_id"] = {"$in": document_ids}

        summary_search_task = asyncio.to_thread(
            vector_db_service.search_similar_vectors,
            query_vector=query_embedding,
            top_k=top_k,
            owner_id_filter=user_id,
            metadata_filter=summary_metadata_filter,
            similarity_threshold=similarity_threshold
        )
        
        # æº–å‚™æ–‡æœ¬ç‰‡æ®µæœç´¢çš„éæ¿¾å™¨
        chunks_metadata_filter = {"type": "chunk"}
        if document_ids:
            chunks_metadata_filter["document_id"] = {"$in": document_ids}

        chunks_search_task = asyncio.to_thread(
            vector_db_service.search_similar_vectors,
            query_vector=query_embedding,
            top_k=top_k,
            owner_id_filter=user_id,
            metadata_filter=chunks_metadata_filter,
            similarity_threshold=similarity_threshold
        )
        
        summary_results, chunks_results = await asyncio.gather(summary_search_task, chunks_search_task)
        
        # 3. åˆä½µä¸¦å»é‡çµæœ
        # ä½¿ç”¨å­—å…¸ä¾†ç¢ºä¿æ¯å€‹ document_id åªä¿ç•™æœ€é«˜åˆ†
        all_results = {}
        
        search_results = summary_results + chunks_results
        
        for result in search_results:
            doc_id = result.document_id
            if doc_id not in all_results or result.similarity_score > all_results[doc_id].similarity_score:
                all_results[doc_id] = result
        
        # 4. æŒ‰åˆ†æ•¸é™åºæ’åº
        final_results = sorted(list(all_results.values()), key=lambda r: r.similarity_score, reverse=True)
        
        logger.info(f"å‚³çµ±å–®éšæ®µæœç´¢å®Œæˆï¼Œåˆä½µå¾Œå…±æ‰¾åˆ° {len(final_results)} å€‹ä¸é‡è¤‡çš„æ–‡æª”ã€‚")
        
        return final_results[:top_k]

    async def _semantic_search_summary_only(
        self,
        db: AsyncIOMotorDatabase,
        queries: List[str],
        top_k: int,
        user_id: Optional[str],
        request_id: Optional[str],
        similarity_threshold: float,
        document_ids: Optional[List[str]] = None
    ) -> List[SemanticSearchResult]:
        """æ‘˜è¦å°ˆç”¨æœç´¢ - é©åˆä¸»é¡Œç´šå•é¡Œ"""
        logger.info(f"ğŸ¯ åŸ·è¡Œæ‘˜è¦å°ˆç”¨æœç´¢ï¼ŒæŸ¥è©¢æ•¸é‡: {len(queries)}")
        
        all_results_map: Dict[str, SemanticSearchResult] = {}
        
        for i, query in enumerate(queries):
            logger.debug(f"åŸ·è¡Œç¬¬ {i+1}/{len(queries)} å€‹æŸ¥è©¢çš„æ‘˜è¦æœç´¢: {query[:50]}...")
            
            # ç”ŸæˆæŸ¥è©¢å‘é‡
            query_embedding = embedding_service.encode_text(query)
            if not query_embedding or not any(query_embedding):
                logger.warning(f"æŸ¥è©¢ '{query[:30]}...' ç„¡æ³•ç”ŸæˆåµŒå…¥å‘é‡ï¼Œè·³é")
                continue
            
            # æº–å‚™æ‘˜è¦æœç´¢çš„éæ¿¾å™¨
            summary_metadata_filter = {"type": "summary"}
            if document_ids:
                summary_metadata_filter["document_id"] = {"$in": document_ids}
            
            # åŸ·è¡Œæ‘˜è¦å‘é‡æœç´¢
            summary_results = await asyncio.to_thread(
                vector_db_service.search_similar_vectors,
                query_vector=query_embedding,
                top_k=top_k * 2,  # å¤šå–ä¸€äº›å€™é¸
                owner_id_filter=user_id,
                metadata_filter=summary_metadata_filter,
                similarity_threshold=similarity_threshold * 0.9  # ç¨å¾®é™ä½é–¾å€¼ä»¥ç²å¾—æ›´å¤šå€™é¸
            )
            
            # æ‡‰ç”¨æŸ¥è©¢æ¬Šé‡ï¼ˆä¸»é¡Œç´šæŸ¥è©¢ç¬¬ä¸€å€‹é€šå¸¸æœ€é‡è¦ï¼‰
            query_weight = 1.2 if i == 0 else 1.0
            
            for result in summary_results:
                weighted_score = result.similarity_score * query_weight
                
                if result.document_id not in all_results_map:
                    all_results_map[result.document_id] = SemanticSearchResult(
                        document_id=result.document_id,
                        similarity_score=weighted_score,
                        summary_text=result.summary_text,
                        metadata=result.metadata
                    )
                else:
                    # ä¿ç•™æœ€é«˜åˆ†
                    if weighted_score > all_results_map[result.document_id].similarity_score:
                        all_results_map[result.document_id].similarity_score = weighted_score
                        all_results_map[result.document_id].summary_text = result.summary_text
        
        # æ’åºä¸¦è¿”å›çµæœ
        final_results = list(all_results_map.values())
        final_results.sort(key=lambda x: x.similarity_score, reverse=True)
        
        logger.info(f"æ‘˜è¦å°ˆç”¨æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_results)} å€‹çµæœ")
        return final_results[:top_k]

    async def _perform_optimized_search_direct(
        self,
        db: AsyncIOMotorDatabase,
        queries: List[str],
        top_k: int,
        user_id: Optional[str],
        request_id: Optional[str],
        query_rewrite_result: Optional[QueryRewriteResult],
        similarity_threshold: float,
        enable_query_expansion: bool
    ) -> List[SemanticSearchResult]:
        """ç°¡åŒ–çš„æœç´¢åŸ·è¡Œé‚è¼¯ - ä½¿ç”¨çµ±ä¸€æœç´¢æ¥å£"""
        
        # ç²å– AI å»ºè­°çš„ç­–ç•¥
        strategy = self._extract_search_strategy(query_rewrite_result)
        
        logger.info(f"ğŸ¯ åŸ·è¡Œæœç´¢ç­–ç•¥: {strategy}")
        
        # çµ±ä¸€æœç´¢èª¿ç”¨
        return await self._unified_search(
            db=db,
            queries=queries,
            search_strategy=strategy,
            top_k=top_k,
            user_id=user_id,
            request_id=request_id,
            similarity_threshold=similarity_threshold
                )
    
    async def _semantic_search_with_hybrid_retrieval(
        self, 
        db: AsyncIOMotorDatabase, 
        queries: List[str], 
        top_k: int, 
        user_id: Optional[Any], 
        request_id: Optional[str], 
        query_rewrite_result: Optional[QueryRewriteResult], 
        similarity_threshold: float, 
        enable_query_expansion: bool
    ) -> List[SemanticSearchResult]:
        """ä½¿ç”¨å…©éšæ®µæ··åˆæª¢ç´¢é€²è¡Œèªç¾©æœç´¢ - å·²é‡æ§‹ä½¿ç”¨çµ±ä¸€é…ç½®"""
        
        # ç‚ºäº†æ—¥èªŒå’Œæœå‹™èª¿ç”¨ï¼Œç¢ºä¿ user_id æ˜¯å­—ä¸²
        user_id_str = str(user_id) if user_id else None

        try:
            # å°å…¥å…©éšæ®µæœç´¢æœå‹™
            from app.services.enhanced_search_service import enhanced_search_service
            
            all_results_map: Dict[str, SemanticSearchResult] = {}
            
            # ç‚ºæ¯å€‹é‡å¯«æŸ¥è©¢åŸ·è¡Œå…©éšæ®µæœç´¢
            for i, query in enumerate(queries):
                logger.debug(f"åŸ·è¡Œç¬¬ {i+1}/{len(queries)} å€‹æŸ¥è©¢çš„ RRF èåˆæœç´¢: {query[:50]}...")
                
                # åŸ·è¡Œ RRF èåˆæœç´¢
                stage_results = await enhanced_search_service.two_stage_hybrid_search(
                    db=db,
                    query=query,
                    user_id=user_id_str,  # å¼·åˆ¶ä½¿ç”¨å­—ä¸²
                    search_type="rrf_fusion",  # å¼·åˆ¶ä½¿ç”¨ RRF èåˆæœç´¢
                    stage1_top_k=min(top_k * 2, 15),  # RRFå…§éƒ¨æœƒä½¿ç”¨æ­¤åƒæ•¸ä½œç‚ºå€™é¸æ•¸
                    stage2_top_k=top_k,
                    similarity_threshold=similarity_threshold * 0.8  # ä¿æŒAIQAåŸæœ‰çš„é–¾å€¼ç­–ç•¥
                )
                
                # ä½¿ç”¨çµ±ä¸€çš„æ¬Šé‡é…ç½®åˆä½µçµæœ
                SearchWeightConfig.merge_weighted_results(all_results_map, stage_results, i)
                
                logger.debug(f"æŸ¥è©¢ {i+1} å®Œæˆï¼Œæœ¬æ¬¡æ‰¾åˆ° {len(stage_results)} å€‹çµæœ")
            
            # æœ€çµ‚çµæœæ’åºå’Œå¤šæ¨£æ€§å„ªåŒ–
            final_results = list(all_results_map.values())
            final_results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            # å¤šæ¨£æ€§å„ªåŒ–
            if len(final_results) > top_k:
                final_results = self._apply_diversity_optimization(final_results, top_k)
            
            result_list = final_results[:top_k]
            
            logger.info(f"å…©éšæ®µæ··åˆæª¢ç´¢å®Œæˆ: {len(queries)} å€‹æŸ¥è©¢ â†’ {len(result_list)} å€‹æœ€çµ‚çµæœ")
            
            await log_event(db=db, level=LogLevel.INFO,
                            message=f"å…©éšæ®µæ··åˆæª¢ç´¢å®Œæˆ: {len(result_list)} å€‹çµæœ",
                            source="service.enhanced_ai_qa.semantic_search_hybrid",
                            user_id=user_id_str, request_id=request_id,
                            details={
                                "query_count": len(queries),
                                "final_results": len(result_list),
                                "search_strategy": "two_stage_hybrid",
                                "diversity_optimization": True
                            })
            
            return result_list
            
        except Exception as e:
            logger.error(f"å…©éšæ®µæ··åˆæª¢ç´¢å¤±æ•—ï¼Œå›é€€åˆ°å‚³çµ±æœç´¢: {str(e)}", exc_info=True)
            await log_event(db=db, level=LogLevel.WARNING,
                            message=f"å…©éšæ®µæ··åˆæª¢ç´¢å¤±æ•—ï¼Œå›é€€åˆ°å‚³çµ±æœç´¢: {str(e)}",
                            source="service.enhanced_ai_qa.semantic_search_hybrid_fallback",
                            user_id=user_id_str, request_id=request_id,
                            details={"error": str(e)})
            
            # å›é€€åˆ°å‚³çµ±æœç´¢
            return await self._semantic_search_legacy(
                db, queries, top_k, user_id_str, request_id, query_rewrite_result, 
                similarity_threshold, enable_query_expansion
            )
    
    async def _semantic_search_legacy(
        self, 
        db: AsyncIOMotorDatabase, 
        queries: List[str], 
        top_k: int, 
        user_id: Optional[str], 
        request_id: Optional[str], 
        query_rewrite_result: Optional[QueryRewriteResult], 
        similarity_threshold: float, 
        enable_query_expansion: bool
    ) -> List[SemanticSearchResult]:
        """å‚³çµ±å–®éšæ®µèªç¾©æœç´¢ - ä¿æŒåŸæœ‰AIQAé‚è¼¯"""
        
        all_results_map: Dict[str, SemanticSearchResult] = {}
        
        # æ”¹é€²çš„å…ƒæ•¸æ“šéæ¿¾
        chroma_metadata_filter: Dict[str, Any] = {}
        if query_rewrite_result and query_rewrite_result.extracted_parameters:
            file_type = query_rewrite_result.extracted_parameters.get("file_type") or (query_rewrite_result.extracted_parameters.get("document_types", [])[0] if query_rewrite_result.extracted_parameters.get("document_types") else None)
            if file_type: 
                chroma_metadata_filter["file_type"] = file_type

        # ä½¿ç”¨çµ±ä¸€çš„å‘é‡ç²å–æ¥å£
        query_vectors = await self._get_query_vectors(queries)
        
        try:
            owner_id_filter_for_vector_db = user_id if user_id else None

            for i, q_item in enumerate(queries):
                if q_item not in query_vectors:
                    continue
                    
                query_vector = query_vectors[q_item]
                # æ ¹æ“šæŸ¥è©¢é¡å‹èª¿æ•´ top_kï¼Œç¢ºä¿å¤šæ¨£æ€§
                adjusted_top_k = min(top_k * 2, 20) if len(queries) > 1 else top_k
                
                # å˜—è©¦å¸¶éæ¿¾æ¢ä»¶çš„æœç´¢
                results = vector_db_service.search_similar_vectors(
                    query_vector=query_vector, 
                    top_k=adjusted_top_k, 
                    owner_id_filter=owner_id_filter_for_vector_db, 
                    metadata_filter=chroma_metadata_filter,
                    similarity_threshold=similarity_threshold * 0.8  # ç•¥å¾®é™ä½é–¾å€¼ä»¥ç²å¾—æ›´å¤šå€™é¸
                )
                
                # å¦‚æœå¸¶éæ¿¾æ¢ä»¶çš„æœç´¢æ²’æœ‰çµæœï¼Œä¸”æœ‰ metadata_filterï¼Œå‰‡å˜—è©¦ä¸å¸¶ metadata_filter çš„æœç´¢
                if not results and chroma_metadata_filter:
                    logger.warning(f"å¸¶ metadata_filter çš„æœç´¢æ²’æœ‰çµæœï¼Œå˜—è©¦å›é€€æœç´¢ã€‚Filter: {chroma_metadata_filter}")
                    results = vector_db_service.search_similar_vectors(
                        query_vector=query_vector, 
                        top_k=adjusted_top_k, 
                        owner_id_filter=owner_id_filter_for_vector_db, 
                        metadata_filter=None,  # å›é€€ï¼šç§»é™¤ metadata_filter
                        similarity_threshold=similarity_threshold * 0.8
                    )
                    if results:
                        logger.info(f"å›é€€æœç´¢æˆåŠŸæ‰¾åˆ° {len(results)} å€‹çµæœ")
                    
                # ä½¿ç”¨çµ±ä¸€çš„æ¬Šé‡é…ç½®åˆä½µçµæœ
                SearchWeightConfig.merge_weighted_results(all_results_map, results, i)

            # é‡æ’åºå’Œå¤šæ¨£æ€§å„ªåŒ–
            final_results = list(all_results_map.values())
            final_results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            # å¤šæ¨£æ€§å„ªåŒ–
            if len(final_results) > top_k:
                final_results = self._apply_diversity_optimization(final_results, top_k)
            
            # æœ€çµ‚éæ¿¾ï¼šç¢ºä¿åˆ†æ•¸ä¸ä½æ–¼èª¿æ•´å¾Œçš„é–¾å€¼
            final_threshold = similarity_threshold
            final_results = [r for r in final_results if r.similarity_score >= final_threshold]
            
            return final_results[:top_k]
            
        except Exception as e:
            logger.error(f"èªç¾©æœç´¢éç¨‹ä¸­ç™¼ç”Ÿç•°å¸¸: {e}", exc_info=True)
            return []

    async def _t2q_filter(self, db: AsyncIOMotorDatabase, document_ids: List[str], extracted_parameters: Dict[str, Any], user_id: Optional[str], request_id: Optional[str]) -> List[str]:
        # This function is no longer called in the main flow but is kept for potential future use.
        return document_ids

    async def _generate_answer_unified(self, db: AsyncIOMotorDatabase, original_query: str, documents_for_context: List[Any], query_rewrite_result: QueryRewriteResult, detailed_document_data: Optional[List[Dict[str, Any]]], ai_generated_query_reasoning: Optional[str], user_id: Optional[str], request_id: Optional[str], model_preference: Optional[str] = None, ensure_chinese_output: Optional[bool] = None, detailed_text_max_length: Optional[int] = None, max_chars_per_doc: Optional[int] = None, conversation_history: Optional[str] = None) -> Tuple[str, int, float, List[LLMContextDocument]]:
        """æ­¥é©Ÿ4: ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆï¼ˆä½¿ç”¨çµ±ä¸€AIæœå‹™ï¼‰- Implements focused context logic."""
        actual_contexts_for_llm: List[LLMContextDocument] = []
        context_parts = []
        
        log_details_context = {
            "num_docs_for_context_initial": len(documents_for_context),
            "original_query_length": len(original_query),
            "intent": query_rewrite_result.intent_analysis[:100] if query_rewrite_result.intent_analysis else None,
            "has_detailed_document_data": bool(detailed_document_data)
        }
        await log_event(db=db, level=LogLevel.DEBUG, message="Assembling context for AI answer generation.",
                        source="service.enhanced_ai_qa.generate_answer_internal", user_id=user_id, request_id=request_id, details=log_details_context)

        try:
            # === èšç„¦ä¸Šä¸‹æ–‡é‚è¼¯ï¼šå„ªå…ˆä½¿ç”¨è©³ç´°è³‡æ–™ï¼Œæå‡æº–ç¢ºæ€§ä¸¦é™ä½ Token æ¶ˆè€— ===
            if detailed_document_data and len(detailed_document_data) > 0:
                logger.info(f"èšç„¦ä¸Šä¸‹æ–‡è·¯å¾‘ï¼šä½¿ç”¨ä¾†è‡ª {len(detailed_document_data)} å€‹ AI é¸ä¸­æ–‡ä»¶çš„è©³ç´°è³‡æ–™")
                
                for i, detail_item in enumerate(detailed_document_data):
                    doc_id_for_detail = str(detail_item.get("_id", f"unknown_detailed_doc_{i}"))
                    detailed_data_str = json.dumps(detail_item, ensure_ascii=False, indent=2)

                    context_preamble = f"æ™ºæ…§æŸ¥è©¢æ–‡ä»¶ {doc_id_for_detail} çš„è©³ç´°è³‡æ–™ï¼š\n"
                    if i == 0 and ai_generated_query_reasoning: # åœ¨ç¬¬ä¸€å€‹æ–‡ä»¶é¡¯ç¤ºæŸ¥è©¢æ¨ç†
                        context_preamble += f"AI æŸ¥è©¢æ¨ç†ï¼š{ai_generated_query_reasoning}\n\n"
                    
                    context_preamble += f"æŸ¥è©¢åˆ°çš„ç²¾æº–è³‡æ–™ï¼š\n{detailed_data_str}\n\n"
                    context_parts.append(context_preamble)
                    actual_contexts_for_llm.append(LLMContextDocument(
                        document_id=doc_id_for_detail, 
                        content_used=detailed_data_str[:300], 
                        source_type="ai_detailed_query"
                    ))
                
                logger.info(f"ä½¿ç”¨èšç„¦ä¸Šä¸‹æ–‡ï¼š{len(context_parts)} å€‹è©³ç´°æŸ¥è©¢çµæœï¼Œç¸½é•·åº¦ç´„ {sum(len(part) for part in context_parts)} å­—ç¬¦")
            
            # === å‚™ç”¨é€šç”¨ä¸Šä¸‹æ–‡é‚è¼¯ï¼šç•¶æ²’æœ‰è©³ç´°è³‡æ–™æ™‚ä½¿ç”¨ ===
            else:
                logger.info("é€šç”¨ä¸Šä¸‹æ–‡è·¯å¾‘ï¼šæ²’æœ‰è©³ç´°è³‡æ–™å¯ç”¨ï¼Œä½¿ç”¨ä¾†è‡ªå‘é‡æœç´¢çš„é€šç”¨æ–‡ä»¶æ‘˜è¦")
                max_general_docs = 5

                for i, doc in enumerate(documents_for_context[:max_general_docs], 1):
                    doc_content_to_use = ""
                    content_source_type = "unknown_general"
                    doc_id_str = str(doc.id) if hasattr(doc, 'id') else f"unknown_general_doc_{i}"
                    
                    # å˜—è©¦ç²å– AI åˆ†æçš„æ‘˜è¦
                    raw_extracted_text = getattr(doc, 'extracted_text', None)
                    ai_summary = None
                    if hasattr(doc, 'analysis') and doc.analysis and hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                        try:
                            analysis_output = AIDocumentAnalysisOutputDetail(**doc.analysis.ai_analysis_output)
                            if analysis_output.key_information and analysis_output.key_information.content_summary:
                                ai_summary = analysis_output.key_information.content_summary
                            elif analysis_output.initial_summary:
                                ai_summary = analysis_output.initial_summary
                        except (ValidationError, Exception):
                            pass
                    
                    # é¸æ“‡æœ€ä½³çš„å…§å®¹ä¾†æº
                    if ai_summary:
                        doc_content_to_use, content_source_type = ai_summary, "general_ai_summary"
                    elif raw_extracted_text and isinstance(raw_extracted_text, str) and raw_extracted_text.strip():
                        # æˆªæ–·éé•·çš„åŸå§‹æ–‡æœ¬
                        truncated_text = raw_extracted_text[:1000] + ("..." if len(raw_extracted_text) > 1000 else "")
                        doc_content_to_use, content_source_type = truncated_text, "general_extracted_text"
                    else:
                        doc_content_to_use, content_source_type = f"æ–‡ä»¶ '{getattr(doc, 'filename', 'N/A')}' æ²’æœ‰å¯ç”¨çš„æ–‡å­—å…§å®¹ã€‚", "general_placeholder"
                    
                    actual_contexts_for_llm.append(LLMContextDocument(
                        document_id=doc_id_str, 
                        content_used=doc_content_to_use[:300], 
                        source_type=content_source_type
                    ))
                    context_parts.append(f"é€šç”¨ä¸Šä¸‹æ–‡æ–‡ä»¶ {i} (ID: {doc_id_str}, ä¾†æº: {content_source_type}):\n{doc_content_to_use}")

                logger.info(f"ä½¿ç”¨é€šç”¨ä¸Šä¸‹æ–‡ï¼š{len(context_parts)} å€‹æ–‡ä»¶æ‘˜è¦ï¼Œç¸½é•·åº¦ç´„ {sum(len(part) for part in context_parts)} å­—ç¬¦")

            query_for_answer_gen = query_rewrite_result.rewritten_queries[0] if query_rewrite_result.rewritten_queries else original_query
            
            # å¦‚æœæœ‰å°è©±æ­·å²ï¼Œå°‡å…¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡é–‹é ­
            if conversation_history:
                context_parts.insert(0, conversation_history)
                logger.info("å·²å°‡å°è©±æ­·å²æ·»åŠ åˆ° AI ä¸Šä¸‹æ–‡ä¸­")

            log_details_ai_call = {"query_for_answer_gen_length": len(query_for_answer_gen), "num_docs_in_final_context": len(actual_contexts_for_llm), "total_context_length": len("\n\n".join(context_parts)), "model_preference": model_preference, "has_conversation_history": bool(conversation_history)}
            await log_event(db=db, level=LogLevel.DEBUG, message="Calling AI for answer generation.", source="service.enhanced_ai_qa.generate_answer_internal", user_id=user_id, request_id=request_id, details=log_details_ai_call)
            
            ai_response: UnifiedAIResponse = await unified_ai_service_simplified.generate_answer(
                user_question=query_for_answer_gen,
                intent_analysis=query_rewrite_result.intent_analysis or "",
                document_context=context_parts,
                db=db,
                model_preference=model_preference,
                ai_ensure_chinese_output=ensure_chinese_output,
                detailed_text_max_length=detailed_text_max_length,
                max_chars_per_doc=max_chars_per_doc
            )
            
            tokens_used = ai_response.token_usage.total_tokens if ai_response.token_usage else 0
            answer_text = "Error: AI service did not return a successful response or content."
            confidence = 0.1

            if ai_response.success and ai_response.output_data:
                if isinstance(ai_response.output_data, AIGeneratedAnswerOutput):
                    answer_text = ai_response.output_data.answer_text
                else:
                    answer_text = f"Error: AI returned unexpected answer format: {type(ai_response.output_data).__name__}."
                
                confidence = min(0.9, 0.3 + (len(actual_contexts_for_llm) * 0.1) + (0.1 if not answer_text.lower().startswith("error") else -0.2))
                await log_event(db=db, level=LogLevel.INFO, message="AI answer generation successful.", source="service.enhanced_ai_qa.generate_answer_internal", user_id=user_id, request_id=request_id, details={"model_used": ai_response.model_used, "response_length": len(answer_text), "tokens": tokens_used, "confidence": confidence})
            else:
                error_msg = ai_response.error_message or "AI failed to generate answer."
                await log_event(db=db, level=LogLevel.ERROR, message=f"AI answer generation failed: {error_msg}", source="service.enhanced_ai_qa.generate_answer_internal", user_id=user_id, request_id=request_id, details={**log_details_ai_call, "error": error_msg})
                answer_text = f"Sorry, I couldn't generate an answer: {error_msg}"

            return answer_text, tokens_used, confidence, actual_contexts_for_llm
            
        except Exception as e:
            await log_event(db=db, level=LogLevel.ERROR, message=f"Unexpected error in _generate_answer_unified: {str(e)}", source="service.enhanced_ai_qa.generate_answer_internal", user_id=user_id, request_id=request_id, details={"error_type": type(e).__name__})
            return f"An internal error occurred while generating the answer: {str(e)}", 0, 0.0, actual_contexts_for_llm

    async def _optimize_field_selection(
        self,
        db: AsyncIOMotorDatabase,
        user_question: str,
        document_analysis_summary: str,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        åŸºæ–¼ç”¨æˆ¶å•é¡Œæ™ºæ…§é¸æ“‡éœ€è¦çš„æ–‡æª”æ¬„ä½ï¼Œé¿å…æŸ¥è©¢éå¤šä¸å¿…è¦çš„è³‡æ–™
        
        Args:
            user_question: ç”¨æˆ¶å•é¡Œ
            document_analysis_summary: æ–‡æª”åˆ†ææ‘˜è¦
            
        Returns:
            å„ªåŒ–å¾Œçš„ projection é…ç½®
        """
        try:
            # åŸºæ–¼å•é¡Œé¡å‹çš„æ™ºæ…§æ¬„ä½æ˜ å°„
            field_mapping = {
                "summary": ["analysis.ai_analysis_output.key_information.content_summary", "analysis.summary"],
                "date": ["analysis.ai_analysis_output.key_information.dates_mentioned", "created_at", "updated_at"],
                "topic": ["analysis.ai_analysis_output.key_information.main_topics", "analysis.ai_analysis_output.key_information.semantic_tags"],
                "concept": ["analysis.ai_analysis_output.key_information.key_concepts", "analysis.key_terms"],
                "action": ["analysis.ai_analysis_output.key_information.action_items"],
                "content": ["extracted_text"],
                "metadata": ["filename", "file_type", "content_type_human_readable", "metadata"],
                "dynamic": ["analysis.ai_analysis_output.key_information.dynamic_fields"]
            }
            
            # åˆ†æå•é¡Œæ„åœ–
            question_lower = user_question.lower()
            selected_fields = set(["_id", "filename"])  # åŸºæœ¬å¿…è¦æ¬„ä½
            
            # åŸºæ–¼é—œéµè©æ™ºæ…§é¸æ“‡æ¬„ä½
            if any(keyword in question_lower for keyword in ["ç¸½çµ", "æ‘˜è¦", "æ¦‚è¦", "summary"]):
                selected_fields.update(field_mapping["summary"])
            
            if any(keyword in question_lower for keyword in ["æ—¥æœŸ", "æ™‚é–“", "when", "date"]):
                selected_fields.update(field_mapping["date"])
                
            if any(keyword in question_lower for keyword in ["ä¸»é¡Œ", "è©±é¡Œ", "topic", "about"]):
                selected_fields.update(field_mapping["topic"])
                
            if any(keyword in question_lower for keyword in ["æ¦‚å¿µ", "concept", "key"]):
                selected_fields.update(field_mapping["concept"])
                
            if any(keyword in question_lower for keyword in ["å¾…è¾¦", "ä»»å‹™", "action", "todo"]):
                selected_fields.update(field_mapping["action"])
                
            if any(keyword in question_lower for keyword in ["å…§å®¹", "æ–‡å­—", "content", "text", "å…·é«”", "è©³ç´°", "ç´°ç¯€", "æŠ€è¡“", "å¯¦ç¾", "æ–¹å¼", "æ©Ÿåˆ¶", "æµç¨‹", "æ­¥é©Ÿ"]):
                selected_fields.update(field_mapping["content"])
                
            if any(keyword in question_lower for keyword in ["æª”å", "é¡å‹", "metadata", "file"]):
                selected_fields.update(field_mapping["metadata"])
            
            # å¦‚æœå•é¡ŒåŒ…å«ç‰¹å®šå¯¦é«”ï¼ˆå¦‚äººåã€å…¬å¸åç­‰ï¼‰ï¼ŒåŒ…å«å‹•æ…‹æ¬„ä½
            if any(char.isupper() for char in user_question) or "èª°" in question_lower or "who" in question_lower:
                selected_fields.update(field_mapping["dynamic"])
            
            # å¦‚æœæ²’æœ‰æ˜ç¢ºåŒ¹é…ï¼Œä½¿ç”¨å¹³è¡¡ç­–ç•¥
            if len(selected_fields) <= 2:
                selected_fields.update(field_mapping["summary"])
                selected_fields.update(field_mapping["topic"])
                selected_fields.update(field_mapping["dynamic"])
                # å°æ–¼è¤‡é›œå•é¡Œï¼Œä¹ŸåŒ…å«éƒ¨åˆ†åŸå§‹å…§å®¹
                if any(complex_keyword in question_lower for complex_keyword in ["å¦‚ä½•", "ç‚ºä»€éº¼", "æ©Ÿåˆ¶", "åŸç†", "æµç¨‹", "æ¶æ§‹"]):
                    selected_fields.update(field_mapping["content"])
            
            # å»ºæ§‹ MongoDB projection
            projection = {field: 1 for field in selected_fields}
            
            await log_event(db=db, level=LogLevel.DEBUG,
                            message=f"æ™ºæ…§æ¬„ä½é¸æ“‡å®Œæˆï¼Œé¸æ“‡äº† {len(selected_fields)} å€‹æ¬„ä½",
                            source="service.enhanced_ai_qa.field_optimization",
                            user_id=user_id,
                            details={
                                "selected_fields": list(selected_fields),
                                "question_keywords": [kw for kw in ["ç¸½çµ", "æ—¥æœŸ", "ä¸»é¡Œ", "æ¦‚å¿µ", "å¾…è¾¦", "å…§å®¹", "æª”å"] if kw in question_lower],
                                "estimated_data_reduction": f"{max(0, 100 - len(selected_fields) * 10)}%"
                            })
            
            return projection
            
        except Exception as e:
            await log_event(db=db, level=LogLevel.ERROR,
                            message=f"æ™ºæ…§æ¬„ä½é¸æ“‡å¤±æ•—ï¼Œä½¿ç”¨é è¨­é…ç½®: {str(e)}",
                            source="service.enhanced_ai_qa.field_optimization_error",
                            user_id=user_id,
                            details={"error": str(e)})
            
            # å›é€€åˆ°åŸºæœ¬æ¬„ä½é¸æ“‡
            return {
                "_id": 1,
                "filename": 1,
                "analysis.ai_analysis_output.key_information": 1,
                "analysis.summary": 1
            }

    async def _batch_detailed_query(
        self,
        db: AsyncIOMotorDatabase,
        user_question: str,
        selected_documents: List[Any],
        user_id: Optional[str] = None,
        request_id: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹æ¬¡è™•ç†å¤šå€‹æ–‡æª”çš„è©³ç´°æŸ¥è©¢ï¼Œæ¸›å°‘AIèª¿ç”¨æ¬¡æ•¸
        
        Args:
            user_question: ç”¨æˆ¶å•é¡Œ
            selected_documents: å·²é¸æ“‡çš„æ–‡æª”åˆ—è¡¨
            
        Returns:
            æ‰¹æ¬¡æŸ¥è©¢çµæœåˆ—è¡¨
        """
        try:
            if not selected_documents:
                return []
            
            # ç²å– Schema ç·©å­˜ï¼ˆç¾åœ¨é€šéç·©å­˜ç®¡ç†å™¨è™•ç†ï¼‰
            # é€™è£¡æº–å‚™ schema è³‡è¨Šä»¥ä¾›å¾ŒçºŒä½¿ç”¨
            document_schema_info = {
                "description": "MongoDB æ–‡æª” Schema è³‡è¨Š",
                "fields": {
                    "analysis.ai_analysis_output.key_information": "çµæ§‹åŒ–è³‡è¨Š",
                    "extracted_text": "æ–‡æœ¬å…§å®¹",
                    "filename": "æª”æ¡ˆåç¨±"
                }
            }
            schema_cache_name = await self.cache_manager.get_or_create_schema_cache(db, document_schema_info, user_id)
            
            all_detailed_data = []
            batch_size = 3  # æ¯æ‰¹æ¬¡è™•ç†çš„æ–‡æª”æ•¸é‡
            
            for i in range(0, len(selected_documents), batch_size):
                batch_docs = selected_documents[i:i + batch_size]
                
                # ç‚ºç•¶å‰æ‰¹æ¬¡æ§‹å»ºä¸Šä¸‹æ–‡
                batch_context = f"ç”¨æˆ¶å•é¡Œï¼š{user_question}\n\n"
                batch_context += "éœ€è¦æŸ¥è©¢çš„æ–‡æª”åˆ—è¡¨ï¼š\n"
                
                for j, doc in enumerate(batch_docs, 1):
                    doc_summary = "ç„¡å¯ç”¨æ‘˜è¦"
                    if hasattr(doc, 'analysis') and doc.analysis:
                        if hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                            try:
                                analysis_output = doc.analysis.ai_analysis_output
                                if 'key_information' in analysis_output and 'content_summary' in analysis_output['key_information']:
                                    doc_summary = analysis_output['key_information']['content_summary']
                            except Exception:
                                pass
                    
                    batch_context += f"{j}. æ–‡æª”ID: {doc.id}\n"
                    batch_context += f"   æª”å: {getattr(doc, 'filename', 'Unknown')}\n"
                    batch_context += f"   æ‘˜è¦: {doc_summary}\n\n"
                
                # ä½¿ç”¨æ™ºæ…§æ¬„ä½é¸æ“‡
                optimized_projection = await self._optimize_field_selection(
                    db, user_question, batch_context, user_id
                )
                
                await log_event(db=db, level=LogLevel.INFO,
                                message=f"é–‹å§‹æ‰¹æ¬¡è™•ç† {len(batch_docs)} å€‹æ–‡æª”ï¼ˆæ‰¹æ¬¡ {i//batch_size + 1}ï¼‰",
                                source="service.enhanced_ai_qa.batch_detailed_query",
                                user_id=user_id, request_id=request_id,
                                details={
                                    "batch_size": len(batch_docs),
                                    "batch_number": i//batch_size + 1,
                                    "total_batches": (len(selected_documents) + batch_size - 1) // batch_size,
                                    "optimized_fields_count": len(optimized_projection)
                                })
                
                # å°æ‰¹æ¬¡ä¸­çš„æ¯å€‹æ–‡æª”åŸ·è¡ŒæŸ¥è©¢
                for doc in batch_docs:
                    try:
                        # ä½¿ç”¨å„ªåŒ–çš„ projection æŸ¥è©¢æ–‡æª”
                        mongo_filter = {"_id": doc.id}
                        safe_projection = remove_projection_path_collisions(optimized_projection)
                        fetched_data = await db.documents.find_one(mongo_filter, projection=safe_projection)
                        
                        if fetched_data:
                            # è³‡æ–™æ¸…ç†
                            def sanitize(data: Any) -> Any:
                                if isinstance(data, dict): 
                                    return {k: sanitize(v) for k, v in data.items()}
                                if isinstance(data, list): 
                                    return [sanitize(i) for i in data]
                                if isinstance(data, uuid.UUID): 
                                    return str(data)
                                return data
                            
                            all_detailed_data.append(sanitize(fetched_data))
                            logger.info(f"æ‰¹æ¬¡æŸ¥è©¢æˆåŠŸç²å–æ–‡æª” {doc.id} çš„è³‡æ–™")
                        else:
                            logger.warning(f"æ‰¹æ¬¡æŸ¥è©¢ä¸­æ–‡æª” {doc.id} æ²’æœ‰è¿”å›è³‡æ–™")
                            
                    except Exception as doc_error:
                        logger.error(f"æ‰¹æ¬¡æŸ¥è©¢æ–‡æª” {doc.id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(doc_error)}")
                        continue
                
                # æ‰¹æ¬¡é–“çš„å°å»¶é²ï¼Œé¿å…éåº¦è² è¼‰
                if i + batch_size < len(selected_documents):
                    await asyncio.sleep(0.1)
            
            await log_event(db=db, level=LogLevel.INFO,
                            message=f"æ‰¹æ¬¡è©³ç´°æŸ¥è©¢å®Œæˆï¼ŒæˆåŠŸè™•ç† {len(all_detailed_data)}/{len(selected_documents)} å€‹æ–‡æª”",
                            source="service.enhanced_ai_qa.batch_detailed_query_complete",
                            user_id=user_id, request_id=request_id,
                            details={
                                "total_requested": len(selected_documents),
                                "successful_queries": len(all_detailed_data),
                                "success_rate": f"{len(all_detailed_data)/len(selected_documents)*100:.1f}%" if selected_documents else "0%"
                            })
            
            return all_detailed_data
            
        except Exception as e:
            await log_event(db=db, level=LogLevel.ERROR,
                            message=f"æ‰¹æ¬¡è©³ç´°æŸ¥è©¢å¤±æ•—: {str(e)}",
                            source="service.enhanced_ai_qa.batch_detailed_query_error",
                            user_id=user_id, request_id=request_id,
                            details={"error": str(e), "document_count": len(selected_documents) if selected_documents else 0})
            return []

enhanced_ai_qa_service = EnhancedAIQAService()