"""
æµå¼å•ç­” API ç«¯é»

åªåœ¨ç­”æ¡ˆç”Ÿæˆéšæ®µä½¿ç”¨æµå¼è¼¸å‡ºï¼Œå‰é¢çš„åˆ†é¡ã€æœç´¢ç­‰æ­¥é©Ÿå¯¦æ™‚ç™¼é€é€²åº¦
"""
import logging
import json
import asyncio
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from motor.motor_asyncio import AsyncIOMotorDatabase
from typing import Optional, AsyncGenerator

from app.dependencies import get_db
from app.models.user_models import User
from app.core.security import get_current_active_user
from app.models.vector_models import AIQARequest
from app.core.logging_utils import AppLogger, log_event, LogLevel

router = APIRouter()
logger = AppLogger(__name__, level=logging.DEBUG).get_logger()


async def generate_streaming_answer(
    db: AsyncIOMotorDatabase,
    request: AIQARequest,
    user_id: str
) -> AsyncGenerator[str, None]:
    """
    æµå¼ç”Ÿæˆç­”æ¡ˆçš„æ ¸å¿ƒé‚è¼¯ - å¯¦æ™‚ç™¼é€æ¯å€‹è™•ç†æ­¥é©Ÿçš„é€²åº¦
    
    å·¥ä½œæµç¨‹ï¼š
    1. æ™ºèƒ½åˆ†é¡ï¼ˆå¯¦æ™‚é€²åº¦ï¼‰
    2. æ–‡æª”æœç´¢ï¼ˆå¯¦æ™‚é€²åº¦ï¼ŒåŒ…å«æŸ¥è©¢é‡å¯«ã€å‘é‡æœç´¢ç­‰ï¼‰
    3. å·¥ä½œæµæ‰¹å‡†ï¼ˆå¦‚éœ€è¦ï¼‰
    4. ç­”æ¡ˆç”Ÿæˆï¼ˆæµå¼è¼¸å‡ºï¼‰â­
    """
    try:
        # === ç™¼é€é–‹å§‹ä¿¡è™Ÿ ===
        yield f"data: {json.dumps({'type': 'progress', 'stage': 'start', 'message': 'ğŸš€ é–‹å§‹è™•ç†æ‚¨çš„å•é¡Œ...'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.05)
        
        # === æ­¥é©Ÿ 1: è¼‰å…¥å°è©±ä¸Šä¸‹æ–‡ ===
        from app.services.qa_workflow.unified_context_helper import unified_context_helper
        from app.services.qa_workflow.question_classifier_service import question_classifier_service
        from app.models.question_models import QuestionIntent
        
        logger.info(f"ğŸš€ [Stream QA] é–‹å§‹è™•ç†å•é¡Œ: {request.question[:50]}...")
        
        conversation_context = None
        cached_documents_info_for_classifier = None
        
        # è¼‰å…¥å°è©±æ­·å²
        if request.conversation_id:
            conversation_context = await unified_context_helper.load_conversation_history_list(
                db=db,
                conversation_id=request.conversation_id,
                user_id=str(user_id) if user_id else None,
                limit=10
            )
            
            if conversation_context:
                logger.info(f"è¼‰å…¥äº† {len(conversation_context)} æ¢æ­·å²æ¶ˆæ¯")
            
            # ç²å–ç·©å­˜æ–‡æª”ä¿¡æ¯
            try:
                from app.crud import crud_conversations
                from uuid import UUID
                
                conversation_uuid = UUID(request.conversation_id)
                user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
                
                cached_doc_ids, _ = await crud_conversations.get_cached_documents(
                    db=db,
                    conversation_id=conversation_uuid,
                    user_id=user_uuid
                )
                
                if cached_doc_ids:
                    from app.crud.crud_documents import get_documents_by_ids
                    documents = await get_documents_by_ids(db, cached_doc_ids)
                    
                    cached_documents_info_for_classifier = []
                    for idx, doc in enumerate(documents, 1):
                        doc_info = {
                            "document_id": str(doc.id),
                            "filename": doc.filename,
                            "reference_number": idx,
                            "summary": ""
                        }
                        
                        try:
                            enriched_data = getattr(doc, 'enriched_data', None)
                            if enriched_data and isinstance(enriched_data, dict):
                                doc_info["summary"] = enriched_data.get('summary', '')
                            
                            if not doc_info["summary"] and hasattr(doc, 'analysis') and doc.analysis:
                                if hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                                    key_info = doc.analysis.ai_analysis_output.get('key_information', {})
                                    if isinstance(key_info, dict):
                                        doc_info["summary"] = key_info.get('content_summary', '')
                        except Exception as e:
                            logger.warning(f"ç²å–æ–‡æª” {idx} æ‘˜è¦å¤±æ•—: {e}")
                        
                        cached_documents_info_for_classifier.append(doc_info)
                    
                    logger.info(f"æº–å‚™äº† {len(cached_documents_info_for_classifier)} å€‹ç·©å­˜æ–‡æª”ä¿¡æ¯ç”¨æ–¼åˆ†é¡")
            except Exception as e:
                logger.warning(f"ç²å–ç·©å­˜æ–‡æª”ä¿¡æ¯å¤±æ•—: {e}")
        
        # === æ­¥é©Ÿ 1.5: è™•ç†æ¾„æ¸…å›ç­” ===
        effective_question = request.question
        if request.workflow_action == 'provide_clarification' and request.clarification_text:
            logger.info(f"ğŸ“ æ”¶åˆ°æ¾„æ¸…å›ç­”: {request.clarification_text}")
            
            # å…ˆä¿å­˜ç”¨æˆ¶çš„æ¾„æ¸…å›ç­”åˆ°å°è©±æ­·å²
            if request.conversation_id:
                from app.crud import crud_conversations
                from uuid import UUID
                try:
                    conversation_uuid = UUID(request.conversation_id)
                    user_uuid = UUID(str(user_id)) if not isinstance(user_id, UUID) else user_id
                    
                    # ç›´æ¥æ·»åŠ ç”¨æˆ¶çš„æ¾„æ¸…å›ç­”æ¶ˆæ¯
                    await crud_conversations.add_message_to_conversation(
                        db=db,
                        conversation_id=conversation_uuid,
                        user_id=user_uuid,
                        role="user",
                        content=request.clarification_text,
                        tokens_used=None
                    )
                    logger.info(f"âœ… å·²ä¿å­˜æ¾„æ¸…å›ç­”åˆ°å°è©±æ­·å²: {request.clarification_text}")
                    
                    # ä½¿ç·©å­˜å¤±æ•ˆä¸¦é‡æ–°è¼‰å…¥å°è©±æ­·å²ï¼ˆåŒ…å«å‰›ä¿å­˜çš„æ¾„æ¸…å›ç­”ï¼‰
                    from app.services.cache.conversation_cache_service import conversation_cache_service
                    await conversation_cache_service.invalidate_conversation(
                        user_id=user_uuid,
                        conversation_id=conversation_uuid
                    )
                    
                    conversation_context = await unified_context_helper.load_conversation_history_list(
                        db=db,
                        conversation_id=request.conversation_id,
                        user_id=str(user_id) if user_id else None,
                        limit=10
                    )
                    logger.info(f"ğŸ”„ é‡æ–°è¼‰å…¥å°è©±æ­·å²ï¼Œç¾åœ¨æœ‰ {len(conversation_context) if conversation_context else 0} æ¢æ¶ˆæ¯")
                except Exception as e:
                    logger.error(f"âŒ ä¿å­˜æ¾„æ¸…å›ç­”å¤±æ•—: {e}")
            
            # å°‡æ¾„æ¸…å›ç­”çµ„åˆåˆ°å•é¡Œä¸­ï¼Œç”¨æ–¼å¾ŒçºŒè™•ç†
            # æ ¼å¼ï¼šã€ŒåŸå§‹å•é¡Œ â†’ æ¾„æ¸…å›ç­”ã€
            effective_question = f"{request.question} â†’ {request.clarification_text}"
            logger.info(f"ğŸ”€ çµ„åˆå¾Œçš„æœ‰æ•ˆå•é¡Œ: {effective_question}")
        
        # === æ­¥é©Ÿ 2: å•é¡Œåˆ†é¡ï¼ˆå¯¦æ™‚é€²åº¦ï¼‰===
        yield f"data: {json.dumps({'type': 'progress', 'stage': 'classifying', 'message': 'ğŸ¯ AI æ­£åœ¨åˆ†æå•é¡Œæ„åœ–...'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.05)
        
        classification = await question_classifier_service.classify_question(
            question=effective_question,  # ä½¿ç”¨çµ„åˆå¾Œçš„å•é¡Œï¼ˆå¦‚æœæœ‰æ¾„æ¸…å›ç­”ï¼‰
            conversation_history=conversation_context,
            has_cached_documents=bool(request.conversation_id),
            cached_documents_info=cached_documents_info_for_classifier,
            db=db,
            user_id=str(user_id) if user_id else None
        )
        
        # ç™¼é€åˆ†é¡çµæœ
        intent_label = {
            'greeting': 'å¯’æš„',
            'chitchat': 'é–’èŠ',
            'document_search': 'æ–‡æª”æœç´¢',
            'simple_factual': 'ç°¡å–®æŸ¥è©¢',
            'complex_analysis': 'è¤‡é›œåˆ†æ',
            'clarification_needed': 'éœ€è¦æ¾„æ¸…',
            'document_detail_query': 'MongoDB è©³ç´°æŸ¥è©¢'
        }.get(classification.intent, classification.intent)
        
        yield f"data: {json.dumps({'type': 'progress', 'stage': 'classified', 'message': f'âœ… å•é¡Œåˆ†é¡ï¼š{intent_label}ï¼ˆç½®ä¿¡åº¦ {classification.confidence:.0%}ï¼‰'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.1)
        
        # é¡¯ç¤º AI æ¨ç†å…§å®¹
        if hasattr(classification, 'reasoning') and classification.reasoning:
            yield f"data: {json.dumps({'type': 'progress', 'stage': 'reasoning', 'message': f'ğŸ’­ AI æ¨ç†', 'detail': classification.reasoning}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
        
        # === æ­¥é©Ÿ 3: æ ¹æ“šæ„åœ–è·¯ç”±è™•ç†ï¼ˆå¯¦æ™‚é€²åº¦ï¼‰===
        from app.services.intent_handlers.greeting_handler import greeting_handler
        from app.services.intent_handlers.clarification_handler import clarification_handler
        from app.services.intent_handlers.simple_factual_handler import simple_factual_handler
        from app.services.intent_handlers.document_search_handler import document_search_handler
        from app.services.intent_handlers.document_detail_query_handler import document_detail_query_handler
        from app.services.intent_handlers.complex_analysis_handler import complex_analysis_handler
        from app.services.enhanced_ai_qa_service import enhanced_ai_qa_service
        
        # ç°¡å–®æ„åœ–ç›´æ¥è™•ç†
        if classification.intent in [QuestionIntent.GREETING, QuestionIntent.CHITCHAT]:
            logger.info("â†’ è™•ç†å¯’æš„/é–’èŠ")
            response = await greeting_handler.handle(
                request, classification, db, user_id, None
            )
            yield f"data: {json.dumps({'type': 'complete', 'answer': response.answer}, ensure_ascii=False)}\n\n"
            return
        
        elif classification.intent == QuestionIntent.CLARIFICATION_NEEDED:
            logger.info("â†’ éœ€è¦æ¾„æ¸…")
            response = await clarification_handler.handle(
                request, classification, db, user_id, None
            )
            yield f"data: {json.dumps({'type': 'approval_needed', 'workflow_state': response.workflow_state}, ensure_ascii=False)}\n\n"
            return
        
        # === æ­¥é©Ÿ 4: MongoDB è©³ç´°æŸ¥è©¢ï¼ˆç›´æ¥ä½¿ç”¨ç·©å­˜æ–‡æª”ï¼‰ ===
        elif classification.intent == QuestionIntent.DOCUMENT_DETAIL_QUERY:
            logger.info("â†’ è™•ç† MongoDB è©³ç´°æŸ¥è©¢ï¼ˆæµå¼è¼¸å‡ºï¼‰")
            
            # è¼‰å…¥ä¸Šä¸‹æ–‡
            context = await enhanced_ai_qa_service._load_context_if_needed(
                db, request, user_id, classification
            )
            
            # æª¢æŸ¥å·¥ä½œæµæ“ä½œ
            workflow_action = getattr(request, 'workflow_action', None)
            
            # ç²å–å·²çŸ¥çš„æ–‡æª”ID
            cached_doc_ids = []
            if context and context.get('cached_document_ids'):
                cached_doc_ids = context['cached_document_ids']
            
            target_doc_ids = []
            if classification.target_document_ids:
                target_doc_ids = classification.target_document_ids
            elif cached_doc_ids:
                target_doc_ids = cached_doc_ids
            
            if not target_doc_ids:
                yield f"data: {json.dumps({'type': 'error', 'message': 'ç„¡æ³•ç¢ºå®šè¦æŸ¥è©¢çš„æ–‡æª”'}, ensure_ascii=False)}\n\n"
                return
            
            # æ­¥é©Ÿ4.1: è«‹æ±‚æ‰¹å‡†è©³ç´°æŸ¥è©¢
            if workflow_action != 'approve_detail_query':
                logger.info("ğŸ”” è«‹æ±‚ç”¨æˆ¶æ‰¹å‡† MongoDB è©³ç´°æŸ¥è©¢")
                
                # ç²å–æ–‡æª”åç¨±
                from app.crud.crud_documents import get_documents_by_ids
                doc_names = []
                try:
                    documents = await get_documents_by_ids(db, [str(doc_id) for doc_id in target_doc_ids])
                    doc_names = [doc.filename for doc in documents if hasattr(doc, 'filename')]
                    logger.info(f"ç²å–åˆ° {len(doc_names)} å€‹æ–‡æª”åç¨±ç”¨æ–¼é¡¯ç¤º")
                except Exception as e:
                    logger.warning(f"ç²å–æ–‡æª”åç¨±å¤±æ•—: {e}")
                
                workflow_state = {
                    "current_step": "awaiting_detail_query_approval",
                    "classification": {
                        "intent": classification.intent,
                        "confidence": classification.confidence,
                        "reasoning": classification.reasoning if hasattr(classification, 'reasoning') else None
                    },
                    "question": request.question,
                    "pending_action": "approve_detail_query",
                    "target_documents": target_doc_ids,
                    "document_names": doc_names,  # æ·»åŠ æ–‡æª”åç¨±
                    "query_type": "è©³ç´°æ•¸æ“šæŸ¥è©¢",
                    "estimated_time": "2-4ç§’"
                }
                yield f"data: {json.dumps({'type': 'approval_needed', 'workflow_state': workflow_state}, ensure_ascii=False)}\n\n"
                return
            
            # å·²æ‰¹å‡†ï¼ŒåŸ·è¡Œè©³ç´°æŸ¥è©¢
            logger.info("âœ… ç”¨æˆ¶æ‰¹å‡† MongoDB è©³ç´°æŸ¥è©¢")
            yield f"data: {json.dumps({'type': 'progress', 'stage': 'detail_query_approved', 'message': 'âœ… é–‹å§‹åŸ·è¡Œè©³ç´°æŸ¥è©¢...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            # æ­¥é©Ÿ4.2: åŸ·è¡Œ MongoDB æŸ¥è©¢
            all_detailed_data = []
            for doc_id in target_doc_ids:
                try:
                    # ä½¿ç”¨ AI ç”ŸæˆæŸ¥è©¢
                    from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified, AIRequest
                    from app.services.ai.unified_ai_config import TaskType
                    from uuid import UUID
                    
                    # å°‡å­—ç¬¦ä¸² ID è½‰æ›ç‚º UUIDï¼ˆå¦‚æœéœ€è¦ï¼‰
                    doc_id_uuid = UUID(str(doc_id)) if not isinstance(doc_id, UUID) else doc_id
                    
                    doc_info = await db.documents.find_one({"_id": doc_id_uuid})
                    if not doc_info:
                        logger.warning(f"ç„¡æ³•æ‰¾åˆ°æ–‡æª” {doc_id}")
                        continue
                    
                    # æº–å‚™ schema ä¿¡æ¯ï¼Œé¿å…åŒ…å«ç„¡æ³•åºåˆ—åŒ–çš„å°è±¡
                    schema_info = {
                        "available_fields": ["filename", "extracted_text", "analysis"],
                        "document_filename": doc_info.get("filename", "æœªçŸ¥æ–‡ä»¶")
                    }
                    
                    ai_request = AIRequest(
                        task_type=TaskType.MONGODB_DETAIL_QUERY_GENERATION,
                        content=f"ç”¨æˆ¶å•é¡Œ: {request.question}",
                        prompt_params={
                            "user_question": request.question,
                            "document_id": str(doc_id_uuid),
                            "document_schema_info": json.dumps(schema_info, ensure_ascii=False)
                        }
                    )
                    
                    ai_response = await unified_ai_service_simplified.process_request(ai_request, db)
                    
                    if ai_response.success:
                        from app.models.ai_models_simplified import AIMongoDBQueryDetailOutput
                        
                        if isinstance(ai_response.output_data, str):
                            query_output = AIMongoDBQueryDetailOutput(**json.loads(ai_response.output_data))
                        else:
                            query_output = ai_response.output_data
                        
                        projection = query_output.projection or {}
                        sub_filter = query_output.sub_filter or {}
                        
                        # åŸ·è¡ŒæŸ¥è©¢
                        query_result = await db.documents.find_one(
                            {"_id": doc_id_uuid, **sub_filter},
                            projection
                        )
                        
                        if query_result:
                            query_result["_reference_number"] = target_doc_ids.index(doc_id) + 1
                            all_detailed_data.append(query_result)
                            logger.info(f"æˆåŠŸç²å–æ–‡æª” {doc_info.get('filename')} çš„è©³ç´°æ•¸æ“š")
                        
                except Exception as e:
                    logger.error(f"æŸ¥è©¢æ–‡æª” {doc_id} å¤±æ•—: {e}")
            
            if not all_detailed_data:
                yield f"data: {json.dumps({'type': 'error', 'message': 'æœªèƒ½ç²å–ä»»ä½•æ–‡æª”è©³ç´°æ•¸æ“š'}, ensure_ascii=False)}\n\n"
                return
            
            # æ­¥é©Ÿ4.3: æµå¼ç”Ÿæˆç­”æ¡ˆ
            yield f"data: {json.dumps({'type': 'progress', 'stage': 'ai_generating', 'message': 'ğŸ¤– AI æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...'}, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.1)
            
            full_answer = ""
            async for chunk in document_detail_query_handler.generate_answer_from_details_stream(
                question=request.question,
                detailed_data=all_detailed_data,
                classification=classification,
                db=db,
                user_id=user_id,
                conversation_id=request.conversation_id,
                context=context
            ):
                full_answer += chunk
                yield f"data: {json.dumps({'type': 'chunk', 'text': chunk}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.01)
            
            logger.info(f"âœ… [Stream QA] è©³ç´°æŸ¥è©¢æµå¼ç”Ÿæˆå®Œæˆï¼Œç¸½é•·åº¦: {len(full_answer)} å­—ç¬¦")
            
            # æ­¥é©Ÿ4.4: ä¿å­˜åˆ°å°è©±
            if request.conversation_id and user_id:
                try:
                    from app.services.qa_workflow.conversation_helper import conversation_helper
                    await conversation_helper.save_qa_to_conversation(
                        db=db,
                        conversation_id=request.conversation_id,
                        user_id=user_id,
                        question=request.question,
                        answer=full_answer,
                        tokens_used=0,
                        source_documents=target_doc_ids
                    )
                    logger.info("ğŸ’¾ å·²ä¿å­˜åˆ°å°è©±æ­·å²")
                except Exception as e:
                    logger.error(f"âŒ ä¿å­˜å°è©±å¤±æ•—: {e}")
            
            # ç™¼é€å…ƒæ•¸æ“š
            metadata = {
                'type': 'metadata',
                'tokens_used': 0,
                'source_documents': target_doc_ids,
                'processing_time': 0,
            }
            yield f"data: {json.dumps(metadata, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
            return
        
        # === æ­¥é©Ÿ 5: éœ€è¦æœç´¢çš„æ„åœ–ï¼ˆæ–‡æª”æœç´¢ã€è¤‡é›œåˆ†æï¼‰===
        if classification.intent in [
            QuestionIntent.DOCUMENT_SEARCH,
            QuestionIntent.COMPLEX_ANALYSIS
        ]:
            # è¼‰å…¥ä¸Šä¸‹æ–‡
            context = await enhanced_ai_qa_service._load_context_if_needed(
                db, request, user_id, classification
            )
            
            # === æª¢æŸ¥æ˜¯å¦éœ€è¦æœç´¢æ‰¹å‡† ===
            from app.services.qa_workflow.workflow_coordinator import workflow_coordinator
            from app.core.config import settings
            
            # æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦å·²ç¶“æ‰¹å‡†é
            already_approved = request.workflow_action in ['approve_search', 'approve_detail_query']
            
            if already_approved:
                logger.info(f"âœ… ç”¨æˆ¶å·²æ‰¹å‡†æ“ä½œ: {request.workflow_action}ï¼Œè·³éæ‰¹å‡†æª¢æŸ¥")
            else:
                config = {
                    'auto_approve_all_searches': getattr(settings, 'AUTO_APPROVE_ALL_SEARCHES', False),
                    'auto_approve_high_confidence': getattr(settings, 'AUTO_APPROVE_HIGH_CONFIDENCE', False)
                }
                
                needs_approval = workflow_coordinator.should_request_search_approval(classification, config)
                
                if needs_approval:
                    logger.info("ğŸ”” éœ€è¦ç”¨æˆ¶æ‰¹å‡†æ–‡æª”æœç´¢")
                    
                    workflow_state = {
                        "current_step": "awaiting_search_approval",
                        "classification": {
                            "intent": classification.intent,
                            "confidence": classification.confidence,
                            "reasoning": classification.reasoning if hasattr(classification, 'reasoning') else None
                        },
                        "question": request.question,
                        "pending_action": "approve_search"
                    }
                    
                    yield f"data: {json.dumps({'type': 'approval_needed', 'workflow_state': workflow_state}, ensure_ascii=False)}\n\n"
                    return
            
            # å·²æ‰¹å‡†æˆ–è‡ªå‹•æ‰¹å‡†ï¼Œç¹¼çºŒè™•ç†
            logger.info("âœ… æœç´¢å·²æ‰¹å‡†ï¼Œé–‹å§‹åŸ·è¡Œ...")
            
            # === æ­¥é©Ÿ 6: åŸ·è¡ŒæŸ¥è©¢é‡å¯«ï¼ˆæ–‡æª”æœç´¢å’Œè¤‡é›œåˆ†æï¼‰===
            query_rewrite_result = None
            rewritten_queries = []
            
            if classification.intent in [QuestionIntent.DOCUMENT_SEARCH, QuestionIntent.COMPLEX_ANALYSIS]:
                yield f"data: {json.dumps({'type': 'progress', 'stage': 'query_rewriting', 'message': 'ğŸ”„ æ­£åœ¨å„ªåŒ–æŸ¥è©¢èªå¥...'}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.05)
                
                # åŸ·è¡ŒæŸ¥è©¢é‡å¯«
                from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified
                
                # å„ªå…ˆä½¿ç”¨åˆ†é¡æ¨ç†ä½œç‚ºé‡å¯«è¼¸å…¥ï¼›è‹¥ç„¡å‰‡ä½¿ç”¨åŒ…å«æ¾„æ¸…çš„æœ‰æ•ˆå•é¡Œï¼›æœ€å¾Œå›é€€åˆ°åŸå§‹å•é¡Œ
                base_rewrite_input = classification.reasoning if hasattr(classification, 'reasoning') and classification.reasoning else effective_question
                query_rewrite_response = await unified_ai_service_simplified.rewrite_query(
                    original_query=base_rewrite_input,
                    model_preference=request.model_preference,
                    user_id=str(user_id) if user_id else None,
                    session_id=request.session_id,
                    db=db
                )
                
                if query_rewrite_response.success and query_rewrite_response.output_data:
                    from app.models.vector_models import QueryRewriteResult
                    from app.models.ai_models_simplified import AIQueryRewriteOutput
                    
                    ai_query_output = query_rewrite_response.output_data
                    if isinstance(ai_query_output, AIQueryRewriteOutput):
                        rewritten_queries = ai_query_output.rewritten_queries or [base_rewrite_input]
                        
                        query_rewrite_result = QueryRewriteResult(
                            original_query=base_rewrite_input,
                            rewritten_queries=rewritten_queries,
                            extracted_parameters=ai_query_output.extracted_parameters or {},
                            intent_analysis=ai_query_output.intent_analysis or "",
                            search_strategy_suggestion=getattr(ai_query_output, 'search_strategy_suggestion', None),
                            query_granularity=getattr(ai_query_output, 'query_granularity', None)
                        )
                        
                        # ç™¼é€æŸ¥è©¢é‡å¯«çµæœ
                        yield f"data: {json.dumps({'type': 'progress', 'stage': 'query_rewrite', 'message': f'âœ… æŸ¥è©¢å„ªåŒ–å®Œæˆ', 'detail': {'count': len(rewritten_queries), 'queries': rewritten_queries}}, ensure_ascii=False)}\n\n"
                        await asyncio.sleep(0.1)
                    else:
                        rewritten_queries = [base_rewrite_input]
                else:
                    logger.warning(f"æŸ¥è©¢é‡å¯«å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹æŸ¥è©¢: {query_rewrite_response.error_message}")
                    rewritten_queries = [base_rewrite_input]
                
                # === æ­¥é©Ÿ 6: å‘é‡æœç´¢ ===
                yield f"data: {json.dumps({'type': 'progress', 'stage': 'vector_search', 'message': 'ğŸ” æ­£åœ¨å‘é‡è³‡æ–™åº«ä¸­æœç´¢ç›¸é—œæ–‡æª”...'}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.05)
                
                # æ±ºå®šæœç´¢ç­–ç•¥
                search_strategy = enhanced_ai_qa_service._extract_search_strategy(query_rewrite_result)
                logger.info(f"ä½¿ç”¨æœç´¢ç­–ç•¥: {search_strategy}")
                
                # åŸ·è¡Œå‘é‡æœç´¢
                semantic_results = await enhanced_ai_qa_service._unified_search(
                    db=db,
                    queries=rewritten_queries,
                    search_strategy=search_strategy,
                    top_k=getattr(request, 'top_k', 5),
                    user_id=str(user_id) if user_id else None,
                    request_id=None,
                    similarity_threshold=getattr(request, 'similarity_threshold', 0.3),
                    document_ids=request.document_ids if hasattr(request, 'document_ids') else None
                )
                
                # ç™¼é€æœç´¢çµæœ
                if semantic_results:
                    doc_count = len(semantic_results)
                    avg_similarity = sum(r.similarity_score for r in semantic_results) / doc_count if doc_count > 0 else 0
                    
                    yield f"data: {json.dumps({'type': 'progress', 'stage': 'search_complete', 'message': f'ğŸ“„ æ‰¾åˆ° {doc_count} å€‹ç›¸é—œæ–‡æª”ï¼ˆå¹³å‡ç›¸ä¼¼åº¦ {avg_similarity:.1%}ï¼‰'}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.1)
                else:
                    yield f"data: {json.dumps({'type': 'progress', 'stage': 'search_complete', 'message': 'âš ï¸ æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”'}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.1)
                
                # === æ­¥é©Ÿ 7: ç²å–å®Œæ•´æ–‡æª” ===
                if semantic_results:
                    yield f"data: {json.dumps({'type': 'progress', 'stage': 'loading_documents', 'message': 'ğŸ“š æ­£åœ¨è¼‰å…¥æ–‡æª”å…§å®¹...'}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.05)
                    
                    from app.crud.crud_documents import get_documents_by_ids
                    document_ids_from_search = [result.document_id for result in semantic_results]
                    full_documents = await get_documents_by_ids(db, document_ids_from_search)
                    
                    logger.info(f"æˆåŠŸè¼‰å…¥ {len(full_documents)} å€‹å®Œæ•´æ–‡æª”")
                else:
                    full_documents = []
                
                # === æ­¥é©Ÿ 7: æº–å‚™ä¸Šä¸‹æ–‡ä¸¦ç”Ÿæˆç­”æ¡ˆ ===
                yield f"data: {json.dumps({'type': 'progress', 'stage': 'preparing_context', 'message': 'ğŸ“ æ­£åœ¨æº–å‚™ä¸Šä¸‹æ–‡...'}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.05)
                
                # 1. è¼‰å…¥å°è©±æ­·å²
                context_parts = []
                if request.conversation_id:
                    from app.services.qa_workflow.unified_context_helper import unified_context_helper
                    conversation_history_text = await unified_context_helper.load_and_format_conversation_history(
                        db=db,
                        conversation_id=request.conversation_id,
                        user_id=str(user_id) if user_id else None,
                        limit=10,  # æœ€å¤šè¼‰å…¥10è¼ªå°è©±
                        max_content_length=3000  # é™åˆ¶æ­·å²é•·åº¦
                    )
                    if conversation_history_text:
                        context_parts.append(f"=== å°è©±æ­·å² ===\n{conversation_history_text}\n")
                        logger.info(f"å·²è¼‰å…¥å°è©±æ­·å²ï¼Œé•·åº¦: {len(conversation_history_text)} å­—ç¬¦")
                
                # 2. æå–æ–‡æª”å…§å®¹ä½œç‚ºä¸Šä¸‹æ–‡
                if full_documents:
                    for doc in full_documents[:5]:  # æœ€å¤šä½¿ç”¨5å€‹æ–‡æª”
                        doc_content = ""
                        
                        # å˜—è©¦ç²å– AI åˆ†ææ‘˜è¦
                        if hasattr(doc, 'analysis') and doc.analysis:
                            if hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                                try:
                                    from app.models.ai_models_simplified import AIDocumentAnalysisOutputDetail
                                    analysis_output = AIDocumentAnalysisOutputDetail(**doc.analysis.ai_analysis_output)
                                    if analysis_output.key_information and analysis_output.key_information.content_summary:
                                        doc_content = analysis_output.key_information.content_summary
                                except Exception:
                                    pass
                        
                        # å›é€€åˆ°åŸå§‹æ–‡æœ¬
                        if not doc_content and hasattr(doc, 'extracted_text') and doc.extracted_text:
                            doc_content = doc.extracted_text[:2000]  # é™åˆ¶é•·åº¦
                        
                        if doc_content:
                            context_parts.append(f"æ–‡æª”: {doc.filename}\n{doc_content}")
                
                # å¦‚æœæ²’æœ‰æ–‡æª”ä½†æœ‰å°è©±æ­·å²ï¼Œä¹Ÿå¯ä»¥å›ç­”
                if not any("æ–‡æª”:" in part or "===" in part for part in context_parts[1:] if len(context_parts) > 1):
                    if not context_parts:
                        context_parts = ["æ²’æœ‰å¯ç”¨çš„æ–‡æª”ä¸Šä¸‹æ–‡"]
                    else:
                        # æœ‰å°è©±æ­·å²ä½†æ²’æœ‰æ–‡æª”
                        logger.info("åªæœ‰å°è©±æ­·å²ï¼Œæ²’æœ‰é¡å¤–çš„æ–‡æª”å…§å®¹")
                
                # ç²å–æ„åœ–åˆ†æ
                intent_analysis = ""
                if query_rewrite_result and query_rewrite_result.intent_analysis:
                    intent_analysis = query_rewrite_result.intent_analysis
                elif hasattr(classification, 'reasoning'):
                    intent_analysis = classification.reasoning
                
                # === æ­¥é©Ÿ 8: æµå¼ç”Ÿæˆç­”æ¡ˆ ===
                yield f"data: {json.dumps({'type': 'progress', 'stage': 'ai_generating', 'message': 'ğŸ¤– AI æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...'}, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.1)
                
                from app.services.ai.unified_ai_service_stream import generate_answer_stream
                
                full_answer = ""
                async for chunk in generate_answer_stream(
                    user_question=request.question,
                    intent_analysis=intent_analysis,
                    document_context=context_parts,
                    model_preference=request.model_preference,
                    user_id=user_id,
                    db=db,
                    detailed_text_max_length=getattr(request, 'detailed_text_max_length', 8000),
                    max_chars_per_doc=getattr(request, 'max_chars_per_doc', None)
                ):
                    full_answer += chunk
                    yield f"data: {json.dumps({'type': 'chunk', 'text': chunk}, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0.01)
                
                logger.info(f"âœ… [Stream QA] æµå¼ç”Ÿæˆå®Œæˆï¼Œç¸½é•·åº¦: {len(full_answer)} å­—ç¬¦")
                
                # === æ­¥é©Ÿ 9: ä¿å­˜åˆ°å°è©± ===
                if request.conversation_id and user_id:
                    try:
                        from app.services.qa_workflow.conversation_helper import conversation_helper
                        await conversation_helper.save_qa_to_conversation(
                            db=db,
                            conversation_id=request.conversation_id,
                            user_id=user_id,
                            question=request.question,
                            answer=full_answer,
                            tokens_used=0,
                            source_documents=[str(doc.id) for doc in full_documents]
                        )
                        logger.info("ğŸ’¾ å·²ä¿å­˜åˆ°å°è©±æ­·å²")
                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜å°è©±å¤±æ•—: {e}")
                
                # ç™¼é€å…ƒæ•¸æ“š
                metadata = {
                    'type': 'metadata',
                    'tokens_used': 0,
                    'source_documents': [str(doc.id) for doc in full_documents],
                    'processing_time': 0,
                }
                yield f"data: {json.dumps(metadata, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
                return
        
        # === ç°¡å–®äº‹å¯¦æŸ¥è©¢ï¼ˆä¸éœ€è¦æ–‡æª”æœç´¢ï¼‰===
        elif classification.intent == QuestionIntent.SIMPLE_FACTUAL:
            logger.info("â†’ è™•ç†ç°¡å–®äº‹å¯¦æŸ¥è©¢")
            response = await simple_factual_handler.handle(
                request, classification, db, user_id, None
            )
            
            if response.answer:
                yield f"data: {json.dumps({'type': 'complete', 'answer': response.answer}, ensure_ascii=False)}\n\n"
            else:
                yield f"data: {json.dumps({'type': 'error', 'message': 'è™•ç†å¤±æ•—'}, ensure_ascii=False)}\n\n"
            return
        
        # === æœªçŸ¥æ„åœ–ï¼Œå›é€€åˆ°æ¨™æº–è™•ç† ===
        else:
            logger.warning(f"æœªçŸ¥æ„åœ–: {classification.intent}ï¼Œä½¿ç”¨æ¨™æº–æµç¨‹")
            yield f"data: {json.dumps({'type': 'error', 'message': f'æš«ä¸æ”¯æŒçš„æ„åœ–é¡å‹: {classification.intent}'}, ensure_ascii=False)}\n\n"
            return
            
    except Exception as e:
        logger.error(f"âŒ [Stream QA] æµå¼å•ç­”å¤±æ•—: {e}", exc_info=True)
        error_msg = {
            'type': 'error',
            'message': f"æµå¼å•ç­”å¤±æ•—: {str(e)}"
        }
        yield f"data: {json.dumps(error_msg, ensure_ascii=False)}\n\n"


def requires_streaming(response) -> bool:
    """åˆ¤æ–·æ˜¯å¦éœ€è¦æµå¼è¼¸å‡º"""
    # å¦‚æœæ˜¯å¯’æš„ã€æ¾„æ¸…ç­‰çŸ­å›ç­”ï¼Œä¸éœ€è¦æµå¼
    if hasattr(response, 'classification') and response.classification:
        intent = response.classification.intent if hasattr(response.classification, 'intent') else ''
        if intent in ['greeting', 'chitchat', 'clarification_needed']:
            return False
    
    # å¦‚æœç­”æ¡ˆå¾ˆçŸ­ï¼ˆ<100å­—ç¬¦ï¼‰ï¼Œä¸éœ€è¦æµå¼
    if hasattr(response, 'answer') and response.answer and len(response.answer) < 100:
        return False
    
    return True


@router.post("/qa/stream")
async def stream_qa(
    request: AIQARequest,
    current_user: User = Depends(get_current_active_user),
    db: AsyncIOMotorDatabase = Depends(get_db)
):
    """
    æµå¼å•ç­”ç«¯é» - å¯¦æ™‚ç™¼é€æ¯å€‹è™•ç†æ­¥é©Ÿçš„é€²åº¦
    
    è¿”å› Server-Sent Events (SSE) æµ
    
    äº‹ä»¶é¡å‹ï¼š
    - progress: è™•ç†é€²åº¦ï¼ˆå‹•æ…‹ï¼Œåªæœ‰å¯¦éš›åŸ·è¡Œçš„æ­¥é©Ÿæ‰ç™¼é€ï¼‰
    - chunk: ç­”æ¡ˆæ–‡æœ¬å¡Š
    - approval_needed: éœ€è¦ç”¨æˆ¶æ‰¹å‡†
    - complete: å®Œæ•´ç­”æ¡ˆï¼ˆå°æ–¼ä¸éœ€è¦æµå¼çš„ç°¡çŸ­å›ç­”ï¼‰
    - metadata: å…ƒæ•¸æ“šä¿¡æ¯
    - error: éŒ¯èª¤ä¿¡æ¯
    """
    logger.info(f"ğŸ“¨ [Stream API] æ”¶åˆ°æµå¼å•ç­”è«‹æ±‚: user={current_user.username}, question={request.question[:50]}")
    
    try:
        return StreamingResponse(
            generate_streaming_answer(db, request, str(current_user.id)),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç·©è¡
            }
        )
    except Exception as e:
        logger.error(f"âŒ [Stream API] å‰µå»ºæµå¼éŸ¿æ‡‰å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å‰µå»ºæµå¼éŸ¿æ‡‰å¤±æ•—: {str(e)}")
