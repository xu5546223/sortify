"""
QA å®Œæ•´ API æµç¨‹æ¸¬è©¦è…³æœ¬

æ¸¬è©¦å•é¡Œ: å¹«æˆ‘æ‰¾æ‰€æœ‰çš„ç½°å–®
èª¿ç”¨çœŸå¯¦ API ç«¯é»ï¼Œé¡¯ç¤ºæ¯ä¸€æ­¥çš„çµæœ
"""
import asyncio
import sys
import os
import json
from datetime import datetime
from typing import Optional

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from motor.motor_asyncio import AsyncIOMotorClient
from app.core.config import settings
from app.services.vector.embedding_service import embedding_service
from app.services.vector.vector_db_service import vector_db_service
from app.services.vector.enhanced_search_service import enhanced_search_service
from app.services.qa_core.qa_query_rewriter import QAQueryRewriter
from app.services.qa_core.qa_search_coordinator import QASearchCoordinator
from app.services.qa_core.qa_answer_service import QAAnswerService
from app.models.vector_models import AIQARequest, QueryRewriteResult


def print_separator(title: str, char: str = "="):
    """æ‰“å°åˆ†éš”ç·š"""
    print(f"\n{char * 80}")
    print(f"ğŸ“Š {title}")
    print(f"{char * 80}")


def print_json(data, indent: int = 2):
    """æ ¼å¼åŒ–æ‰“å° JSON"""
    if hasattr(data, 'model_dump'):
        data = data.model_dump()
    elif hasattr(data, '__dict__'):
        data = data.__dict__
    print(json.dumps(data, ensure_ascii=False, indent=indent, default=str))


async def test_qa_full_flow():
    """æ¸¬è©¦å®Œæ•´çš„ QA æµç¨‹"""
    
    print("=" * 80)
    print("ğŸ” QA å®Œæ•´ API æµç¨‹æ¸¬è©¦")
    print("=" * 80)
    
    # æ¸¬è©¦å•é¡Œ
    test_query = "å¹«æˆ‘æ‰¾æ‰€æœ‰çš„ç½°å–®"
    print(f"\nğŸ“ æ¸¬è©¦å•é¡Œ: {test_query}")
    print("-" * 80)
    
    # é€£æ¥ MongoDB (ä½¿ç”¨æ­£ç¢ºçš„ UUID è¡¨ç¤ºæ–¹å¼)
    client = AsyncIOMotorClient(
        settings.MONGODB_URL,
        uuidRepresentation='standard'  # é‡è¦ï¼šèˆ‡æ‡‰ç”¨ç¨‹å¼ä¿æŒä¸€è‡´
    )
    db = client[settings.DB_NAME]
    
    # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«é›†åˆ
    vector_db_service.create_collection(768)
    
    # ç²å–æ¸¬è©¦ç”¨æˆ¶
    import uuid as uuid_module
    sample_doc = await db.documents.find_one({})
    if not sample_doc:
        print("âŒ è³‡æ–™åº«ä¸­æ²’æœ‰æ–‡æª”")
        return
    
    owner_id = sample_doc.get("owner_id")
    if isinstance(owner_id, uuid_module.UUID):
        user_id = str(owner_id)
    elif isinstance(owner_id, bytes):
        user_id = str(uuid_module.UUID(bytes=owner_id))
    else:
        user_id = str(owner_id)
    print(f"ğŸ‘¤ ä½¿ç”¨ç”¨æˆ¶ ID: {user_id}")
    
    # åˆå§‹åŒ–æœå‹™
    query_rewriter = QAQueryRewriter()
    search_coordinator = QASearchCoordinator()
    answer_service = QAAnswerService()
    
    total_tokens = 0
    
    # ========== Step 1: æŸ¥è©¢é‡å¯« ==========
    print_separator("Step 1: æŸ¥è©¢é‡å¯« (Query Rewrite)")
    
    query_rewrite_result, rewrite_tokens = await query_rewriter.rewrite_query(
        db=db,
        original_query=test_query,
        user_id=user_id,
        request_id="test_request_001",
        query_rewrite_count=3
    )
    total_tokens += rewrite_tokens
    
    print(f"\nâœ… æŸ¥è©¢é‡å¯«å®Œæˆï¼Œæ¶ˆè€— {rewrite_tokens} tokens")
    print(f"\nğŸ“‹ åŸå§‹æŸ¥è©¢: {query_rewrite_result.original_query}")
    print(f"\nğŸ“‹ é‡å¯«å¾Œçš„æŸ¥è©¢:")
    for i, q in enumerate(query_rewrite_result.rewritten_queries, 1):
        print(f"   {i}. {q}")
    print(f"\nğŸ“‹ æ„åœ–åˆ†æ: {query_rewrite_result.intent_analysis}")
    print(f"\nğŸ“‹ æŸ¥è©¢ç²’åº¦: {query_rewrite_result.query_granularity}")
    print(f"\nğŸ“‹ å»ºè­°æœç´¢ç­–ç•¥: {query_rewrite_result.search_strategy_suggestion}")
    print(f"\nğŸ“‹ æå–çš„åƒæ•¸:")
    print_json(query_rewrite_result.extracted_parameters)
    
    # ========== Step 2: æ±ºå®šæœç´¢ç­–ç•¥ ==========
    print_separator("Step 2: æ±ºå®šæœç´¢ç­–ç•¥")
    
    from app.services.qa_orchestrator import extract_search_strategy
    search_strategy = extract_search_strategy(query_rewrite_result)
    print(f"\nğŸ¯ é¸æ“‡çš„æœç´¢ç­–ç•¥: {search_strategy}")
    
    # ========== Step 3: åŸ·è¡Œå‘é‡æœç´¢ ==========
    print_separator("Step 3: åŸ·è¡Œå‘é‡æœç´¢")
    
    queries = query_rewrite_result.rewritten_queries if query_rewrite_result.rewritten_queries else [test_query]
    
    search_results = await search_coordinator.unified_search(
        db=db,
        queries=queries,
        user_id=user_id,
        search_strategy=search_strategy,
        top_k=5,
        similarity_threshold=0.3,
        enable_diversity_optimization=True
    )
    
    print(f"\nâœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} å€‹çµæœ")
    
    for i, result in enumerate(search_results, 1):
        print(f"\n--- æœç´¢çµæœ {i} ---")
        print(f"ğŸ“ Document ID: {result.document_id}")
        print(f"ğŸ“ˆ ç›¸ä¼¼åº¦/RRFåˆ†æ•¸: {result.similarity_score:.4f}")
        print(f"ğŸ“ è¡Œè™Ÿç¯„åœ: {result.start_line} - {result.end_line}")
        print(f"ğŸ“¦ Chunk é¡å‹: {result.chunk_type}")
        print(f"ğŸ“ summary_text (å‰ 400 å­—):")
        text_preview = result.summary_text[:400] + "..." if len(result.summary_text) > 400 else result.summary_text
        # ç¸®é€²é¡¯ç¤º
        for line in text_preview.split('\n'):
            print(f"   {line}")
        
        # é¡¯ç¤ºé‡è¦ metadata
        if result.metadata:
            print(f"ğŸ·ï¸ é—œéµ Metadata:")
            for key in ['type', 'vectorization_strategy', 'chunk_summary', 'search_strategy']:
                if result.metadata.get(key):
                    print(f"   - {key}: {result.metadata[key]}")
    
    # ========== Step 4: ç²å–å®Œæ•´æ–‡æª” ==========
    print_separator("Step 4: ç²å–å®Œæ•´æ–‡æª”")
    
    documents = []
    if search_results:
        from app.crud.crud_documents import get_documents_by_ids
        document_ids = [result.document_id for result in search_results]
        
        try:
            documents = await get_documents_by_ids(db, document_ids)
            print(f"\nâœ… ç²å–åˆ° {len(documents)} å€‹å®Œæ•´æ–‡æª”")
            
            for i, doc in enumerate(documents, 1):
                print(f"\n--- æ–‡æª” {i} ---")
                print(f"ğŸ“ ID: {doc.id}")
                print(f"ğŸ“ æ–‡ä»¶å: {doc.filename}")
                print(f"ğŸ“‚ æ–‡ä»¶é¡å‹: {doc.file_type}")
                
                # ç²å– AI åˆ†æçš„æ‘˜è¦
                ai_summary = None
                if hasattr(doc, 'analysis') and doc.analysis:
                    if hasattr(doc.analysis, 'ai_analysis_output') and isinstance(doc.analysis.ai_analysis_output, dict):
                        key_info = doc.analysis.ai_analysis_output.get("key_information", {})
                        if isinstance(key_info, dict):
                            ai_summary = key_info.get("content_summary", "")
                
                print(f"ğŸ“ AI æ‘˜è¦ (content_summary):")
                if ai_summary:
                    preview = ai_summary[:300] + "..." if len(ai_summary) > 300 else ai_summary
                    for line in preview.split('\n'):
                        print(f"   {line}")
                else:
                    print(f"   (ç„¡)")
        except Exception as e:
            print(f"âš ï¸ ç²å–æ–‡æª”å¤±æ•—: {e}")
            documents = []
    
    # ========== Step 5: ä½¿ç”¨æœç´¢çµæœç›´æ¥ç”Ÿæˆç­”æ¡ˆ (å„ªåŒ–æ–¹æ¡ˆ) ==========
    print_separator("Step 5: ä½¿ç”¨æœç´¢çµæœç›´æ¥ç”Ÿæˆ AI ç­”æ¡ˆ (å„ªåŒ–æ–¹æ¡ˆ)")
    
    if search_results:
        print("\nğŸ¤– æ­£åœ¨ä½¿ç”¨æœç´¢çµæœçš„ summary_text ç›´æ¥ç”Ÿæˆç­”æ¡ˆ...")
        print("ğŸ“Œ é€™æ˜¯å„ªåŒ–æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨å‘é‡æœç´¢è¿”å›çš„ chunk å…§å®¹ï¼Œä¸ä¾è³´ MongoDB æ–‡æª”")
        
        # æ§‹å»ºä¸Šä¸‹æ–‡ - ç›´æ¥ä½¿ç”¨æœç´¢çµæœçš„ summary_text
        context_parts = []
        for i, result in enumerate(search_results[:5], 1):
            chunk_type = result.metadata.get('type', 'unknown') if result.metadata else 'unknown'
            strategy = result.metadata.get('vectorization_strategy', '') if result.metadata else ''
            chunk_summary = result.metadata.get('chunk_summary', '') if result.metadata else ''
            
            context = f"""=== æ–‡æª” {i}ï¼ˆå¼•ç”¨ç·¨è™Ÿ: citation:{i}ï¼‰===
Document ID: {result.document_id}
ç›¸ä¼¼åº¦åˆ†æ•¸: {result.similarity_score:.4f}
å‘é‡é¡å‹: {chunk_type}
å‘é‡åŒ–ç­–ç•¥: {strategy}
è¡Œè™Ÿç¯„åœ: {result.start_line or 'N/A'} - {result.end_line or 'N/A'}
Chunk æ‘˜è¦: {chunk_summary}

å…§å®¹:
{result.summary_text}
"""
            context_parts.append(context)
        
        # é¡¯ç¤ºå°‡è¦å‚³çµ¦ AI çš„ä¸Šä¸‹æ–‡
        print(f"\nğŸ“‹ å°‡å‚³çµ¦ AI çš„ä¸Šä¸‹æ–‡ ({len(context_parts)} å€‹):")
        print("-" * 40)
        for ctx in context_parts:
            print(ctx[:500] + "..." if len(ctx) > 500 else ctx)
            print("-" * 40)
        
        # èª¿ç”¨ AI ç”Ÿæˆç­”æ¡ˆ
        from app.services.ai.unified_ai_service_simplified import unified_ai_service_simplified
        
        ai_response = await unified_ai_service_simplified.generate_answer(
            user_question=test_query,
            intent_analysis=query_rewrite_result.intent_analysis or "",
            document_context=context_parts,
            db=db,
            model_preference=None
        )
        
        if ai_response.success and ai_response.output_data:
            answer_tokens = ai_response.token_usage.total_tokens if ai_response.token_usage else 0
            total_tokens += answer_tokens
            
            answer_text = ai_response.output_data.answer_text if hasattr(ai_response.output_data, 'answer_text') else str(ai_response.output_data)
            
            print(f"\nâœ… AI ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            print(f"ğŸ“Š æ¶ˆè€— tokens: {answer_tokens}")
            print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {ai_response.model_used}")
            print(f"\nğŸ“ AI ç”Ÿæˆçš„ç­”æ¡ˆ:")
            print("=" * 60)
            print(answer_text)
            print("=" * 60)
        else:
            print(f"\nâŒ AI ç­”æ¡ˆç”Ÿæˆå¤±æ•—: {ai_response.error_message}")
    else:
        print("\nâš ï¸ æ²’æœ‰æœç´¢çµæœå¯ç”¨æ–¼ç”Ÿæˆç­”æ¡ˆ")
    
    # ========== ç¸½çµ ==========
    print_separator("ç¸½çµ", "=")
    
    print(f"\nğŸ“Š ç¸½æ¶ˆè€— tokens: {total_tokens}")
    print(f"\nâš ï¸ é—œéµç™¼ç¾:")
    print("-" * 40)
    print("1. å‘é‡æœç´¢è¿”å›çš„ summary_text åŒ…å«:")
    print("   - æ‘˜è¦å‘é‡: æ–‡ä»¶å+æ‘˜è¦+é—œéµè©çš„çµ„åˆ")
    print("   - Chunk å‘é‡: [Summary]+[Content] æ··åˆå…§å®¹ æˆ– åŸå§‹æ–‡æœ¬")
    print("")
    print("2. ç›®å‰ QA æµç¨‹:")
    print("   - æœç´¢çµæœçš„ summary_text è¢«ä¸Ÿæ£„")
    print("   - é‡æ–°å¾ MongoDB ç²å–æ–‡æª”")
    print("   - ä½¿ç”¨æ–‡æª”ç´šçš„ content_summary ä½œç‚ºä¸Šä¸‹æ–‡")
    print("")
    print("3. å„ªåŒ–å»ºè­°:")
    print("   - å°‡æœç´¢åˆ°çš„ chunk å…§å®¹ç›´æ¥å‚³çµ¦ AI")
    print("   - åˆ©ç”¨è¡Œè™Ÿè³‡è¨Šæä¾›ç²¾ç¢ºå¼•ç”¨")
    
    # é—œé–‰é€£æ¥
    client.close()
    
    print("\n" + "=" * 80)
    print("âœ… æ¸¬è©¦å®Œæˆ")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(test_qa_full_flow())
