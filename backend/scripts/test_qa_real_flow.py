"""
QA çœŸå¯¦æµç¨‹æ¸¬è©¦è…³æœ¬

æ¸¬è©¦å•é¡Œ: å¹«æˆ‘æ‰¾æ‰€æœ‰çš„ç½°å–®
èª¿ç”¨çœŸå¯¦çš„ qa_orchestrator æµç¨‹ï¼Œé¡¯ç¤ºæ¯ä¸€æ­¥çš„å¯¦éš›çµæœ
ç”¨æ–¼è¨ºæ–·ç›®å‰ç³»çµ±çš„å•é¡Œ
"""
import asyncio
import sys
import os
import json
from datetime import datetime
from typing import Optional

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from motor.motor_asyncio import AsyncIOMotorClient
from app.core.config import settings
from app.services.vector.vector_db_service import vector_db_service
from app.services.qa_orchestrator import qa_orchestrator
from app.models.vector_models import AIQARequest


def print_separator(title: str, char: str = "="):
    """æ‰“å°åˆ†éš”ç·š"""
    print(f"\n{char * 80}")
    print(f"ğŸ“Š {title}")
    print(f"{char * 80}")


def print_json(data, indent: int = 2):
    """æ ¼å¼åŒ–æ‰“å° JSON"""
    if hasattr(data, 'model_dump'):
        data = data.model_dump()
    elif hasattr(data, '__dict__'):
        data = data.__dict__
    print(json.dumps(data, ensure_ascii=False, indent=indent, default=str))


async def test_qa_real_flow():
    """æ¸¬è©¦çœŸå¯¦çš„ QA æµç¨‹"""
    
    print("=" * 80)
    print("ğŸ” QA çœŸå¯¦æµç¨‹æ¸¬è©¦ (ä½¿ç”¨ qa_orchestrator)")
    print("=" * 80)
    
    # æ¸¬è©¦å•é¡Œ
    test_query = "å¹«æˆ‘æ‰¾æ‰€æœ‰çš„ç½°å–®"
    print(f"\nğŸ“ æ¸¬è©¦å•é¡Œ: {test_query}")
    print("-" * 80)
    
    # é€£æ¥ MongoDB (ä½¿ç”¨æ­£ç¢ºçš„ UUID è¡¨ç¤ºæ–¹å¼)
    client = AsyncIOMotorClient(
        settings.MONGODB_URL,
        uuidRepresentation='standard'
    )
    db = client[settings.DB_NAME]
    
    # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«é›†åˆ
    vector_db_service.create_collection(768)
    
    # ç²å–æ¸¬è©¦ç”¨æˆ¶
    import uuid as uuid_module
    sample_doc = await db.documents.find_one({})
    if not sample_doc:
        print("âŒ è³‡æ–™åº«ä¸­æ²’æœ‰æ–‡æª”")
        return
    
    owner_id = sample_doc.get("owner_id")
    if isinstance(owner_id, uuid_module.UUID):
        user_id = owner_id
    elif isinstance(owner_id, bytes):
        user_id = uuid_module.UUID(bytes=owner_id)
    else:
        user_id = uuid_module.UUID(str(owner_id))
    
    print(f"ğŸ‘¤ ä½¿ç”¨ç”¨æˆ¶ ID: {user_id}")
    
    # ========== æ§‹å»ºçœŸå¯¦çš„ QA è«‹æ±‚ ==========
    print_separator("æ§‹å»º QA è«‹æ±‚")
    
    qa_request = AIQARequest(
        question=test_query,
        context_limit=5,
        use_semantic_search=True,
        model_preference=None,
        query_rewrite_count=3,
        similarity_threshold=0.3
    )
    
    print(f"ğŸ“‹ è«‹æ±‚åƒæ•¸:")
    print(f"   - question: {qa_request.question}")
    print(f"   - context_limit: {qa_request.context_limit}")
    print(f"   - query_rewrite_count: {qa_request.query_rewrite_count}")
    print(f"   - similarity_threshold: {qa_request.similarity_threshold}")
    
    # ========== èª¿ç”¨çœŸå¯¦çš„ QA æµç¨‹ ==========
    print_separator("èª¿ç”¨ qa_orchestrator.process_qa_request (çœŸå¯¦æµç¨‹)")
    
    print("\nğŸš€ é–‹å§‹åŸ·è¡ŒçœŸå¯¦ QA æµç¨‹...")
    print("ğŸ“Œ é€™æœƒèª¿ç”¨å®Œæ•´çš„: æŸ¥è©¢é‡å¯« â†’ å‘é‡æœç´¢ â†’ ç²å–æ–‡æª” â†’ ç”Ÿæˆç­”æ¡ˆ")
    print("-" * 40)
    
    start_time = datetime.now()
    
    try:
        # èª¿ç”¨çœŸå¯¦çš„ QA æµç¨‹
        response = await qa_orchestrator.process_qa_request(
            db=db,
            request=qa_request,
            user_id=user_id,
            request_id="test_real_flow_001"
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… QA æµç¨‹å®Œæˆï¼Œè€—æ™‚: {duration:.2f} ç§’")
        
        # ========== é¡¯ç¤ºæŸ¥è©¢é‡å¯«çµæœ ==========
        print_separator("Step 1: æŸ¥è©¢é‡å¯«çµæœ")
        
        if response.query_rewrite_result:
            qr = response.query_rewrite_result
            print(f"\nğŸ“‹ åŸå§‹æŸ¥è©¢: {qr.original_query}")
            print(f"\nğŸ“‹ é‡å¯«å¾Œçš„æŸ¥è©¢:")
            for i, q in enumerate(qr.rewritten_queries or [], 1):
                print(f"   {i}. {q}")
            print(f"\nğŸ“‹ æ„åœ–åˆ†æ: {qr.intent_analysis}")
            print(f"ğŸ“‹ æŸ¥è©¢ç²’åº¦: {qr.query_granularity}")
            print(f"ğŸ“‹ å»ºè­°æœç´¢ç­–ç•¥: {qr.search_strategy_suggestion}")
        else:
            print("âš ï¸ ç„¡æŸ¥è©¢é‡å¯«çµæœ")
        
        # ========== é¡¯ç¤ºå‘é‡æœç´¢çµæœ ==========
        print_separator("Step 2: å‘é‡æœç´¢çµæœ (semantic_search_contexts)")
        
        if response.semantic_search_contexts:
            print(f"\nğŸ“„ æœç´¢åˆ° {len(response.semantic_search_contexts)} å€‹çµæœ")
            
            for i, ctx in enumerate(response.semantic_search_contexts, 1):
                print(f"\n--- æœç´¢çµæœ {i} ---")
                print(f"ğŸ“ Document ID: {ctx.document_id}")
                print(f"ğŸ“ˆ ç›¸ä¼¼åº¦: {ctx.similarity_score:.4f}")
                print(f"ğŸ“ summary_or_chunk_text (å‰ 400 å­—):")
                text_preview = ctx.summary_or_chunk_text[:400] + "..." if len(ctx.summary_or_chunk_text) > 400 else ctx.summary_or_chunk_text
                for line in text_preview.split('\n')[:10]:
                    print(f"   {line}")
                if ctx.metadata:
                    print(f"ğŸ·ï¸ Metadata:")
                    for key in ['type', 'vectorization_strategy', 'chunk_summary']:
                        if ctx.metadata.get(key):
                            print(f"   - {key}: {ctx.metadata[key]}")
        else:
            print("âš ï¸ ç„¡å‘é‡æœç´¢çµæœ")
        
        # ========== é¡¯ç¤ºå¯¦éš›æä¾›çµ¦ LLM çš„ä¸Šä¸‹æ–‡ ==========
        print_separator("Step 3: å¯¦éš›æä¾›çµ¦ LLM çš„ä¸Šä¸‹æ–‡ (llm_context_documents)")
        
        if response.llm_context_documents:
            print(f"\nğŸ“„ æä¾›çµ¦ LLM çš„ä¸Šä¸‹æ–‡æ•¸é‡: {len(response.llm_context_documents)}")
            
            for i, ctx in enumerate(response.llm_context_documents, 1):
                print(f"\n--- LLM ä¸Šä¸‹æ–‡ {i} ---")
                print(f"ğŸ“ Document ID: {ctx.document_id}")
                print(f"ğŸ“¦ ä¾†æºé¡å‹: {ctx.source_type}")
                print(f"ğŸ“ content_used (å‰ 400 å­—):")
                text_preview = ctx.content_used[:400] + "..." if len(ctx.content_used) > 400 else ctx.content_used
                for line in text_preview.split('\n')[:10]:
                    print(f"   {line}")
        else:
            print("âš ï¸ ç„¡ LLM ä¸Šä¸‹æ–‡æ–‡æª”")
        
        # ========== é¡¯ç¤º AI ç”Ÿæˆçš„ç­”æ¡ˆ ==========
        print_separator("Step 4: AI ç”Ÿæˆçš„ç­”æ¡ˆ")
        
        print(f"\nğŸ“Š æ¶ˆè€— tokens: {response.tokens_used}")
        print(f"ğŸ“Š ä¿¡å¿ƒåˆ†æ•¸: {response.confidence_score}")
        print(f"ğŸ“Š è™•ç†æ™‚é–“: {response.processing_time:.2f} ç§’")
        print(f"ğŸ“Š ä¾†æºæ–‡æª”æ•¸: {len(response.source_documents)}")
        
        print(f"\nğŸ“ AI ç”Ÿæˆçš„ç­”æ¡ˆ:")
        print("=" * 60)
        print(response.answer)
        print("=" * 60)
        
        # ========== å•é¡Œè¨ºæ–· ==========
        print_separator("å•é¡Œè¨ºæ–·", "!")
        
        print("\nğŸ” å°æ¯”åˆ†æ:")
        print("-" * 40)
        
        # æª¢æŸ¥æœç´¢çµæœå’Œ LLM ä¸Šä¸‹æ–‡çš„å·®ç•°
        if response.semantic_search_contexts and response.llm_context_documents:
            search_content_sample = response.semantic_search_contexts[0].summary_or_chunk_text[:200] if response.semantic_search_contexts else ""
            llm_content_sample = response.llm_context_documents[0].content_used[:200] if response.llm_context_documents else ""
            
            print(f"\nğŸ“Œ å‘é‡æœç´¢è¿”å›çš„å…§å®¹ (å‰ 200 å­—):")
            print(f"   {search_content_sample}...")
            
            print(f"\nğŸ“Œ å¯¦éš›æä¾›çµ¦ LLM çš„å…§å®¹ (å‰ 200 å­—):")
            print(f"   {llm_content_sample}...")
            
            # æª¢æŸ¥æ˜¯å¦ç›¸åŒ
            if search_content_sample != llm_content_sample:
                print(f"\nâš ï¸ å•é¡Œç™¼ç¾: æœç´¢çµæœå’Œ LLM ä¸Šä¸‹æ–‡ä¸ä¸€è‡´ï¼")
                print(f"   - æœç´¢çµæœåŒ…å«å…·é«”çš„ chunk å…§å®¹")
                print(f"   - ä½† LLM æ”¶åˆ°çš„æ˜¯æ–‡æª”ç´šæ‘˜è¦ (content_summary)")
                print(f"   - é€™æ„å‘³è‘—æœç´¢åˆ°çš„ç²¾ç¢ºå…§å®¹è¢«ä¸Ÿæ£„äº†ï¼")
            else:
                print(f"\nâœ… æœç´¢çµæœå’Œ LLM ä¸Šä¸‹æ–‡ä¸€è‡´")
        
        # æª¢æŸ¥ source_type
        if response.llm_context_documents:
            source_types = [ctx.source_type for ctx in response.llm_context_documents]
            print(f"\nğŸ“Œ LLM ä¸Šä¸‹æ–‡ä¾†æºé¡å‹: {source_types}")
            
            if "ai_summary" in source_types:
                print(f"   âš ï¸ ä½¿ç”¨çš„æ˜¯ 'ai_summary' (æ–‡æª”ç´šæ‘˜è¦)")
                print(f"   âš ï¸ è€Œä¸æ˜¯æœç´¢åˆ°çš„å…·é«” chunk å…§å®¹")
        
    except Exception as e:
        print(f"\nâŒ QA æµç¨‹å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    # é—œé–‰é€£æ¥
    client.close()
    
    print("\n" + "=" * 80)
    print("âœ… æ¸¬è©¦å®Œæˆ")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(test_qa_real_flow())
