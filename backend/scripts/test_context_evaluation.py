"""
ä¸Šä¸‹æ–‡è©•ä¼°æ¸¬è©¦è…³æœ¬

å°ˆé–€ç”¨æ–¼è©•ä¼°é›»è…¦ç«¯ QA æµç¨‹ä¸­æä¾›çµ¦ AI çš„ä¸Šä¸‹æ–‡å…§å®¹
é€šé monkey-patching ä¾†æ””æˆªå¯¦éš›å‚³çµ¦ AI çš„ä¸Šä¸‹æ–‡
"""
import asyncio
import sys
import os
import json
from datetime import datetime

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from motor.motor_asyncio import AsyncIOMotorClient
from app.core.config import settings
from app.services.vector.vector_db_service import vector_db_service
from app.models.vector_models import AIQARequest


def print_separator(title: str, char: str = "="):
    """æ‰“å°åˆ†éš”ç·š"""
    print(f"\n{char * 80}")
    print(f"ğŸ“Š {title}")
    print(f"{char * 80}")


# å…¨å±€è®Šé‡ç”¨æ–¼æ•ç²ä¸Šä¸‹æ–‡
captured_contexts = []


async def test_context_evaluation():
    """è©•ä¼°é›»è…¦ç«¯ QA æµç¨‹ä¸­æä¾›çµ¦ AI çš„ä¸Šä¸‹æ–‡"""
    
    print("=" * 80)
    print("ğŸ” é›»è…¦ç«¯ QA ä¸Šä¸‹æ–‡è©•ä¼°")
    print("=" * 80)
    
    # æ¸¬è©¦å•é¡Œ
    test_query = "å¹«æˆ‘æ‰¾æ‰€æœ‰çš„ç½°å–®"
    print(f"\nğŸ“ æ¸¬è©¦å•é¡Œ: {test_query}")
    
    # é€£æ¥ MongoDB
    client = AsyncIOMotorClient(
        settings.MONGODB_URL,
        uuidRepresentation='standard'
    )
    db = client[settings.DB_NAME]
    
    # åˆå§‹åŒ–å‘é‡è³‡æ–™åº«é›†åˆ
    vector_db_service.create_collection(768)
    
    # ç²å–æ¸¬è©¦ç”¨æˆ¶
    import uuid as uuid_module
    sample_doc = await db.documents.find_one({})
    if not sample_doc:
        print("âŒ è³‡æ–™åº«ä¸­æ²’æœ‰æ–‡æª”")
        return
    
    owner_id = sample_doc.get("owner_id")
    if isinstance(owner_id, uuid_module.UUID):
        user_id = str(owner_id)
    elif isinstance(owner_id, bytes):
        user_id = str(uuid_module.UUID(bytes=owner_id))
    else:
        user_id = str(owner_id)
    
    print(f"ğŸ‘¤ ä½¿ç”¨ç”¨æˆ¶ ID: {user_id}")
    
    # Monkey-patch unified_ai_service_simplified.generate_answer ä¾†æ•ç²ä¸Šä¸‹æ–‡
    from app.services.ai import unified_ai_service_simplified as ai_module
    original_generate_answer = ai_module.unified_ai_service_simplified.generate_answer
    
    async def patched_generate_answer(
        user_question,
        intent_analysis,
        document_context,
        db=None,
        **kwargs
    ):
        """æ””æˆªä¸¦è¨˜éŒ„å‚³çµ¦ AI çš„ä¸Šä¸‹æ–‡"""
        captured_contexts.append({
            'user_question': user_question,
            'intent_analysis': intent_analysis,
            'document_context': document_context,
            'kwargs': kwargs
        })
        
        # èª¿ç”¨åŸå§‹æ–¹æ³•
        return await original_generate_answer(
            user_question=user_question,
            intent_analysis=intent_analysis,
            document_context=document_context,
            db=db,
            **kwargs
        )
    
    # æ‡‰ç”¨ patch
    ai_module.unified_ai_service_simplified.generate_answer = patched_generate_answer
    
    try:
        # æ§‹å»º QA è«‹æ±‚
        qa_request = AIQARequest(
            question=test_query,
            context_limit=5,
            use_semantic_search=True,
            model_preference=None,
            query_rewrite_count=3,
            similarity_threshold=0.3,
            workflow_action='approve_search'
        )
        
        print_separator("åŸ·è¡Œæµå¼ QA è™•ç†")
        
        # å°å…¥ä¸¦èª¿ç”¨æµå¼è™•ç†
        from app.services.qa_orchestrator import qa_orchestrator
        
        event_count = 0
        final_answer = ""
        
        async for event in qa_orchestrator.process_qa_request_intelligent_stream(
            db=db,
            request=qa_request,
            user_id=user_id,
            request_id="test_context_eval"
        ):
            event_count += 1
            if event.type == 'progress':
                print(f"   ğŸ“ {event.data.get('stage', '')}: {event.data.get('message', '')}")
            elif event.type == 'chunk':
                final_answer += event.data.get('text', '')
            elif event.type == 'complete':
                if event.data.get('answer'):
                    final_answer = event.data.get('answer')
        
        print(f"\nâœ… è™•ç†å®Œæˆï¼Œå…± {event_count} å€‹äº‹ä»¶")
        
        # åˆ†ææ•ç²çš„ä¸Šä¸‹æ–‡
        print_separator("ä¸Šä¸‹æ–‡è©•ä¼°çµæœ")
        
        if not captured_contexts:
            print("âŒ æœªæ•ç²åˆ°ä»»ä½•ä¸Šä¸‹æ–‡ï¼")
        else:
            for i, ctx in enumerate(captured_contexts, 1):
                print(f"\n{'='*60}")
                print(f"ğŸ“‹ æ•ç²çš„ä¸Šä¸‹æ–‡ #{i}")
                print(f"{'='*60}")
                
                print(f"\nğŸ“ ç”¨æˆ¶å•é¡Œ: {ctx['user_question']}")
                print(f"\nğŸ’­ æ„åœ–åˆ†æ: {ctx['intent_analysis'][:200]}..." if len(ctx['intent_analysis']) > 200 else f"\nğŸ’­ æ„åœ–åˆ†æ: {ctx['intent_analysis']}")
                
                print(f"\nğŸ“„ æ–‡æª”ä¸Šä¸‹æ–‡æ•¸é‡: {len(ctx['document_context'])}")
                
                # è©³ç´°åˆ†ææ¯å€‹ä¸Šä¸‹æ–‡
                for j, doc_ctx in enumerate(ctx['document_context'], 1):
                    print(f"\n--- ä¸Šä¸‹æ–‡ {j} ---")
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ…å«å„ªåŒ–çš„ chunk å…§å®¹ (ç²¾ç°¡æ ¼å¼)
                    if 'å¼•ç”¨ç·¨è™Ÿ: citation:' in doc_ctx and 'æ‘˜è¦:' in doc_ctx and 'å…§å®¹:' in doc_ctx:
                        print(f"âœ… ä½¿ç”¨å„ªåŒ–çš„ç²¾ç°¡ä¸Šä¸‹æ–‡")
                        
                        # æå–é—œéµä¿¡æ¯
                        lines = doc_ctx.split('\n')
                        for line in lines[:15]:  # é¡¯ç¤ºå‰ 15 è¡Œ
                            print(f"   {line}")
                        if len(lines) > 15:
                            print(f"   ... (é‚„æœ‰ {len(lines) - 15} è¡Œ)")
                    
                    elif 'æ‘˜è¦:' in doc_ctx and 'é—œéµæ¦‚å¿µ:' in doc_ctx:
                        print(f"âš ï¸ ä½¿ç”¨èˆŠçš„æ–‡æª”æ‘˜è¦ä¸Šä¸‹æ–‡")
                        print(f"   {doc_ctx[:400]}...")
                    
                    elif 'å°è©±æ­·å²' in doc_ctx:
                        print(f"â„¹ï¸ å°è©±æ­·å²ä¸Šä¸‹æ–‡")
                        print(f"   {doc_ctx[:200]}...")
                    
                    else:
                        print(f"ğŸ“„ å…¶ä»–ä¸Šä¸‹æ–‡é¡å‹")
                        print(f"   {doc_ctx[:400]}...")
        
        # è©•ä¼°ç¸½çµ
        print_separator("è©•ä¼°ç¸½çµ", "!")
        
        if captured_contexts:
            ctx = captured_contexts[-1]  # ä½¿ç”¨æœ€å¾Œä¸€å€‹ï¼ˆç­”æ¡ˆç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼‰
            
            has_optimized_context = any(
                'å¼•ç”¨ç·¨è™Ÿ: citation:' in doc_ctx and 'æ‘˜è¦:' in doc_ctx and 'å…§å®¹:' in doc_ctx
                for doc_ctx in ctx['document_context']
            )
            
            has_old_summary = any(
                'æ‘˜è¦:' in doc_ctx and 'é—œéµæ¦‚å¿µ:' in doc_ctx and 'å‘é‡é¡å‹:' not in doc_ctx
                for doc_ctx in ctx['document_context']
            )
            
            if has_optimized_context:
                print("\nâœ… è©•ä¼°çµæœ: å„ªåŒ–ç”Ÿæ•ˆï¼")
                print("   - ä½¿ç”¨äº†æœç´¢çµæœçš„ chunk å…§å®¹ (æ–¹æ¡ˆ C)")
                print("   - AI å¯ä»¥çœ‹åˆ°å…·é«”çš„é•è¦äº‹å¯¦ã€æ³•æ¢ã€é‡‘é¡ç­‰")
            elif has_old_summary:
                print("\nâš ï¸ è©•ä¼°çµæœ: å„ªåŒ–æœªç”Ÿæ•ˆï¼")
                print("   - ä»åœ¨ä½¿ç”¨èˆŠçš„æ–‡æª”æ‘˜è¦")
                print("   - éœ€è¦æª¢æŸ¥ document_search_handler çš„ä¿®æ”¹")
            else:
                print("\nâ“ è©•ä¼°çµæœ: ç„¡æ³•ç¢ºå®š")
                print("   - ä¸Šä¸‹æ–‡æ ¼å¼ä¸ç¬¦åˆé æœŸ")
        
        # é¡¯ç¤ºæœ€çµ‚ç­”æ¡ˆ
        if final_answer:
            print_separator("AI ç”Ÿæˆçš„ç­”æ¡ˆ")
            print(final_answer[:1500] + "..." if len(final_answer) > 1500 else final_answer)
        
    finally:
        # æ¢å¾©åŸå§‹æ–¹æ³•
        ai_module.unified_ai_service_simplified.generate_answer = original_generate_answer
        client.close()
    
    print("\n" + "=" * 80)
    print("âœ… è©•ä¼°å®Œæˆ")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(test_context_evaluation())
