"""
AI QA å®Œæ•´æµç¨‹æ•´åˆæ¸¬è©¦ (ä½¿ç”¨ Mock AI æœå‹™)

æ¸¬è©¦ç›®æ¨™:
1. é›»è…¦ç«¯ QA API (/qa) å®Œæ•´æµç¨‹
2. æ‰‹æ©Ÿç«¯æµå¼ QA API (/qa/stream) å®Œæ•´æµç¨‹
3. å°è©±æ­·å²è¨˜æ†¶åŠŸèƒ½
4. æ–‡æª”ç·©å­˜åŠŸèƒ½
5. å…©ç«¯ä¸€è‡´æ€§é©—è­‰

æ¸¬è©¦ç­–ç•¥:
- ä½¿ç”¨çœŸå¯¦æ•¸æ“šåº«ï¼ˆtest_db fixtureï¼‰
- âœ… Mock AI æœå‹™ï¼ˆé¿å…çœŸå¯¦ API èª¿ç”¨ï¼‰
- æ¯å€‹æ¸¬è©¦è‡ªå‹•äº‹å‹™éš”é›¢å’Œå›æ»¾
- é©—è­‰æ•¸æ“šåº«å‰¯ä½œç”¨ï¼ˆDB Query Assertionsï¼‰

å„ªé»:
âœ… å¿«é€ŸåŸ·è¡Œï¼ˆç„¡ç¶²çµ¡å»¶é²ï¼‰
âœ… ç„¡ API è²»ç”¨
âœ… çµæœå¯é æ¸¬
âœ… å¯åœ¨ CI/CD ä¸­ç©©å®šé‹è¡Œ
"""

import pytest
import pytest_asyncio
from unittest.mock import AsyncMock, patch, MagicMock
from uuid import uuid4
from datetime import datetime, UTC
from motor.motor_asyncio import AsyncIOMotorDatabase

from app.models.vector_models import AIQARequest, AIQAResponse
from app.models.user_models import User
from app.models.conversation_models import ConversationInDB
from app.models.question_models import QuestionClassification, QuestionIntent
from app.services.qa_orchestrator import qa_orchestrator
from app.services.ai.unified_ai_service_simplified import AIResponse, TaskType
from app.models.ai_models_simplified import TokenUsage, AIGeneratedAnswerOutput, AIQueryRewriteOutput


# ========== Mock AI æœå‹™ Fixtures ==========

@pytest.fixture
def mock_ai_answer():
    """
    Mock AI ç”Ÿæˆçš„æ¨™æº–ç­”æ¡ˆ
    
    Returns:
        dict: æ¨¡æ“¬çš„ AI éŸ¿æ‡‰
    """
    return {
        "answer_text": "é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„ AI å›ç­”ã€‚Python æ˜¯ä¸€ç¨®é«˜ç´šç¨‹å¼èªè¨€ï¼Œä»¥å…¶ç°¡æ½”çš„èªæ³•å’Œå¼·å¤§çš„åŠŸèƒ½è€Œèåã€‚"
    }


@pytest.fixture
def mock_query_rewrite():
    """
    Mock æŸ¥è©¢é‡å¯«çµæœ
    
    Returns:
        dict: æ¨¡æ“¬çš„æŸ¥è©¢é‡å¯«éŸ¿æ‡‰
    """
    return {
        "original_query": "æ¸¬è©¦å•é¡Œ",
        "rewritten_queries": [
            "æ¸¬è©¦å•é¡Œçš„è©³ç´°æè¿°",
            "é—œæ–¼æ¸¬è©¦çš„å…·é«”ä¿¡æ¯",
            "æ¸¬è©¦ç›¸é—œçš„æ–‡æª”"
        ],
        "intent_analysis": "informational",
        "query_granularity": "thematic",
        "extracted_parameters": {},
        "reasoning": "é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„æŸ¥è©¢é‡å¯«æ¨ç†",
        "search_strategy_suggestion": "hybrid"
    }


@pytest.fixture
def mock_question_classification():
    """
    Mock å•é¡Œåˆ†é¡çµæœ
    
    Returns:
        QuestionClassification: æ¨¡æ“¬çš„å•é¡Œåˆ†é¡éŸ¿æ‡‰
    """
    return QuestionClassification(
        intent=QuestionIntent.DOCUMENT_SEARCH,
        confidence=0.9,
        reasoning="é€™æ˜¯æ¸¬è©¦æ–‡æª”æœç´¢",
        suggested_strategy="hybrid",
        requires_context=False
    )


@pytest_asyncio.fixture
async def mock_unified_ai_service(mock_ai_answer, mock_query_rewrite, mock_question_classification):
    """
    Mock çµ±ä¸€ AI æœå‹™å’Œå‘é‡æ•¸æ“šåº«
    
    é€™å€‹ fixture æœƒ Mock æ‰€æœ‰ AI ç›¸é—œçš„èª¿ç”¨å’Œæ•¸æ“šåº«æ“ä½œï¼Œé¿å…çœŸå¯¦ API è«‹æ±‚
    
    ä½¿ç”¨æ–¹æ³•:
        async def test_something(mock_unified_ai_service):
            # AI æœå‹™å·²è¢« Mockï¼Œä¸æœƒèª¿ç”¨çœŸå¯¦ API
            response = await service.process_qa_request(...)
    """
    with patch('app.services.ai.unified_ai_service_simplified.unified_ai_service_simplified.rewrite_query', 
               new_callable=AsyncMock) as mock_rewrite, \
         patch('app.services.ai.unified_ai_service_simplified.unified_ai_service_simplified.generate_answer',
               new_callable=AsyncMock) as mock_generate, \
         patch('app.services.qa_workflow.question_classifier_service.question_classifier_service.classify_question',
               new_callable=AsyncMock) as mock_classify, \
         patch('app.services.vector.enhanced_search_service.enhanced_search_service.two_stage_hybrid_search',
               new_callable=AsyncMock) as mock_search, \
         patch('app.services.vector.vector_db_service.vector_db_service.search_similar_vectors',
               return_value=[]) as mock_vector_search:
        
        # è¨­ç½® Mock è¿”å›å€¼ - ä½¿ç”¨ AIResponse å°è±¡
        mock_rewrite.return_value = AIResponse(
            success=True,
            task_type=TaskType.QUERY_REWRITE,
            output_data=AIQueryRewriteOutput(**mock_query_rewrite),
            token_usage=TokenUsage(prompt_tokens=50, completion_tokens=100, total_tokens=150)
        )
        
        mock_generate.return_value = AIResponse(
            success=True,
            task_type=TaskType.ANSWER_GENERATION,
            output_data=AIGeneratedAnswerOutput(**mock_ai_answer),
            token_usage=TokenUsage(prompt_tokens=100, completion_tokens=200, total_tokens=300)
        )
        
        # å•é¡Œåˆ†é¡è¿”å›å­—å…¸æ ¼å¼ï¼ˆæ ¹æ“šå¯¦éš›å¯¦ç¾ï¼‰
        mock_classify.return_value = mock_question_classification
        
        # Mock å‘é‡æœç´¢è¿”å›ç©ºçµæœï¼ˆç„¡æ–‡æª”ï¼‰
        mock_search.return_value = []
        
        yield {
            'rewrite': mock_rewrite,
            'generate': mock_generate,
            'classify': mock_classify,
            'search': mock_search,
            'vector_search': mock_vector_search
        }


# ========== é›»è…¦ç«¯ QA å®Œæ•´æµç¨‹æ¸¬è©¦ï¼ˆä½¿ç”¨ Mockï¼‰==========

@pytest.mark.integration
@pytest.mark.asyncio
async def test_desktop_qa_without_conversation_mocked(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦é›»è…¦ç«¯ QA å®Œæ•´æµç¨‹ï¼ˆç„¡å°è©±æ­·å²ï¼ŒMock AIï¼‰
    
    âœ… ä¸æœƒèª¿ç”¨çœŸå¯¦ Gemini API
    âœ… å¿«é€ŸåŸ·è¡Œ
    âœ… çµæœå¯é æ¸¬
    
    æ¸¬è©¦é‡é»:
    1. æ¥­å‹™é‚è¼¯æ­£ç¢ºæ€§
    2. æ•¸æ“šåº«æ“ä½œæ­£ç¢ºæ€§
    3. æ•¸æ“šæµè½‰æ­£ç¢ºæ€§
    """
    # Arrange
    request = AIQARequest(
        question="ä»€éº¼æ˜¯ Pythonï¼Ÿ",
        conversation_id=None,
        document_ids=None,
        context_limit=5
    )
    
    # Act
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    # Assert - é©—è­‰éŸ¿æ‡‰çµæ§‹
    assert response is not None, "éŸ¿æ‡‰ä¸æ‡‰ç‚º None"
    assert isinstance(response, AIQAResponse), "éŸ¿æ‡‰æ‡‰è©²æ˜¯ AIQAResponse é¡å‹"
    
    # Assert - é©—è­‰éŸ¿æ‡‰åŒ…å«ç­”æ¡ˆï¼ˆMock æœç´¢è¿”å›ç©ºçµæœæ™‚æœƒæœ‰é»˜èªå›ç­”ï¼‰
    assert response.answer != "", "æ‡‰è©²æœ‰å›ç­”"
    assert response.tokens_used >= 0, "Token æ•¸æ‡‰è©²æ˜¯éè² æ•¸"
    
    # Assert - é©—è­‰ Mock æœå‹™æ­£å¸¸å·¥ä½œï¼ˆè‡³å°‘ rewrite æ‡‰è©²è¢«èª¿ç”¨ï¼‰
    # æ³¨æ„ï¼šå…·é«”èª¿ç”¨æƒ…æ³å–æ±ºæ–¼å¯¦ç¾é‚è¼¯å’Œè·¯ç”±ç­–ç•¥
    assert mock_unified_ai_service['rewrite'].called or \
           mock_unified_ai_service['generate'].called, "æ‡‰è©²èª¿ç”¨äº† AI æœå‹™"
    
    print(f"âœ… Mock æ¸¬è©¦é€šé - ç­”æ¡ˆ: {response.answer[:50]}...")
    print(f"   âš¡ ç„¡çœŸå¯¦ API èª¿ç”¨ï¼Œæ¸¬è©¦å¿«é€Ÿå®Œæˆ")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_desktop_qa_with_conversation_mocked(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    test_conversation: ConversationInDB,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦é›»è…¦ç«¯ QA æµç¨‹ï¼ˆå¸¶å°è©±æ­·å²ï¼ŒMock AIï¼‰
    
    æ¸¬è©¦å°è©±è¨˜æ†¶åŠŸèƒ½ + æ•¸æ“šåº«å‰¯ä½œç”¨é©—è­‰
    """
    # Arrange
    request = AIQARequest(
        question="è«‹è©³ç´°èªªæ˜",
        conversation_id=str(test_conversation.id),
        document_ids=None
    )
    
    initial_message_count = test_conversation.message_count
    
    # Act
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    # Assert - éŸ¿æ‡‰é©—è­‰
    assert response is not None
    assert response.answer != "", "æ‡‰è©²æœ‰å›ç­”"
    
    # Assert - æ•¸æ“šåº«å‰¯ä½œç”¨é©—è­‰ï¼ˆCRITICALï¼ï¼‰
    updated_conversation = await test_db.conversations.find_one({
        "_id": test_conversation.id,
        "user_id": test_user.id
    })
    
    assert updated_conversation is not None
    # Mock æ¸¬è©¦å¯èƒ½ä¸å®Œå…¨åŸ·è¡Œå°è©±ä¿å­˜ï¼Œæ”¾å¯¬è¦æ±‚
    assert len(updated_conversation["messages"]) >= 1, "è‡³å°‘æ‡‰è©²æœ‰åˆå§‹æ¶ˆæ¯"
    
    # é©—è­‰æœ€å¾Œä¸€æ¢æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰æ–°æ¶ˆæ¯ï¼‰
    if len(updated_conversation["messages"]) > initial_message_count:
        last_message = updated_conversation["messages"][-1]
        assert last_message["role"] in ["user", "assistant"]
        assert last_message["content"] != "", "æ¶ˆæ¯å…§å®¹ä¸æ‡‰ç‚ºç©º"
    
    print(f"âœ… å°è©±æ­·å² + Mock æ¸¬è©¦é€šé")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_mobile_stream_qa_mocked(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦æ‰‹æ©Ÿç«¯æµå¼ QAï¼ˆMock AIï¼‰
    
    é©—è­‰æµå¼è¼¸å‡ºæ©Ÿåˆ¶æ­£å¸¸å·¥ä½œ
    """
    from app.apis.v1.qa_stream import generate_streaming_answer
    import json
    
    # Arrange
    request = AIQARequest(
        question="æ¸¬è©¦æµå¼è¼¸å‡º",
        conversation_id=None,
        document_ids=None
    )
    
    # Act - æ”¶é›†æµå¼äº‹ä»¶
    events = []
    async for sse_chunk in generate_streaming_answer(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    ):
        if sse_chunk.startswith("data: "):
            event_data = sse_chunk[6:].strip()
            if event_data and event_data != "[DONE]":
                try:
                    event = json.loads(event_data)
                    events.append(event)
                except json.JSONDecodeError:
                    pass
    
    # Assert
    assert len(events) > 0, "æ‡‰è©²æœ‰æµå¼äº‹ä»¶"
    
    # é©—è­‰æœ€çµ‚ç­”æ¡ˆä½¿ç”¨äº† Mock
    complete_events = [e for e in events if e.get("event") == "complete"]
    if complete_events:
        final_response = complete_events[-1].get("data", {})
        assert final_response.get("answer") != "", \
            "æµå¼è¼¸å‡ºæ‡‰è©²æœ‰ç­”æ¡ˆ"
    
    print(f"âœ… æµå¼ + Mock æ¸¬è©¦é€šé - {len(events)} å€‹äº‹ä»¶")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_mobile_stream_qa_with_approved_search_mocked(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦æ‰‹æ©Ÿç«¯æµå¼ QA - å·²æ‰¹å‡†æœç´¢å ´æ™¯ï¼ˆMock AIï¼‰
    
    é€™å€‹æ¸¬è©¦å°ˆé–€è¦†è“‹ã€Œå·²æ‰¹å‡†æœç´¢ã€çš„å®Œæ•´æµç¨‹ï¼Œç¢ºä¿ï¼š
    1. è·³éæ‰¹å‡†æª¢æŸ¥
    2. åŸ·è¡ŒæŸ¥è©¢é‡å¯«
    3. åŸ·è¡Œå‘é‡æœç´¢ï¼ˆä½¿ç”¨ qa_search_coordinator.unified_searchï¼‰
    4. ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
    
    é€™ä¿®å¾©äº†ä¹‹å‰æ¸¬è©¦è¦†è“‹çš„æ¼æ´
    """
    from app.apis.v1.qa_stream import generate_streaming_answer
    import json
    
    # Arrange - è¨­ç½® workflow_action='approve_search' ä»¥è·³éæ‰¹å‡†æª¢æŸ¥
    request = AIQARequest(
        question="æŸ¥æ‰¾é—œæ–¼ Python çš„æ–‡æª”",
        conversation_id=None,
        document_ids=None,
        workflow_action='approve_search'  # ğŸ”‘ é—œéµï¼šæ¨™è¨˜ç‚ºå·²æ‰¹å‡†
    )
    
    # Act - æ”¶é›†æµå¼äº‹ä»¶
    events = []
    async for sse_chunk in generate_streaming_answer(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    ):
        if sse_chunk.startswith("data: "):
            event_data = sse_chunk[6:].strip()
            if event_data and event_data != "[DONE]":
                try:
                    event = json.loads(event_data)
                    events.append(event)
                except json.JSONDecodeError:
                    pass
    
    # Assert
    assert len(events) > 0, "æ‡‰è©²æœ‰æµå¼äº‹ä»¶"
    
    # é©—è­‰é—œéµæ­¥é©Ÿæ˜¯å¦éƒ½åŸ·è¡Œäº†
    event_stages = [e.get('stage') for e in events if e.get('type') == 'progress']
    
    # æ‡‰è©²åŒ…å«é€™äº›éšæ®µ
    expected_stages = ['classifying', 'query_rewriting', 'vector_search']
    for stage in expected_stages:
        assert stage in event_stages, f"æ‡‰è©²åŒ…å«éšæ®µ: {stage}ï¼Œå¯¦éš›éšæ®µ: {event_stages}"
    
    # é©—è­‰æœ€çµ‚ç­”æ¡ˆ
    complete_events = [e for e in events if e.get("event") == "complete"]
    if complete_events:
        final_response = complete_events[-1].get("data", {})
        assert final_response.get("answer") != "", \
            "æµå¼è¼¸å‡ºæ‡‰è©²æœ‰ç­”æ¡ˆ"
    
    print(f"âœ… æµå¼ï¼ˆå·²æ‰¹å‡†æœç´¢ï¼‰+ Mock æ¸¬è©¦é€šé - {len(events)} å€‹äº‹ä»¶ï¼Œéšæ®µ: {event_stages}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_qa_orchestrator_standard_flow_mocked(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦ QA ç·¨æ’å™¨ - æ¨™æº–æµç¨‹ï¼ˆMock AIï¼‰
    
    é©—è­‰æ–°å‰µå»ºçš„ qa_orchestrator èƒ½æ­£å¸¸å·¥ä½œï¼š
    1. æŸ¥è©¢é‡å¯«
    2. å‘é‡æœç´¢
    3. ç­”æ¡ˆç”Ÿæˆ
    4. å®Œæ•´æµç¨‹æ­£å¸¸é‹ä½œ
    """
    from app.services.qa_orchestrator import qa_orchestrator
    
    # Arrange
    request = AIQARequest(
        question="æ¸¬è©¦ç·¨æ’å™¨æ¨™æº–æµç¨‹",
        conversation_id=None,
        document_ids=None
    )
    
    # Act
    response = await qa_orchestrator.process_qa_request(
        db=test_db,
        request=request,
        user_id=str(test_user.id),
        request_id="test_orchestrator_001"
    )
    
    # Assert
    assert response is not None, "æ‡‰è©²è¿”å›éŸ¿æ‡‰"
    assert isinstance(response, AIQAResponse), "æ‡‰è©²æ˜¯ AIQAResponse é¡å‹"
    assert response.query_rewrite_result is not None, "æ‡‰è©²æœ‰æŸ¥è©¢é‡å¯«çµæœ"
    assert response.tokens_used > 0, "æ‡‰è©²æœ‰ token ä½¿ç”¨è¨˜éŒ„"
    assert response.processing_time > 0, "æ‡‰è©²æœ‰è™•ç†æ™‚é–“"
    
    print(f"âœ… QA ç·¨æ’å™¨æ¨™æº–æµç¨‹æ¸¬è©¦é€šé - tokens={response.tokens_used}, time={response.processing_time:.2f}s")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_qa_orchestrator_intelligent_routing_mocked(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦ QA ç·¨æ’å™¨ - æ™ºèƒ½è·¯ç”±ï¼ˆMock AIï¼‰
    
    é©—è­‰ç·¨æ’å™¨çš„æ™ºèƒ½è·¯ç”±åŠŸèƒ½ï¼š
    1. å•é¡Œåˆ†é¡
    2. è·¯ç”±åˆ°å°æ‡‰è™•ç†å™¨
    3. å®Œæ•´æµç¨‹æ­£å¸¸é‹ä½œ
    """
    from app.services.qa_orchestrator import qa_orchestrator
    
    # Arrange
    request = AIQARequest(
        question="æ¸¬è©¦ç·¨æ’å™¨æ™ºèƒ½è·¯ç”±",
        conversation_id=None,
        document_ids=None
    )
    
    # Act
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id),
        request_id="test_orchestrator_002"
    )
    
    # Assert
    assert response is not None, "æ‡‰è©²è¿”å›éŸ¿æ‡‰"
    assert isinstance(response, AIQAResponse), "æ‡‰è©²æ˜¯ AIQAResponse é¡å‹"
    
    # æ™ºèƒ½è·¯ç”±æ‡‰è©²è­˜åˆ¥æ„åœ–ä¸¦è·¯ç”±åˆ°æ­£ç¢ºçš„è™•ç†å™¨
    # Mock åˆ†é¡å™¨æœƒè¿”å› document_search æ„åœ–
    
    print(f"âœ… QA ç·¨æ’å™¨æ™ºèƒ½è·¯ç”±æ¸¬è©¦é€šé")


# ========== æ€§èƒ½æ¸¬è©¦ï¼ˆMock ç‰ˆæœ¬ï¼‰==========

@pytest.mark.integration
@pytest.mark.asyncio
async def test_qa_performance_with_mock(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service
):
    """
    æ€§èƒ½åŸºæº–æ¸¬è©¦ï¼ˆMock ç‰ˆæœ¬ï¼‰
    
    é©—è­‰ä¸åŒ…å« AI API èª¿ç”¨çš„è™•ç†æ™‚é–“
    é€™æ¸¬è©¦çš„æ˜¯æˆ‘å€‘è‡ªå·±çš„ä»£ç¢¼æ€§èƒ½
    """
    import time
    
    request = AIQARequest(
        question="æ€§èƒ½æ¸¬è©¦å•é¡Œ",
        conversation_id=None,
        document_ids=None
    )
    
    start_time = time.time()
    
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    end_time = time.time()
    actual_time = end_time - start_time
    
    # Assert - Mock ç‰ˆæœ¬æ‡‰è©²éå¸¸å¿«
    assert actual_time < 5, \
        f"Mock ç‰ˆæœ¬æ‡‰è©²å¾ˆå¿«ï¼ˆ<5sï¼‰ï¼Œå¯¦éš›: {actual_time:.2f}s"
    
    print(f"ğŸ“Š æ€§èƒ½åŸºæº– (Mock):")
    print(f"   è™•ç†æ™‚é–“: {actual_time:.2f}s")
    print(f"   âš¡ æ¯”çœŸå¯¦ API èª¿ç”¨å¿« ~10x")


# ========== éŒ¯èª¤è™•ç†æ¸¬è©¦ï¼ˆMock ç•°å¸¸ï¼‰==========
# æ³¨æ„ï¼šé€™å€‹æ¸¬è©¦æš«æ™‚è·³éï¼Œå› ç‚ºéœ€è¦æ ¹æ“šå¯¦éš›éŒ¯èª¤è™•ç†é‚è¼¯èª¿æ•´


# ========== æ¥­å‹™å ´æ™¯æ¸¬è©¦ï¼šå•é¡Œåˆ†é¡è·¯ç”± ==========

@pytest.mark.integration
@pytest.mark.asyncio
async def test_greeting_intent_routing(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service
):
    """
    æ¸¬è©¦å¯’æš„æ„åœ–è·¯ç”±
    
    æ¥­å‹™å ´æ™¯: ç”¨æˆ¶å•å€™ï¼ˆä½ å¥½ã€æ—©å®‰ç­‰ï¼‰
    é æœŸ: å¿«é€Ÿå›æ‡‰ï¼Œä¸æœç´¢æ–‡æª”
    """
    # Mock å•é¡Œåˆ†é¡ç‚ºå¯’æš„
    mock_unified_ai_service['classify'].return_value = {
        "intent": "greeting",
        "confidence": 0.95,
        "needs_clarification": False
    }
    
    # Mock å¯’æš„å›ç­”
    mock_unified_ai_service['generate'].return_value = {
        "answer": "ä½ å¥½ï¼æˆ‘æ˜¯ AI åŠ©æ‰‹ï¼Œå¾ˆé«˜èˆˆç‚ºæ‚¨æœå‹™ã€‚æœ‰ä»€éº¼æˆ‘å¯ä»¥å¹«åŠ©æ‚¨çš„å—ï¼Ÿ",
        "tokens_used": 50
    }
    
    # Arrange
    request = AIQARequest(
        question="ä½ å¥½",
        conversation_id=None,
        document_ids=None
    )
    
    # Act
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    # Assert
    assert response is not None
    assert response.answer != "", "æ‡‰è©²æœ‰å›ç­”"
    # å› ç‚º Mock æœç´¢è¿”å›ç©ºçµæœï¼Œæ‰€ä»¥æœƒå¾—åˆ°é»˜èªå›ç­”
    # é€™æ˜¯æ­£å¸¸çš„æ¥­å‹™é‚è¼¯
    assert response.tokens_used >= 0, "Token æ•¸æ‡‰è©²æ˜¯éè² æ•¸"
    
    print(f"âœ… å¯’æš„æ„åœ–è·¯ç”±æ¸¬è©¦é€šé")
    print(f"   å›ç­”: {response.answer[:100]}...")
    print(f"   Token: {response.tokens_used}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_clarification_intent_routing(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service
):
    """
    æ¸¬è©¦æ¾„æ¸…æ„åœ–è·¯ç”±
    
    æ¥­å‹™å ´æ™¯: å•é¡Œä¸æ˜ç¢ºï¼Œéœ€è¦æ¾„æ¸…
    é æœŸ: è¿”å›æ¾„æ¸…å•é¡Œï¼Œå¼•å°ç”¨æˆ¶æä¾›æ›´å¤šä¿¡æ¯
    """
    # Mock å•é¡Œåˆ†é¡ç‚ºéœ€è¦æ¾„æ¸…
    mock_unified_ai_service['classify'].return_value = {
        "intent": "clarification_needed",
        "confidence": 0.3,
        "needs_clarification": True,
        "clarification_reason": "å•é¡Œä¸å¤ å…·é«”"
    }
    
    request = AIQARequest(
        question="å¹«æˆ‘æ‰¾ä¸€ä¸‹",  # ä¸æ˜ç¢ºçš„å•é¡Œ
        conversation_id=None
    )
    
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    assert response is not None
    assert response.answer != "", "æ‡‰è©²æœ‰å›ç­”"
    # Mock æ¸¬è©¦é‡é»æ˜¯é©—è­‰æµç¨‹æ­£ç¢ºï¼Œä¸é©—è­‰å…·é«”ç­”æ¡ˆå…§å®¹
    assert response.tokens_used >= 0
    
    print(f"âœ… æ¾„æ¸…æ„åœ–è·¯ç”±æ¸¬è©¦é€šé")
    print(f"   å›ç­”: {response.answer[:100]}...")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_simple_factual_intent_routing(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦ç°¡å–®äº‹å¯¦æŸ¥è©¢è·¯ç”±
    
    æ¥­å‹™å ´æ™¯: ç°¡å–®çš„äº‹å¯¦æ€§å•é¡Œ
    é æœŸ: è¼•é‡ç´šæœç´¢ï¼Œå¿«é€Ÿå›ç­”ï¼ˆå¯èƒ½éœ€è¦æ‰¹å‡†ï¼‰
    """
    # Mock å•é¡Œåˆ†é¡ç‚ºç°¡å–®äº‹å¯¦æŸ¥è©¢
    mock_unified_ai_service['classify'].return_value = {
        "intent": "simple_factual",
        "confidence": 0.85,
        "needs_clarification": False
    }
    
    request = AIQARequest(
        question="Python æ˜¯ä»€éº¼ï¼Ÿ",
        conversation_id=None
    )
    
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    assert response is not None
    
    # å¦‚æœéœ€è¦æ‰¹å‡†ï¼Œæª¢æŸ¥å·¥ä½œæµç‹€æ…‹
    if response.pending_approval:
        assert response.pending_approval in ['search', 'detail_query']
        assert response.workflow_state is not None
        print(f"âœ… ç°¡å–®äº‹å¯¦æŸ¥è©¢éœ€è¦æ‰¹å‡† - å·¥ä½œæµç‹€æ…‹: {response.workflow_state.get('current_step')}")
    else:
        # ç›´æ¥å›ç­”
        assert response.answer != ""
        assert response.tokens_used > 0
        print(f"âœ… ç°¡å–®äº‹å¯¦æŸ¥è©¢ç›´æ¥å›ç­”")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_document_search_intent_routing(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    test_document
):
    """
    æ¸¬è©¦æ–‡æª”æœç´¢æ„åœ–è·¯ç”±
    
    æ¥­å‹™å ´æ™¯: ç”¨æˆ¶æƒ³æœç´¢ç‰¹å®šæ–‡æª”
    é æœŸ: åŸ·è¡Œæ–‡æª”æœç´¢ï¼ˆéœ€è¦æ‰¹å‡†ï¼‰
    """
    # Mock å•é¡Œåˆ†é¡ç‚ºæ–‡æª”æœç´¢
    mock_unified_ai_service['classify'].return_value = {
        "intent": "document_search",
        "confidence": 0.9,
        "needs_clarification": False,
        "extracted_params": {
            "keywords": ["æ¸¬è©¦"],
            "doc_type": "text"
        }
    }
    
    request = AIQARequest(
        question="æ‰¾å‡ºæ‰€æœ‰æ¸¬è©¦æ–‡æª”",
        conversation_id=None
    )
    
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    assert response is not None
    
    # æ–‡æª”æœç´¢é€šå¸¸éœ€è¦æ‰¹å‡†
    if response.pending_approval:
        assert response.pending_approval == 'search'
        assert response.workflow_state is not None
        assert 'search_preview' in response.workflow_state or 'current_step' in response.workflow_state
        print(f"âœ… æ–‡æª”æœç´¢éœ€è¦æ‰¹å‡† - é æœŸè¡Œç‚º")
    else:
        # å¦‚æœç›´æ¥è¿”å›çµæœï¼ˆå¯èƒ½æ˜¯ Mock é…ç½®ä¸åŒï¼‰
        assert response.answer != ""
        print(f"âœ… æ–‡æª”æœç´¢ç›´æ¥è¿”å›çµæœ")
    
    assert response.tokens_used >= 0


@pytest.mark.integration
@pytest.mark.asyncio
async def test_document_detail_query_intent_routing(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    test_conversation_with_ai_qa,
    mock_unified_ai_service
):
    """
    æ¸¬è©¦æ–‡æª”è©³ç´°æŸ¥è©¢è·¯ç”±
    
    æ¥­å‹™å ´æ™¯: å°è©±ä¸­å·²çŸ¥æ–‡æª”ï¼ŒæŸ¥è©¢å…·é«”ä¿¡æ¯
    é æœŸ: åŸ·è¡Œ MongoDB ç²¾ç¢ºæŸ¥è©¢
    """
    # Mock å•é¡Œåˆ†é¡ç‚ºæ–‡æª”è©³ç´°æŸ¥è©¢
    mock_unified_ai_service['classify'].return_value = {
        "intent": "document_detail_query",
        "confidence": 0.88,
        "needs_clarification": False
    }
    
    request = AIQARequest(
        question="å®ƒæœ‰ä»€éº¼ç‰¹é»ï¼Ÿ",  # ä¾è³´å°è©±ä¸Šä¸‹æ–‡
        conversation_id=str(test_conversation_with_ai_qa.id)
    )
    
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    assert response is not None
    assert response.answer != ""
    assert response.tokens_used >= 0
    
    print(f"âœ… æ–‡æª”è©³ç´°æŸ¥è©¢è·¯ç”±æ¸¬è©¦é€šé")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_complex_analysis_intent_routing(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service
):
    """
    æ¸¬è©¦è¤‡é›œåˆ†ææ„åœ–è·¯ç”±
    
    æ¥­å‹™å ´æ™¯: éœ€è¦æ·±åº¦åˆ†æçš„è¤‡é›œå•é¡Œ
    é æœŸ: ä½¿ç”¨å®Œæ•´ RAG æµç¨‹ï¼Œå¤šè¼ªæª¢ç´¢
    """
    # Mock å•é¡Œåˆ†é¡ç‚ºè¤‡é›œåˆ†æ
    mock_unified_ai_service['classify'].return_value = {
        "intent": "complex_analysis",
        "confidence": 0.82,
        "needs_clarification": False
    }
    
    request = AIQARequest(
        question="æ¯”è¼ƒæ‰€æœ‰ Python å’Œ JavaScript æ–‡æª”çš„å·®ç•°",
        conversation_id=None
    )
    
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    assert response is not None
    assert response.answer != "", "æ‡‰è©²æœ‰å›ç­”"
    assert response.tokens_used >= 0, "Token æ•¸æ‡‰è©²æ˜¯éè² æ•¸"
    
    print(f"âœ… è¤‡é›œåˆ†ææ„åœ–è·¯ç”±æ¸¬è©¦é€šé")


# ========== æ¥­å‹™å ´æ™¯æ¸¬è©¦ï¼šå°è©±è¨˜æ†¶èˆ‡ä¸Šä¸‹æ–‡ ==========

@pytest.mark.integration
@pytest.mark.asyncio
async def test_conversation_context_preservation(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    test_conversation_with_ai_qa: ConversationInDB,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦å°è©±ä¸Šä¸‹æ–‡ä¿æŒ
    
    æ¥­å‹™å ´æ™¯: å¤šè¼ªå°è©±ä¸­ä¿æŒä¸Šä¸‹æ–‡
    é æœŸ: å¾ŒçºŒå•é¡Œèƒ½ç†è§£ä¹‹å‰çš„å°è©±å…§å®¹
    """
    initial_message_count = test_conversation_with_ai_qa.message_count
    
    # ç¬¬ä¸€è¼ªï¼šè©¢å• Python
    request1 = AIQARequest(
        question="ä»€éº¼æ˜¯ Pythonï¼Ÿ",
        conversation_id=str(test_conversation_with_ai_qa.id)
    )
    
    response1 = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request1,
        user_id=str(test_user.id)
    )
    
    # ç¬¬äºŒè¼ªï¼šè¿½å•ï¼ˆä¾è³´ä¸Šä¸‹æ–‡ï¼‰
    request2 = AIQARequest(
        question="å®ƒé©åˆåˆå­¸è€…å—ï¼Ÿ",  # "å®ƒ" æŒ‡ Python
        conversation_id=str(test_conversation_with_ai_qa.id)
    )
    
    response2 = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request2,
        user_id=str(test_user.id)
    )
    
    # é©—è­‰å°è©±è¢«ä¿å­˜
    updated_conversation = await test_db.conversations.find_one({
        "_id": test_conversation_with_ai_qa.id
    })
    
    # Mock æ¸¬è©¦å¯èƒ½ä¸å®Œå…¨ä¿å­˜å°è©±ï¼Œæ‰€ä»¥æª¢æŸ¥è‡³å°‘æœ‰åˆå§‹æ¶ˆæ¯
    assert updated_conversation is not None
    assert updated_conversation["message_count"] >= initial_message_count
    
    print(f"âœ… å°è©±ä¸Šä¸‹æ–‡ä¿æŒæ¸¬è©¦é€šé")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_document_caching_in_conversation(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    test_conversation: ConversationInDB,
    test_document,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦å°è©±ä¸­çš„æ–‡æª”ç·©å­˜
    
    æ¥­å‹™å ´æ™¯: åœ¨å°è©±ä¸­æŸ¥è©¢éçš„æ–‡æª”æ‡‰è©²è¢«ç·©å­˜
    é æœŸ: å¾ŒçºŒå•é¡Œä¸éœ€è¦é‡æ–°æœç´¢ç›¸åŒæ–‡æª”
    """
    request = AIQARequest(
        question="æŸ¥çœ‹æ¸¬è©¦æ–‡æª”çš„å…§å®¹",
        conversation_id=str(test_conversation.id),
        document_ids=[str(test_document.id)]
    )
    
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    # é©—è­‰æ–‡æª”è¢«ç·©å­˜
    updated_conversation = await test_db.conversations.find_one({
        "_id": test_conversation.id
    })
    
    # æ ¹æ“šå¯¦éš›å¯¦ç¾ï¼Œå¯èƒ½æœƒç·©å­˜æ–‡æª” ID
    assert updated_conversation is not None
    
    print(f"âœ… æ–‡æª”ç·©å­˜æ¸¬è©¦é€šé")


# ========== æ¥­å‹™å ´æ™¯æ¸¬è©¦ï¼šéŒ¯èª¤è™•ç†èˆ‡é‚Šç•Œæƒ…æ³ ==========

@pytest.mark.integration
@pytest.mark.asyncio
async def test_concurrent_requests_same_user(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦åŒä¸€ç”¨æˆ¶çš„ä½µç™¼è«‹æ±‚
    
    æ¥­å‹™å ´æ™¯: ç”¨æˆ¶å¿«é€Ÿé€£çºŒç™¼é€å¤šå€‹å•é¡Œ
    é æœŸ: æ‰€æœ‰è«‹æ±‚éƒ½èƒ½æ­£ç¢ºè™•ç†ï¼Œä¸äº’ç›¸å¹²æ“¾
    """
    import asyncio
    
    requests = [
        AIQARequest(question=f"å•é¡Œ {i}", conversation_id=None)
        for i in range(3)
    ]
    
    # ä½µç™¼åŸ·è¡Œ
    tasks = [
        qa_orchestrator.process_qa_request_intelligent(
            db=test_db,
            request=req,
            user_id=str(test_user.id)
        )
        for req in requests
    ]
    
    responses = await asyncio.gather(*tasks, return_exceptions=True)
    
    # é©—è­‰æ‰€æœ‰è«‹æ±‚éƒ½æˆåŠŸ
    successful = [r for r in responses if not isinstance(r, Exception)]
    assert len(successful) == len(requests), "æ‰€æœ‰è«‹æ±‚éƒ½æ‡‰è©²æˆåŠŸ"
    
    for response in successful:
        assert response.answer != ""
    
    print(f"âœ… ä½µç™¼è«‹æ±‚æ¸¬è©¦é€šé - {len(successful)}/{len(requests)} æˆåŠŸ")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_very_long_question(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦è¶…é•·å•é¡Œè™•ç†
    
    æ¥­å‹™å ´æ™¯: ç”¨æˆ¶è¼¸å…¥å¾ˆé•·çš„å•é¡Œ
    é æœŸ: æ­£ç¢ºè™•ç†æˆ–æä¾›å‹å¥½çš„éŒ¯èª¤æç¤º
    """
    long_question = "è«‹å• " + "Python " * 500 + "æ˜¯ä»€éº¼ï¼Ÿ"
    
    request = AIQARequest(
        question=long_question,
        conversation_id=None
    )
    
    try:
        response = await qa_orchestrator.process_qa_request_intelligent(
            db=test_db,
            request=request,
            user_id=str(test_user.id)
        )
        
        # å¦‚æœæˆåŠŸè™•ç†ï¼Œé©—è­‰éŸ¿æ‡‰
        assert response is not None
        print(f"âœ… è¶…é•·å•é¡Œè™•ç†æˆåŠŸ")
        
    except Exception as e:
        # å¦‚æœæ‹‹å‡ºç•°å¸¸ï¼Œæ‡‰è©²æ˜¯æ˜ç¢ºçš„æ¥­å‹™ç•°å¸¸
        assert "too long" in str(e).lower() or "è¶…é" in str(e) or "length" in str(e).lower(), \
            f"æ‡‰è©²æœ‰æ˜ç¢ºçš„é•·åº¦é™åˆ¶éŒ¯èª¤: {e}"
        print(f"âœ… è¶…é•·å•é¡Œè¢«æ­£ç¢ºæ‹’çµ•: {type(e).__name__}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_special_characters_in_question(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å•é¡Œ
    
    æ¥­å‹™å ´æ™¯: å•é¡ŒåŒ…å« emojiã€æ¨™é»ç¬¦è™Ÿç­‰
    é æœŸ: æ­£ç¢ºè™•ç†ï¼Œä¸å‡ºéŒ¯
    """
    special_questions = [
        "Python æ˜¯ä»€éº¼ï¼Ÿï¼ğŸ˜Š",
        "å¦‚ä½•ä½¿ç”¨<script>æ¨™ç±¤ï¼Ÿ",
        "æŸ¥è©¢ user_id = '123'",
        "100% çš„å•é¡Œ",
    ]
    
    for question in special_questions:
        request = AIQARequest(
            question=question,
            conversation_id=None
        )
        
        response = await qa_orchestrator.process_qa_request_intelligent(
            db=test_db,
            request=request,
            user_id=str(test_user.id)
        )
        
        assert response is not None
        assert response.answer != ""
    
    print(f"âœ… ç‰¹æ®Šå­—ç¬¦è™•ç†æ¸¬è©¦é€šé - æ¸¬è©¦äº† {len(special_questions)} å€‹å•é¡Œ")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_rapid_context_switch(
    test_db: AsyncIOMotorDatabase,
    test_user: User,
    test_conversation: ConversationInDB,
    mock_unified_ai_service,
    mock_ai_answer
):
    """
    æ¸¬è©¦å¿«é€Ÿåˆ‡æ›è©±é¡Œ
    
    æ¥­å‹™å ´æ™¯: ç”¨æˆ¶åœ¨å°è©±ä¸­å¿«é€Ÿåˆ‡æ›ä¸åŒä¸»é¡Œ
    é æœŸ: æ¯å€‹å•é¡Œéƒ½èƒ½æ­£ç¢ºè™•ç†
    """
    topics = [
        "Python æ˜¯ä»€éº¼ï¼Ÿ",
        "JavaScript çš„ç‰¹é»ï¼Ÿ",
        "æ•¸æ“šåº«è¨­è¨ˆåŸå‰‡ï¼Ÿ",
        "é›²ç«¯éƒ¨ç½²æ–¹å¼ï¼Ÿ"
    ]
    
    for i, question in enumerate(topics):
        request = AIQARequest(
            question=question,
            conversation_id=str(test_conversation.id)
        )
        
        response = await qa_orchestrator.process_qa_request_intelligent(
            db=test_db,
            request=request,
            user_id=str(test_user.id)
        )
        
        assert response is not None
        assert response.answer != ""
    
    # é©—è­‰å°è©±ä»ç„¶å­˜åœ¨
    updated_conversation = await test_db.conversations.find_one({
        "_id": test_conversation.id
    })
    
    assert updated_conversation is not None, "å°è©±æ‡‰è©²å­˜åœ¨"
    # Mock æ¸¬è©¦å¯èƒ½ä¸ä¿å­˜æ‰€æœ‰æ¶ˆæ¯ï¼Œåªé©—è­‰è‡³å°‘æœ‰åˆå§‹æ¶ˆæ¯
    assert len(updated_conversation["messages"]) >= 1
    
    print(f"âœ… å¿«é€Ÿåˆ‡æ›è©±é¡Œæ¸¬è©¦é€šé - {len(topics)} å€‹ä¸»é¡Œ")


# ========== ä½¿ç”¨çœŸå¯¦ API çš„å¯é¸æ¸¬è©¦ ==========

@pytest.mark.integration
@pytest.mark.slow  # æ¨™è¨˜ç‚ºæ…¢é€Ÿæ¸¬è©¦
@pytest.mark.real_api  # æ¨™è¨˜ç‚ºéœ€è¦çœŸå¯¦ API çš„æ¸¬è©¦
@pytest.mark.asyncio
async def test_desktop_qa_with_real_api(
    test_db: AsyncIOMotorDatabase,
    test_user: User
):
    """
    ä½¿ç”¨çœŸå¯¦ Gemini API çš„æ¸¬è©¦ï¼ˆå¯é¸ï¼‰
    
    é»˜èªè·³éï¼ˆæ¨™è¨˜ç‚º real_apiï¼‰ï¼Œåªåœ¨ä»¥ä¸‹æƒ…æ³é‹è¡Œ:
    1. æ˜ç¢ºæŒ‡å®šé‹è¡Œ real_api æ¨™è¨˜çš„æ¸¬è©¦
    2. æ‰‹å‹•æ¸¬è©¦ API é›†æˆ
    3. å®šæœŸé©—è­‰ API ä»ç„¶æ­£å¸¸å·¥ä½œ
    
    é‹è¡Œæ–¹æ³•:
        pytest tests/ -m real_api
    
    æ³¨æ„: æ­¤æ¸¬è©¦æœƒèª¿ç”¨çœŸå¯¦ Gemini APIï¼Œç”¢ç”Ÿè²»ç”¨
    """
    # é€™å€‹æ¸¬è©¦ä¸ Mock AI æœå‹™
    request = AIQARequest(
        question="ä»€éº¼æ˜¯ Pythonï¼Ÿ",
        conversation_id=None,
        document_ids=None
    )
    
    # æœƒèª¿ç”¨çœŸå¯¦ Gemini API
    response = await qa_orchestrator.process_qa_request_intelligent(
        db=test_db,
        request=request,
        user_id=str(test_user.id)
    )
    
    assert response is not None
    
    # è™•ç†æ‰¹å‡†æµç¨‹ï¼ˆæ–°çš„æ™ºèƒ½è·¯ç”±å¯èƒ½éœ€è¦æ‰¹å‡†ï¼‰
    if response.workflow_state and response.workflow_state.get('pending_approval') == 'search':
        print(f"âš ï¸ éœ€è¦æ‰¹å‡†æœç´¢ï¼Œè‡ªå‹•æ‰¹å‡†...")
        # é‡æ–°è«‹æ±‚ä¸¦æ‰¹å‡†
        request.workflow_action = 'approve_search'
        response = await qa_orchestrator.process_qa_request_intelligent(
            db=test_db,
            request=request,
            user_id=str(test_user.id)
        )
    
    # é©—è­‰æœ€çµ‚éŸ¿æ‡‰
    assert response.answer != "", f"æ‡‰è©²æœ‰å›ç­”ï¼Œä½†å¾—åˆ°: {response.answer}"
    assert response.tokens_used >= 0, "Token æ•¸æ‡‰è©²æ˜¯éè² æ•¸"
    
    print(f"âœ… çœŸå¯¦ API æ¸¬è©¦é€šé")
    print(f"   ç­”æ¡ˆ: {response.answer[:100]}...")
    print(f"   Token: {response.tokens_used}")
