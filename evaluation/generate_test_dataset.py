import json
import asyncio
import logging
import argparse
import aiohttp
import os
import time
import signal
from typing import List, Dict, Any, Optional
from datetime import datetime
import uuid
from pathlib import Path
import random
from dotenv import load_dotenv
import google.generativeai as genai

# === ç’°å¢ƒé…ç½®è¼‰å…¥ ===
def load_environment_config():
    """è¼‰å…¥ç’°å¢ƒé…ç½®ï¼ŒæŒ‰å„ªå…ˆé †åºæŸ¥æ‰¾ .env æ–‡ä»¶"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    
    print("ğŸ” æª¢æŸ¥é‹è¡Œç’°å¢ƒ...")
    print(f"ğŸ“ è…³æœ¬ç›®éŒ„: {script_dir}")
    print(f"ğŸ“ é …ç›®æ ¹ç›®éŒ„: {project_root}")
    
    # æŒ‰å„ªå…ˆé †åºæŸ¥æ‰¾ .env æ–‡ä»¶
    possible_paths = [
        os.path.join(script_dir, '.env'),           # evaluation/.env
        os.path.join(project_root, '.env'),         # é …ç›®æ ¹ç›®éŒ„/.env
        os.path.join(project_root, 'backend', '.env')  # backend/.env
    ]
    
    print("ğŸ” æŸ¥æ‰¾ .env æ–‡ä»¶...")
    dotenv_path = None
    for i, path in enumerate(possible_paths, 1):
        print(f"   {i}. æª¢æŸ¥: {path}")
        if os.path.exists(path):
            print(f"      âœ… æ‰¾åˆ°æ–‡ä»¶")
            dotenv_path = path
            break
        else:
            print(f"      âŒ æ–‡ä»¶ä¸å­˜åœ¨")
    
    if not dotenv_path:
        print("\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ° .env æ–‡ä»¶")
        print("ğŸ“ è«‹åœ¨ä»¥ä¸‹ä»»ä¸€ä½ç½®å‰µå»º .env æ–‡ä»¶:")
        for path in possible_paths:
            print(f"   - {path}")
        print("\nğŸ’¡ å¯ä»¥åƒè€ƒ evaluation/.env.example å‰µå»ºé…ç½®æ–‡ä»¶")
        exit(1)
    
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    print(f"ğŸ“ æ­£åœ¨è¼‰å…¥: {dotenv_path}")
    result = load_dotenv(dotenv_path=dotenv_path, override=True)
    if result:
        print(f"âœ… æˆåŠŸè¼‰å…¥ç’°å¢ƒé…ç½®æ–‡ä»¶: {dotenv_path}")
    else:
        print(f"âš ï¸  ç’°å¢ƒæ–‡ä»¶è¼‰å…¥å¯èƒ½æœ‰å•é¡Œ: {dotenv_path}")
    
    return dotenv_path

def validate_required_env_vars():
    """é©—è­‰å¿…è¦çš„ç’°å¢ƒè®Šæ•¸"""
    required_vars = {
        'API_URL': 'å¾Œç«¯APIçš„åŸºç¤URL (ä¾‹å¦‚: http://localhost:8000)',
        'USERNAME': 'ç™»å…¥ç”¨çš„ç”¨æˆ¶å',
        'PASSWORD': 'ç™»å…¥ç”¨çš„å¯†ç¢¼',
        'GOOGLE_API_KEY': 'Google Gemini APIé‡‘é‘°'
    }
    
    print("ğŸ” æª¢æŸ¥ç’°å¢ƒè®Šæ•¸...")
    missing_vars = []
    found_vars = []
    
    for var, description in required_vars.items():
        value = os.getenv(var)
        if not value or not value.strip():
            missing_vars.append((var, description))
            print(f"âŒ {var}: æœªè¨­ç½®æˆ–ç‚ºç©º")
        else:
            found_vars.append(var)
            # å°æ–¼æ•æ„Ÿä¿¡æ¯åªé¡¯ç¤ºå‰å¹¾å€‹å­—ç¬¦
            if var in ['PASSWORD', 'GOOGLE_API_KEY']:
                display_value = value[:10] + "..." if len(value) > 10 else value[:5] + "..."
            else:
                display_value = value
            print(f"âœ… {var}: {display_value}")
    
    if missing_vars:
        print("\nâŒ ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸:")
        for var, desc in missing_vars:
            print(f"   - {var}: {desc}")
        
        print("\nğŸ“ .env æ–‡ä»¶é…ç½®ç¤ºä¾‹:")
        print("API_URL=http://localhost:8000")
        print("USERNAME=your_username")
        print("PASSWORD=your_password")
        print("GOOGLE_API_KEY=your_google_api_key")
        print("# å¯é¸é…ç½®")
        print("GENERATION_MODEL=gemini-1.5-flash")
        print("VALIDATION_MODEL=gemini-1.5-flash")
        print("API_RATE_LIMIT=15")
        print("ENABLE_AI_VALIDATION=true")
        print("DETAIL_QUESTIONS_PER_DOC=2")
        
        exit(1)
    
    print(f"âœ… æ‰€æœ‰ {len(found_vars)} å€‹å¿…è¦çš„ç’°å¢ƒè®Šæ•¸éƒ½å·²æ­£ç¢ºè¨­ç½®")

# è¼‰å…¥å’Œé©—è­‰ç’°å¢ƒé…ç½®
load_environment_config()
validate_required_env_vars()

# === æ—¥èªŒé…ç½® ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('generate_test_dataset.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class GoogleAPIRateLimiter:
    """Googleå…è²»APIé€Ÿç‡é™åˆ¶å™¨ - 15æ¬¡/åˆ†é˜"""
    
    def __init__(self, requests_per_minute: int = 15):
        self.requests_per_minute = requests_per_minute
        self.min_interval = 60.0 / requests_per_minute  # 4ç§’é–“éš”
        self.request_times = []
        self.lock = asyncio.Lock()
        
        logger.info(f"ğŸ• APIé€Ÿç‡é™åˆ¶å™¨åˆå§‹åŒ–: {requests_per_minute}æ¬¡/åˆ†é˜ï¼Œæœ€å°é–“éš”: {self.min_interval:.1f}ç§’")
    
    async def wait_if_needed(self):
        """ç­‰å¾…é©ç•¶çš„æ™‚é–“é–“éš”ä»¥ç¬¦åˆé€Ÿç‡é™åˆ¶"""
        async with self.lock:
            now = time.time()
            
            # æ¸…ç†1åˆ†é˜å‰çš„è¨˜éŒ„
            self.request_times = [t for t in self.request_times if now - t < 60]
            
            # å¦‚æœé”åˆ°åˆ†é˜é™åˆ¶ï¼Œç­‰å¾…åˆ°æœ€æ—©è«‹æ±‚çš„1åˆ†é˜å¾Œ
            if len(self.request_times) >= self.requests_per_minute:
                wait_time = 60 - (now - self.request_times[0]) + 0.1  # åŠ 0.1ç§’ç·©è¡
                if wait_time > 0:
                    logger.info(f"â³ é”åˆ°åˆ†é˜é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                    await asyncio.sleep(wait_time)
                    now = time.time()
                    self.request_times = [t for t in self.request_times if now - t < 60]
            
            # ç¢ºä¿æœ€å°é–“éš”
            if self.request_times:
                time_since_last = now - self.request_times[-1]
                if time_since_last < self.min_interval:
                    wait_time = self.min_interval - time_since_last
                    logger.debug(f"â±ï¸  ç­‰å¾…æœ€å°é–“éš”: {wait_time:.1f} ç§’")
                    await asyncio.sleep(wait_time)
                    now = time.time()
            
            # è¨˜éŒ„é€™æ¬¡è«‹æ±‚
            self.request_times.append(now)
            logger.debug(f"ğŸ“Š ç•¶å‰è«‹æ±‚è¨ˆæ•¸: {len(self.request_times)}/åˆ†é˜")

class TestDatasetGenerator:
    """æ¸¬è©¦æ•¸æ“šé›†ç”Ÿæˆå™¨ - ç›´æ¥ä½¿ç”¨Google APIï¼Œå¯¦ç¾1+Nå•é¡Œç”Ÿæˆç­–ç•¥"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨å’ŒAPIé€£æ¥"""
        self.api_base_url = os.getenv('API_URL', 'http://localhost:8000')
        self.api_username = os.getenv('USERNAME')
        self.api_password = os.getenv('PASSWORD')
        
        # Google API é…ç½®
        self.google_api_key = os.getenv('GOOGLE_API_KEY')
        if not self.google_api_key:
            raise ValueError("è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½® GOOGLE_API_KEY")
        
        genai.configure(api_key=self.google_api_key)
        
        # æ¨¡å‹é…ç½® - å¾ç’°å¢ƒè®Šé‡è®€å–
        self.generation_model_name = os.getenv('GENERATION_MODEL', 'gemini-1.5-flash')
        self.validation_model_name = os.getenv('VALIDATION_MODEL', 'gemini-1.5-flash')
        
        print(f"ğŸ¤– ç”Ÿæˆæ¨¡å‹: {self.generation_model_name}")
        print(f"ğŸ§ é©—è­‰æ¨¡å‹: {self.validation_model_name}")
        
        self.generation_model = genai.GenerativeModel(self.generation_model_name)
        self.validation_model = genai.GenerativeModel(self.validation_model_name)
        
        if not self.api_username or not self.api_password:
            raise ValueError("è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½® USERNAME å’Œ PASSWORD")
        
        # ç”¨æ–¼ç·©å­˜å¾APIç²å–çš„æ–‡æª”å¡Š
        self._chunks_cache = {}
        
        self.session = None
        self.access_token = None
        
        # ç”Ÿæˆé…ç½®
        self.concurrent_limit = int(os.getenv('CONCURRENT_LIMIT', '1'))
        self.enable_ai_validation = os.getenv('ENABLE_AI_VALIDATION', 'true').lower() == 'true'
        
        # Google APIé€Ÿç‡é™åˆ¶
        api_rate_limit = int(os.getenv('API_RATE_LIMIT', '15'))  # 15æ¬¡/åˆ†é˜
        self.rate_limiter = GoogleAPIRateLimiter(api_rate_limit)
        
        # å•é¡Œç”Ÿæˆç­–ç•¥é…ç½®
        self.detail_questions_per_doc = int(os.getenv('DETAIL_QUESTIONS_PER_DOC', '2'))  # æ¯å€‹æ–‡æª”ç”Ÿæˆ2å€‹ç´°ç¯€ç´šå•é¡Œ
        
        # å„ªé›…çµ‚æ­¢å’Œä¸­é–“ä¿å­˜é…ç½®
        self.graceful_shutdown = False
        self.intermediate_save_interval = 10  # æ¯è™•ç†10å€‹æ–‡æª”ä¿å­˜ä¸€æ¬¡
        self.current_output_path = None
        self.current_qa_pairs = []
        
        logger.info(f"æ¸¬è©¦æ•¸æ“šé›†ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ¤– ç”Ÿæˆæ¨¡å‹: {self.generation_model_name}")
        logger.info(f"ğŸ§ é©—è­‰æ¨¡å‹: {self.validation_model_name}")
        logger.info(f"APIé€Ÿç‡é™åˆ¶: {api_rate_limit}æ¬¡/åˆ†é˜")
        logger.info(f"AIé©—è­‰: {'å•Ÿç”¨' if self.enable_ai_validation else 'ç¦ç”¨'}")
        logger.info(f"æ¯å€‹æ–‡æª”ç´°ç¯€ç´šå•é¡Œæ•¸é‡: {self.detail_questions_per_doc}")
        logger.info(f"ğŸ’¾ ä¸­é–“ä¿å­˜é–“éš”: æ¯{self.intermediate_save_interval}å€‹æ–‡æª”")
        
        # è¨­ç½®ä¿¡è™Ÿè™•ç†
        self._setup_signal_handlers()

    async def initialize_api_connection(self):
        """åˆå§‹åŒ–APIé€£æ¥å’Œèªè­‰"""
        self.session = aiohttp.ClientSession()
        if not await self._login_and_get_token():
            raise Exception("ç„¡æ³•ç²å–èªè­‰ token")
        logger.info("APIèªè­‰æˆåŠŸ")

    async def _login_and_get_token(self) -> bool:
        """ç™»å…¥ä¸¦ç²å– JWT token"""
        login_url = f"{self.api_base_url}/api/v1/auth/token"
        login_data = {"username": self.api_username, "password": self.api_password}
        
        try:
            async with self.session.post(login_url, data=login_data) as response:
                response.raise_for_status()
                result = await response.json()
                self.access_token = result.get("access_token")
                if self.access_token:
                    logger.info("ç™»å…¥æˆåŠŸï¼Œç²å–åˆ° JWT token")
                    return True
                else:
                    logger.error("ç™»å…¥å¤±æ•—ï¼šéŸ¿æ‡‰ä¸­æ²’æœ‰ access_token")
                    return False
        except Exception as e:
            logger.error(f"ç™»å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def get_auth_headers(self) -> Dict[str, str]:
        """ç²å–èªè­‰æ¨™é ­"""
        if not self.access_token:
            raise ValueError("æœªç²å–åˆ° access_tokenï¼Œè«‹å…ˆç™»å…¥")
        return {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json"
        }
    
    def _clean_json_response(self, response_text: str) -> str:
        """æ¸…ç† Google AI å›æ‡‰ä¸­çš„ markdown ä»£ç¢¼å¡Šæ¨™è¨˜ï¼Œæ”¯æ´å¤šè¡ŒJSON"""
        # ç§»é™¤ ```json å’Œ ``` æ¨™è¨˜
        cleaned = response_text.strip()
        
        # æª¢æŸ¥æ˜¯å¦ä»¥ ```json é–‹é ­
        if cleaned.startswith('```json'):
            cleaned = cleaned[7:]  # ç§»é™¤ ```json
        elif cleaned.startswith('```'):
            cleaned = cleaned[3:]  # ç§»é™¤ ```
        
        # æª¢æŸ¥æ˜¯å¦ä»¥ ``` çµå°¾
        if cleaned.endswith('```'):
            cleaned = cleaned[:-3]  # ç§»é™¤çµå°¾çš„ ```
        
        # è™•ç†å¯èƒ½çš„æ›è¡Œç¬¦å’Œå¤šé¤˜ç©ºæ ¼
        cleaned = cleaned.strip()
        
        # å¦‚æœæ–‡æœ¬è¢«æˆªæ–·ï¼Œå˜—è©¦æ‰¾åˆ°å®Œæ•´çš„JSON
        if not cleaned.endswith('}') and '}' in cleaned:
            # æ‰¾åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´çš„ } 
            last_brace = cleaned.rfind('}')
            if last_brace != -1:
                cleaned = cleaned[:last_brace + 1]
        
        return cleaned.strip()

    async def get_all_user_documents(self) -> List[Dict]:
        """ç²å–ç”¨æˆ¶æ‰€æœ‰æ–‡æª”"""
        headers = self.get_auth_headers()
        url = f"{self.api_base_url}/api/v1/documents/"
        
        all_documents = []
        skip = 0
        limit = 100
        
        while True:
            params = {"limit": limit, "skip": skip}
            
            try:
                async with self.session.get(url, headers=headers, params=params) as response:
                    response.raise_for_status()
                    result = await response.json()
                    documents = result.get("items", [])
                    
                    if not documents:
                        break
                        
                    all_documents.extend(documents)
                    skip += limit
                    
                    logger.debug(f"å·²ç²å– {len(all_documents)} å€‹æ–‡æª”...")
                    
            except Exception as e:
                logger.error(f"ç²å–ç”¨æˆ¶æ–‡æª”å¤±æ•—: {e}")
                break
        
        logger.info(f"ç¸½å…±ç²å–åˆ° {len(all_documents)} å€‹ç”¨æˆ¶æ–‡æª”")
        return all_documents

    async def _get_all_chunks_for_doc(self, doc_id: str) -> List[Dict]:
        """
        é€šéæ–°çš„APIç«¯é»ï¼Œç›´æ¥ç²å–ä¸€å€‹æ–‡æª”çš„æ‰€æœ‰å¡Šï¼ˆæ‘˜è¦å’Œæ–‡æœ¬ï¼‰ã€‚
        å¯¦ç¾äº†æœ¬åœ°ç·©å­˜ä»¥é¿å…é‡è¤‡èª¿ç”¨ã€‚
        """
        if doc_id in self._chunks_cache:
            return self._chunks_cache[doc_id]

        headers = self.get_auth_headers()
        url = f"{self.api_base_url}/api/v1/vector-db/documents/{doc_id}/chunks"
        
        try:
            async with self.session.get(url, headers=headers) as response:
                # æ–°å¢èª¿è©¦æ—¥èªŒ: ç„¡è«–å¦‚ä½•éƒ½æ‰“å°ç‹€æ…‹å’Œå…§å®¹
                logger.debug(f"APIèª¿ç”¨: GET {url} | ç‹€æ…‹ç¢¼: {response.status}")
                
                if response.status == 200:
                    chunks = await response.json()
                    # æé«˜æ—¥èªŒç´šåˆ¥ä»¥ä¾¿åœ¨çµ‚ç«¯ä¸­ç¸½æ˜¯å¯è¦‹
                    logger.info(f"ç‚ºæ–‡æª” {doc_id} å¾APIæ”¶åˆ° {len(chunks)} å€‹å¡Š")
                    self._chunks_cache[doc_id] = chunks
                    return chunks
                else:
                    # å¦‚æœç‹€æ…‹ç¢¼ä¸ç‚º200ï¼Œè¨˜éŒ„éŒ¯èª¤ä¸¦è¿”å›ç©ºåˆ—è¡¨
                    error_text = await response.text()
                    logger.error(f"ç‚ºæ–‡æª” {doc_id} ç²å–å¡Šæ™‚APIè¿”å›éŒ¯èª¤ç‹€æ…‹ {response.status}: {error_text}")
                    self._chunks_cache[doc_id] = []
                    return []

        except aiohttp.ClientError as e:
            logger.error(f"ç‚ºæ–‡æª” {doc_id} ç²å–æ‰€æœ‰å¡Šæ™‚ç™¼ç”Ÿç¶²çµ¡éŒ¯èª¤: {e}")
            self._chunks_cache[doc_id] = []
            return []

    async def get_document_summary(self, doc_id: str, doc_title: str) -> Optional[str]:
        """å¾æ‰€æœ‰å¡Šä¸­éæ¿¾å‡ºæ–‡æª”æ‘˜è¦"""
        all_chunks = await self._get_all_chunks_for_doc(doc_id)
        if not all_chunks:
            return None
        
        # ä¿®æ­£ï¼šä½¿ç”¨ 'type' == 'summary' ä¾†è­˜åˆ¥æ‘˜è¦å¡Š
        for chunk in all_chunks:
            if chunk.get('metadata', {}).get('type') == 'summary':
                # èª¿è©¦ï¼šæ‰“å°æ‘˜è¦å¡Šçš„å®Œæ•´çµæ§‹
                logger.info(f"æ‰¾åˆ°æ‘˜è¦å¡Šï¼å®Œæ•´çµæ§‹: {chunk}")
                
                # ä¿®æ­£ï¼šæ‘˜è¦å…§å®¹ç›´æ¥å­˜å„²åœ¨ summary_text å­—æ®µä¸­
                summary_content = chunk.get('summary_text')
                if not summary_content:
                    # å¾Œå‚™æ–¹æ¡ˆï¼šå˜—è©¦å¾ payload.page_content ç²å–
                    summary_content = chunk.get('payload', {}).get('page_content')
                
                if summary_content:
                    logger.info(f"æˆåŠŸæå–æ‘˜è¦å…§å®¹ï¼Œé•·åº¦: {len(summary_content)}")
                    return summary_content
                else:
                    logger.warning(f"æ‘˜è¦å¡Šå­˜åœ¨ä½†å…§å®¹ç‚ºç©ºã€‚summary_text: {chunk.get('summary_text')}, payload: {chunk.get('payload')}")
        
        logger.warning(f"åœ¨æ–‡æª” {doc_id} çš„ {len(all_chunks)} å€‹å¡Šä¸­æ‰¾ä¸åˆ°æ‘˜è¦å¡Š")
        # æ–°å¢èª¿è©¦æ—¥èªŒ: æ‰“å°æ‰€æœ‰å¡Šçš„å…ƒæ•¸æ“šä»¥ä¾›åˆ†æ
        if all_chunks:
            logger.debug(f"æ–‡æª” {doc_id} çš„å¡Šå…ƒæ•¸æ“šå¦‚ä¸‹:")
            for i, chunk in enumerate(all_chunks):
                logger.debug(f"  å¡Š {i+1}: metadata={chunk.get('metadata')}")
        return None

    async def get_document_chunks(self, doc_id: str, doc_title: str) -> List[Dict]:
        """å¾æ‰€æœ‰å¡Šä¸­éæ¿¾å‡ºæ–‡æœ¬å…§å®¹å¡Š"""
        all_chunks = await self._get_all_chunks_for_doc(doc_id)
        if not all_chunks:
            return []

        # ä¿®æ­£ï¼šä½¿ç”¨ 'type' == 'chunk' ä¾†è­˜åˆ¥æ–‡æœ¬å¡Š
        text_chunks = []
        for chunk in all_chunks:
            if chunk.get('metadata', {}).get('type') == 'chunk':
                # ä¿®æ­£ï¼šè™•ç†æ–‡æœ¬å¡Šçš„å…§å®¹æå–
                chunk_content = chunk.get('summary_text')
                if not chunk_content:
                    # å¾Œå‚™æ–¹æ¡ˆï¼šå˜—è©¦å¾ payload.page_content ç²å–
                    chunk_content = chunk.get('payload', {}).get('page_content')
                
                if chunk_content and len(chunk_content.strip()) >= 20:  # éæ¿¾å¤ªçŸ­çš„å¡Š
                    # æ§‹é€ çµ±ä¸€çš„æ•¸æ“šçµæ§‹
                    formatted_chunk = {
                        'chunk_id': chunk.get('id', str(uuid.uuid4())),
                        'content': chunk_content,
                        'document_id': doc_id,
                        'document_title': doc_title,
                        'metadata': chunk.get('metadata', {}),
                        'similarity_score': chunk.get('similarity_score', 1.0)
                    }
                    text_chunks.append(formatted_chunk)
        
        logger.info(f"ç‚ºæ–‡æª” {doc_id} ç²å¾— {len(text_chunks)} å€‹æœ‰æ•ˆæ–‡æœ¬å¡Š")
        return text_chunks

    async def generate_summary_question_via_google(self, doc_summary: str, doc_title: str, doc_id: str) -> Optional[Dict]:
        """ç›´æ¥é€šéGoogle APIç”Ÿæˆä¸»é¡Œç´šå•é¡Œ"""
        if not doc_summary or not doc_summary.strip():
            return None
            
        # ç­‰å¾…é€Ÿç‡é™åˆ¶
        await self.rate_limiter.wait_if_needed()
        
        prompt = f"""è«‹åŸºæ–¼ä»¥ä¸‹æ–‡æª”çš„æ‘˜è¦è³‡è¨Šï¼Œç”Ÿæˆä¸€å€‹ä¸»é¡Œç´šå•é¡Œã€‚

æ–‡æª”æ¨™é¡Œï¼š{doc_title}
æ–‡æª”æ‘˜è¦ï¼š{doc_summary}

è¦æ±‚ï¼š
1. å•é¡Œæ‡‰è©²æ¸¬è©¦å°æ–‡æª”æ•´é«”æ ¸å¿ƒä¸»æ—¨å’Œä¸»è¦å…§å®¹çš„ç†è§£
2. å•é¡Œè¦è‡ªç„¶ã€å…·é«”ï¼Œé©åˆç”¨æ–¼æª¢ç´¢ç³»çµ±è©•ä¼°
3. ç­”æ¡ˆæ‡‰è©²èƒ½å¾æ–‡æª”çš„æ‘˜è¦å…§å®¹ä¸­æ‰¾åˆ°
4. å•é¡Œé¡å‹æ‡‰è©²æ˜¯"ä¸»é¡Œç´š"å•é¡Œ

è«‹åªå›ç­”ä¸€å€‹JSONç‰©ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{"question": "å•é¡Œå…§å®¹", "answer": "åŸºæ–¼æ‘˜è¦çš„ç­”æ¡ˆ", "question_type": "ä¸»é¡Œç´š", "confidence": 0.9}}

åªå›ç­”JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

        try:
            response = self.generation_model.generate_content(prompt)
            answer_text = response.text.strip()
            
            # æ¸…ç†å›æ‡‰æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„ markdown ä»£ç¢¼å¡Šæ¨™è¨˜
            cleaned_text = self._clean_json_response(answer_text)
            
            # å˜—è©¦è§£æJSON
            try:
                qa_data = json.loads(cleaned_text)
                
                if not qa_data.get('question') or not qa_data.get('answer'):
                    logger.warning(f"Google APIå›æ‡‰ç¼ºå°‘å•é¡Œæˆ–ç­”æ¡ˆ: {qa_data}")
                    return None
                
                logger.debug(f"æˆåŠŸç”Ÿæˆä¸»é¡Œç´šå•é¡Œ: {qa_data['question'][:50]}...")
                
                return {
                    'question': qa_data['question'],
                    'ground_truth': qa_data['answer'],
                    'expected_relevant_doc_ids': [doc_id],
                    'question_type': 'ä¸»é¡Œç´š',
                    'confidence': qa_data.get('confidence', 0.9),
                    'document_id': doc_id,
                    'document_title': doc_title,
                    'generated_at': datetime.now().isoformat(),
                    'generation_method': 'google_api_summary',
                    'generation_model': self.generation_model_name
                }
                
            except json.JSONDecodeError as e:
                logger.error(f"Google APIå›æ‡‰JSONè§£æéŒ¯èª¤: {e}, æ¸…ç†å¾Œæ–‡æœ¬: {cleaned_text[:200]}...")
                return None
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¸»é¡Œç´šå•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None

    async def generate_detail_question_via_google(self, chunk: Dict) -> Optional[Dict]:
        """ç›´æ¥é€šéGoogle APIç”Ÿæˆç´°ç¯€ç´šå•é¡Œ"""
        content = chunk.get('content', '').strip()
        if not content:
            return None
        
        # ç­‰å¾…é€Ÿç‡é™åˆ¶
        await self.rate_limiter.wait_if_needed()
        
        prompt = f"""è«‹åŸºæ–¼ä»¥ä¸‹æ–‡æª”ç‰‡æ®µï¼Œç”Ÿæˆä¸€å€‹ç´°ç¯€ç´šå•é¡Œã€‚

æ–‡æª”ç‰‡æ®µå…§å®¹ï¼š{content}

è¦æ±‚ï¼š
1. å•é¡Œæ‡‰è©²æ¸¬è©¦å°ç‰‡æ®µä¸­å…·é«”äº‹å¯¦ã€ç´°ç¯€çš„ç²¾ç¢ºç†è§£
2. å•é¡Œè¦é‡å°ç‰‡æ®µä¸­çš„å…·é«”ä¿¡æ¯ï¼ˆå¦‚æ•¸å­—ã€åç¨±ã€æ—¥æœŸã€å®šç¾©ç­‰ï¼‰
3. ç­”æ¡ˆå¿…é ˆåªèƒ½å¾é€™å€‹ç‰¹å®šç‰‡æ®µä¸­æ‰¾åˆ°
4. å•é¡Œè¦è‡ªç„¶ã€å…·é«”ï¼Œé©åˆç”¨æ–¼æª¢ç´¢ç³»çµ±è©•ä¼°
5. å•é¡Œé¡å‹æ‡‰è©²æ˜¯"ç´°ç¯€ç´š"å•é¡Œ

è«‹åªå›ç­”ä¸€å€‹JSONç‰©ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{"question": "å•é¡Œå…§å®¹", "answer": "åŸºæ–¼ç‰‡æ®µçš„å…·é«”ç­”æ¡ˆ", "question_type": "ç´°ç¯€ç´š", "confidence": 0.8}}

åªå›ç­”JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

        try:
            response = self.generation_model.generate_content(prompt)
            answer_text = response.text.strip()
            
            # æ¸…ç†å›æ‡‰æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„ markdown ä»£ç¢¼å¡Šæ¨™è¨˜
            cleaned_text = self._clean_json_response(answer_text)
            
            # å˜—è©¦è§£æJSON
            try:
                qa_data = json.loads(cleaned_text)
                
                if not qa_data.get('question') or not qa_data.get('answer'):
                    logger.warning(f"Google APIå›æ‡‰ç¼ºå°‘å•é¡Œæˆ–ç­”æ¡ˆ: {qa_data}")
                    return None
                
                logger.debug(f"æˆåŠŸç”Ÿæˆç´°ç¯€ç´šå•é¡Œ: {qa_data['question'][:50]}...")
                
                return {
                    'question': qa_data['question'],
                    'ground_truth': qa_data['answer'],
                    'expected_relevant_doc_ids': [chunk.get('document_id', '')],
                    'question_type': 'ç´°ç¯€ç´š',
                    'confidence': qa_data.get('confidence', 0.8),
                    'source_chunk_id': chunk.get('chunk_id', ''),
                    'document_id': chunk.get('document_id', ''),
                    'document_title': chunk.get('document_title', ''),
                    'generated_at': datetime.now().isoformat(),
                    'generation_method': 'google_api_chunk',
                    'generation_model': self.generation_model_name
                }
                
            except json.JSONDecodeError as e:
                logger.error(f"Google APIå›æ‡‰JSONè§£æéŒ¯èª¤: {e}, æ¸…ç†å¾Œæ–‡æœ¬: {cleaned_text[:200]}...")
                return None
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆç´°ç¯€ç´šå•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return None

    async def validate_qa_pair_via_google(self, qa_pair: Dict) -> bool:
        """é€šéGoogle APIé©—è­‰å•ç­”å°è³ªé‡"""
        if not self.enable_ai_validation:
            return True
            
        # ç­‰å¾…é€Ÿç‡é™åˆ¶
        await self.rate_limiter.wait_if_needed()
        
        question = qa_pair.get('question', '')
        answer = qa_pair.get('ground_truth', '')
        
        validation_prompt = f"""
è«‹è©•ä¼°ä»¥ä¸‹å•ç­”å°çš„è³ªé‡ï¼š

å•é¡Œï¼š{question}
ç­”æ¡ˆï¼š{answer}

è©•ä¼°æ¨™æº–ï¼š
1. å•é¡Œæ˜¯å¦æ¸…æ™°ã€å…·é«”ã€æœ‰æ„ç¾©ï¼Ÿ
2. ç­”æ¡ˆæ˜¯å¦æº–ç¢ºã€å®Œæ•´ã€èˆ‡å•é¡Œç›¸é—œï¼Ÿ
3. å•é¡Œæ˜¯å¦é©åˆç”¨æ–¼æª¢ç´¢ç³»çµ±è©•ä¼°ï¼Ÿ
4. æ•´é«”è³ªé‡æ˜¯å¦é”åˆ°æ¸¬è©¦æ•¸æ“šæ¨™æº–ï¼Ÿ

è«‹ä»¥ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼š
{{
    "is_valid": true/false,
    "quality_score": 0.0-1.0,
    "issues": ["ç™¼ç¾çš„å•é¡Œåˆ—è¡¨"],
    "recommendation": "æ”¹é€²å»ºè­°"
}}
"""
        
        try:
            response = self.validation_model.generate_content(validation_prompt)
            answer_text = response.text.strip()
            
            # æ¸…ç†å›æ‡‰æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„ markdown ä»£ç¢¼å¡Šæ¨™è¨˜
            cleaned_text = self._clean_json_response(answer_text)
            
            try:
                validation_data = json.loads(cleaned_text)
                is_valid = validation_data.get('is_valid', False)
                quality_score = validation_data.get('quality_score', 0.0)
                
                # è¨­å®šè³ªé‡é–¾å€¼ - å¯èª¿æ•´æ¨™æº– (0.5=å¯¬é¬†, 0.6=æ¨™æº–, 0.7=åš´æ ¼)
                return is_valid and quality_score >= 0.6
                
            except json.JSONDecodeError:
                logger.warning(f"é©—è­‰å›æ‡‰æ ¼å¼éŒ¯èª¤: {answer_text[:100]}...")
                logger.debug(f"æ¸…ç†å¾Œæ–‡æœ¬: {cleaned_text[:100]}...")
                return False
                
        except Exception as e:
            logger.warning(f"é©—è­‰å•ç­”å°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return True  # å¦‚æœé©—è­‰å¤±æ•—ï¼Œé»˜èªæ¥å—

    def estimate_generation_time(self, num_documents: int) -> Dict[str, float]:
        """ä¼°ç®—ç”Ÿæˆæ™‚é–“"""
        # æ¯å€‹æ–‡æª”ï¼š1å€‹ä¸»é¡Œç´šå•é¡Œ + Nå€‹ç´°ç¯€ç´šå•é¡Œ
        questions_per_doc = 1 + self.detail_questions_per_doc
        total_questions = num_documents * questions_per_doc
        
        # å¦‚æœå•Ÿç”¨é©—è­‰ï¼Œæ¯å€‹å•é¡Œéœ€è¦é¡å¤–çš„é©—è­‰èª¿ç”¨
        api_calls_per_question = 2 if self.enable_ai_validation else 1
        total_api_calls = total_questions * api_calls_per_question
        
        # åŸºæ–¼15æ¬¡/åˆ†é˜çš„é™åˆ¶è¨ˆç®—
        estimated_minutes = total_api_calls / self.rate_limiter.requests_per_minute
        estimated_seconds = estimated_minutes * 60
        
        return {
            'num_documents': num_documents,
            'questions_per_doc': questions_per_doc,
            'total_questions': total_questions,
            'api_calls_per_question': api_calls_per_question,
            'total_api_calls': total_api_calls,
            'estimated_minutes': estimated_minutes,
            'estimated_seconds': estimated_seconds
        }

    async def generate_test_dataset(self, target_document_ratio: float = 0.5, output_path: str = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸¬è©¦æ•¸æ“šé›†ä¸»æµç¨‹ - 1+Nå•é¡Œç”Ÿæˆç­–ç•¥ï¼Œæ”¯æ´å„ªé›…çµ‚æ­¢"""
        logger.info(f"é–‹å§‹ç”Ÿæˆæ¸¬è©¦æ•¸æ“šé›†ï¼Œæ–‡æª”é¸æ“‡æ¯”ä¾‹: {target_document_ratio}")
        
        # è¨­ç½®è¼¸å‡ºè·¯å¾‘å’Œåˆå§‹åŒ–ä¸­é–“ä¿å­˜
        self.current_output_path = output_path
        self.current_qa_pairs = []
        
        # æ­¥é©Ÿ1: ç²å–æ‰€æœ‰æ–‡æª”
        logger.info("æ­¥é©Ÿ1: ç²å–ç”¨æˆ¶æ‰€æœ‰æ–‡æª”...")
        all_documents = await self.get_all_user_documents()
        
        if not all_documents:
            raise Exception("ç„¡æ³•ç²å–æ–‡æª”ï¼Œè«‹æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦æœ‰æ–‡æª”æ•¸æ“š")
        
        # æ­¥é©Ÿ2: éš¨æ©Ÿé¸æ“‡ä¸€åŠæ–‡æª”
        num_selected = max(1, int(len(all_documents) * target_document_ratio))
        selected_documents = random.sample(all_documents, num_selected)
        
        logger.info(f"æ­¥é©Ÿ2: å¾ {len(all_documents)} å€‹æ–‡æª”ä¸­éš¨æ©Ÿé¸æ“‡ {num_selected} å€‹æ–‡æª”")
        
        # ä¼°ç®—æ™‚é–“
        time_estimate = self.estimate_generation_time(num_selected)
        logger.info(f"â±ï¸  é ä¼°ç”Ÿæˆæ™‚é–“: {time_estimate['estimated_minutes']:.1f} åˆ†é˜")
        logger.info(f"ğŸ“ é ä¼°APIèª¿ç”¨æ¬¡æ•¸: {time_estimate['total_api_calls']} æ¬¡")
        logger.info(f"ğŸ“‹ æ¯å€‹æ–‡æª”å°‡ç”Ÿæˆ: 1å€‹ä¸»é¡Œç´šå•é¡Œ + {self.detail_questions_per_doc}å€‹ç´°ç¯€ç´šå•é¡Œ")
        if self.enable_ai_validation:
            logger.info(f"ğŸ” AIé©—è­‰å·²å•Ÿç”¨ï¼Œæ¯å€‹å•é¡Œå°‡é€²è¡Œè³ªé‡é©—è­‰")
        logger.info(f"ğŸ’¾ æ”¯æ´ Ctrl+C å„ªé›…çµ‚æ­¢ï¼Œæœƒè‡ªå‹•ä¿å­˜å·²ç”Ÿæˆçš„å•ç­”å°")
        
        # æ­¥é©Ÿ3: ç‚ºæ¯å€‹æ–‡æª”ç”Ÿæˆ1+Nå•é¡Œ
        logger.info("æ­¥é©Ÿ3: é–‹å§‹ç”Ÿæˆå•ç­”å°...")
        
        all_qa_pairs = []
        start_time = time.time()
        
        summary_question_count = 0
        detail_question_count = 0
        validated_count = 0
        
        for i, doc in enumerate(selected_documents):
            # æª¢æŸ¥æ˜¯å¦éœ€è¦å„ªé›…çµ‚æ­¢
            if self.graceful_shutdown:
                logger.info(f"æ”¶åˆ°çµ‚æ­¢ä¿¡è™Ÿï¼Œåœæ­¢è™•ç†ã€‚å·²è™•ç† {i}/{num_selected} å€‹æ–‡æª”")
                break
            doc_id = doc.get('id', '')
            doc_title = doc.get('title') or doc.get('filename', 'Unknown')
            
            if not doc_id:
                logger.warning(f"æ–‡æª” {doc_title} æ²’æœ‰IDï¼Œè·³é")
                continue
            
            logger.info(f"ğŸ“„ è™•ç†æ–‡æª” {i+1}/{num_selected}: {doc_title}")
            
            # ç”Ÿæˆä¸»é¡Œç´šå•é¡Œï¼ˆåŸºæ–¼æ–‡æª”æ‘˜è¦ï¼‰
            logger.info(f"   ğŸ¯ ç”Ÿæˆä¸»é¡Œç´šå•é¡Œ...")
            doc_summary = await self.get_document_summary(doc_id, doc_title)
            
            if doc_summary:
                summary_qa = await self.generate_summary_question_via_google(doc_summary, doc_title, doc_id)
                if summary_qa:
                    # é©—è­‰å•ç­”å°
                    if await self.validate_qa_pair_via_google(summary_qa):
                        all_qa_pairs.append(summary_qa)
                        self.current_qa_pairs = all_qa_pairs.copy()  # æ›´æ–°ä¸­é–“ä¿å­˜å‰¯æœ¬
                        summary_question_count += 1
                        validated_count += 1
                        logger.info(f"   âœ… ä¸»é¡Œç´šå•é¡Œç”Ÿæˆä¸¦é©—è­‰æˆåŠŸ")
                    else:
                        logger.warning(f"   âŒ ä¸»é¡Œç´šå•é¡Œæœªé€šéé©—è­‰")
                else:
                    logger.warning(f"   âŒ ä¸»é¡Œç´šå•é¡Œç”Ÿæˆå¤±æ•—")
            else:
                logger.warning(f"   âš ï¸  ç„¡æ³•ç²å–æ–‡æª”æ‘˜è¦ï¼Œè·³éä¸»é¡Œç´šå•é¡Œ")
            
            # ç”Ÿæˆç´°ç¯€ç´šå•é¡Œï¼ˆåŸºæ–¼æ–‡æª”chunksï¼‰
            logger.info(f"   ğŸ” ç”Ÿæˆç´°ç¯€ç´šå•é¡Œ...")
            doc_chunks = await self.get_document_chunks(doc_id, doc_title)
            
            if doc_chunks:
                # é¸æ“‡æœ€å¥½çš„Nå€‹chunks
                selected_chunks = doc_chunks[:self.detail_questions_per_doc]
                
                for j, chunk in enumerate(selected_chunks):
                    detail_qa = await self.generate_detail_question_via_google(chunk)
                    if detail_qa:
                        # é©—è­‰å•ç­”å°
                        if await self.validate_qa_pair_via_google(detail_qa):
                            all_qa_pairs.append(detail_qa)
                            self.current_qa_pairs = all_qa_pairs.copy()  # æ›´æ–°ä¸­é–“ä¿å­˜å‰¯æœ¬
                            detail_question_count += 1
                            validated_count += 1
                            logger.info(f"   âœ… ç´°ç¯€ç´šå•é¡Œ {j+1} ç”Ÿæˆä¸¦é©—è­‰æˆåŠŸ")
                        else:
                            logger.warning(f"   âŒ ç´°ç¯€ç´šå•é¡Œ {j+1} æœªé€šéé©—è­‰")
                    else:
                        logger.warning(f"   âŒ ç´°ç¯€ç´šå•é¡Œ {j+1} ç”Ÿæˆå¤±æ•—")
            else:
                logger.warning(f"   âš ï¸  ç„¡æ³•ç²å–æ–‡æª”chunksï¼Œè·³éç´°ç¯€ç´šå•é¡Œ")
            
            # ä¸­é–“ä¿å­˜æª¢æŸ¥
            if output_path and (i + 1) % self.intermediate_save_interval == 0:
                self._save_intermediate_results()
                logger.info(f"ğŸ’¾ å·²è‡ªå‹•ä¿å­˜é€²åº¦ ({i+1}/{num_selected} æ–‡æª”)")
            
            # é¡¯ç¤ºé€²åº¦
            elapsed = time.time() - start_time
            if i > 0:
                avg_time_per_doc = elapsed / (i + 1)
                remaining_docs = num_selected - (i + 1)
                estimated_remaining = avg_time_per_doc * remaining_docs
                logger.info(f"ğŸ“Š é€²åº¦: {i+1}/{num_selected} æ–‡æª”è™•ç†å®Œæˆï¼Œé ä¼°å‰©é¤˜æ™‚é–“: {estimated_remaining/60:.1f}åˆ†é˜")
        
        # ç”Ÿæˆçµ±è¨ˆä¿¡æ¯
        total_time = time.time() - start_time
        total_generated = summary_question_count + detail_question_count
        expected_total = num_selected * (1 + self.detail_questions_per_doc)
        
        statistics = {
            "total_documents_available": len(all_documents),
            "documents_selected": num_selected,
            "document_selection_ratio": target_document_ratio,
            "summary_questions_generated": summary_question_count,
            "detail_questions_generated": detail_question_count,
            "total_qa_pairs": len(all_qa_pairs),
            "total_generated_before_validation": total_generated,
            "validation_success_count": validated_count,
            "questions_per_document_target": 1 + self.detail_questions_per_doc,
            "questions_per_document_actual": len(all_qa_pairs) / num_selected if num_selected > 0 else 0,
            "generation_success_rate": total_generated / expected_total if expected_total > 0 else 0,
            "validation_success_rate": validated_count / total_generated if total_generated > 0 else 0,
            "overall_success_rate": len(all_qa_pairs) / expected_total if expected_total > 0 else 0,
            "total_generation_time_seconds": total_time,
            "average_time_per_document": total_time / num_selected if num_selected > 0 else 0,
            "api_rate_limit": self.rate_limiter.requests_per_minute,
            "ai_validation_enabled": self.enable_ai_validation,
            "generation_model": self.generation_model_name,
            "validation_model": self.validation_model_name,
            "generated_at": datetime.now().isoformat(),
            "generation_strategy": "1+N_questions_per_document"
        }
        
        logger.info(f"ğŸ‰ æ¸¬è©¦æ•¸æ“šé›†ç”Ÿæˆå®Œæˆï¼")
        logger.info(f"ğŸ“Š çµ±è¨ˆä¿¡æ¯:")
        logger.info(f"   - å¯ç”¨æ–‡æª”: {statistics['total_documents_available']}")
        logger.info(f"   - é¸æ“‡æ–‡æª”: {statistics['documents_selected']}")
        logger.info(f"   - ä¸»é¡Œç´šå•é¡Œ: {statistics['summary_questions_generated']}")
        logger.info(f"   - ç´°ç¯€ç´šå•é¡Œ: {statistics['detail_questions_generated']}")
        logger.info(f"   - ç¸½å•ç­”å°: {statistics['total_qa_pairs']}")
        logger.info(f"   - ç”ŸæˆæˆåŠŸç‡: {statistics['generation_success_rate']:.2%}")
        logger.info(f"   - é©—è­‰æˆåŠŸç‡: {statistics['validation_success_rate']:.2%}")
        logger.info(f"   - æ•´é«”æˆåŠŸç‡: {statistics['overall_success_rate']:.2%}")
        logger.info(f"   - ç¸½è€—æ™‚: {statistics['total_generation_time_seconds']/60:.1f} åˆ†é˜")
        logger.info(f"   - å¹³å‡æ™‚é–“/æ–‡æª”: {statistics['average_time_per_document']:.1f} ç§’")
        
        # ä¿å­˜çµæœ
        if output_path:
            # ç¢ºä¿è¼¸å‡ºè·¯å¾‘æœ‰æ­£ç¢ºçš„å‰¯æª”å
            eval_output_path = output_path
            if not eval_output_path.endswith('.json'):
                eval_output_path += '.json'
                logger.info(f"å·²è‡ªå‹•æ·»åŠ  .json å‰¯æª”å: {eval_output_path}")
            
            # ç‚ºè©•ä¼°è…³æœ¬ä¿å­˜ç´”æ¸¬è©¦æ¡ˆä¾‹æ ¼å¼ (æ¨™æº–JSONæ•¸çµ„)
            with open(eval_output_path, 'w', encoding='utf-8') as f:
                json.dump(all_qa_pairs, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“ è©•ä¼°ç”¨æ¸¬è©¦æ•¸æ“šé›†å·²ä¿å­˜åˆ°: {eval_output_path}")
            logger.info(f"ğŸ“Š æ ¼å¼: ç´”JSONæ•¸çµ„ï¼ŒåŒ…å« {len(all_qa_pairs)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
            
            # ä¿å­˜åŒ…å«å®Œæ•´å…ƒæ•¸æ“šçš„è©³ç´°ç‰ˆæœ¬
            detailed_output_path = eval_output_path.replace('.json', '_detailed.json')
            detailed_output_data = {
                "test_cases": all_qa_pairs,
                "metadata": {
                    "dataset_version": "2.0",
                    "generated_by": "google_api_1_plus_n_strategy",
                    "statistics": statistics,
                    "generation_completed": not self.graceful_shutdown
                }
            }
            
            with open(detailed_output_path, 'w', encoding='utf-8') as f:
                json.dump(detailed_output_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“ è©³ç´°æ¸¬è©¦æ•¸æ“šé›†å·²ä¿å­˜åˆ°: {detailed_output_path}")
            
            # å¦‚æœæ˜¯å„ªé›…çµ‚æ­¢ï¼Œé¡å¤–èªªæ˜
            if self.graceful_shutdown:
                logger.warning(f"âš ï¸  ç”Ÿæˆè¢«ä¸­æ–·ï¼Œä½†å·²ä¿å­˜ {len(all_qa_pairs)} å€‹æœ‰æ•ˆå•ç­”å°")
        
        return {
            "qa_pairs": all_qa_pairs,
            "statistics": statistics
        }

    def _setup_signal_handlers(self):
        """è¨­ç½®ä¿¡è™Ÿè™•ç†"""
        signal.signal(signal.SIGINT, self._handle_signal)
        signal.signal(signal.SIGTERM, self._handle_signal)

    def _handle_signal(self, signum, frame):
        """è™•ç†ä¿¡è™Ÿ"""
        logger.info(f"æ”¶åˆ°ä¿¡è™Ÿ {signum}ï¼Œé–‹å§‹å„ªé›…çµ‚æ­¢...")
        self.graceful_shutdown = True
        # ç«‹å³ä¿å­˜ç•¶å‰é€²åº¦
        if self.current_qa_pairs and self.current_output_path:
            self._save_intermediate_results()

    def _save_intermediate_results(self):
        """ä¿å­˜ä¸­é–“çµæœ"""
        if not self.current_qa_pairs or not self.current_output_path:
            return
        
        try:
            # ç¢ºä¿è¼¸å‡ºè·¯å¾‘æœ‰æ­£ç¢ºçš„å‰¯æª”å
            output_path = self.current_output_path
            if not output_path.endswith('.json'):
                output_path += '.json'
            
            # ä¿å­˜è©•ä¼°ç”¨çš„ç´”æ¸¬è©¦æ¡ˆä¾‹æ ¼å¼
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.current_qa_pairs, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å¸¶çµ±è¨ˆä¿¡æ¯çš„è©³ç´°ç‰ˆæœ¬
            detailed_path = output_path.replace('.json', '_detailed.json')
            detailed_data = {
                "test_cases": self.current_qa_pairs,
                "metadata": {
                    "dataset_version": "2.0",
                    "generated_by": "google_api_1_plus_n_strategy_interrupted",
                    "intermediate_save": True,
                    "save_timestamp": datetime.now().isoformat(),
                    "total_qa_pairs": len(self.current_qa_pairs)
                }
            }
            
            with open(detailed_path, 'w', encoding='utf-8') as f:
                json.dump(detailed_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ä¸­é–“çµæœå·²ä¿å­˜: {len(self.current_qa_pairs)} å€‹å•ç­”å°")
            logger.info(f"ğŸ“ è©•ä¼°æ ¼å¼: {output_path}")
            logger.info(f"ğŸ“ è©³ç´°æ ¼å¼: {detailed_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ä¸­é–“çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    async def close(self):
        """é—œé–‰é€£æ¥"""
        if self.session and not self.session.closed:
            await self.session.close()

async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='åŸºæ–¼Google APIçš„1+Næ¸¬è©¦æ•¸æ“šé›†ç”Ÿæˆå™¨')
    parser.add_argument('--document-ratio', type=float, default=0.5, help='é¸æ“‡æ–‡æª”çš„æ¯”ä¾‹ (é è¨­: 0.5ï¼Œå³ä¸€åŠæ–‡æª”)')
    parser.add_argument('--detail-questions', type=int, default=2, help='æ¯å€‹æ–‡æª”ç”Ÿæˆçš„ç´°ç¯€ç´šå•é¡Œæ•¸é‡ (é è¨­: 2)')
    parser.add_argument('--output', type=str, help='è¼¸å‡ºæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--disable-validation', action='store_true', help='ç¦ç”¨AIé©—è­‰ä»¥ç¯€çœAPIèª¿ç”¨')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥å¿…è¦çš„ç’°å¢ƒè®Šæ•¸
    required_vars = ['API_URL', 'USERNAME', 'PASSWORD', 'GOOGLE_API_KEY']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        logger.error(f"ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸: {', '.join(missing_vars)}")
        logger.error("è«‹åœ¨ .env æ–‡ä»¶ä¸­é…ç½®é€™äº›è®Šæ•¸")
        return
    
    # è¨­å®šè¼¸å‡ºè·¯å¾‘ï¼Œç¢ºä¿æœ‰æ­£ç¢ºçš„ .json å‰¯æª”å
    if not args.output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.output = f"test_dataset_1plus{args.detail_questions}_{timestamp}.json"
    else:
        # å¦‚æœç”¨æˆ¶æŒ‡å®šäº†è¼¸å‡ºè·¯å¾‘ï¼Œç¢ºä¿æœ‰ .json å‰¯æª”å
        if not args.output.endswith('.json'):
            args.output += '.json'
            logger.info(f"å·²è‡ªå‹•æ·»åŠ  .json å‰¯æª”å: {args.output}")
    
    # è¨­å®šç´°ç¯€ç´šå•é¡Œæ•¸é‡
    os.environ['DETAIL_QUESTIONS_PER_DOC'] = str(args.detail_questions)
    
    # è¨­å®šé©—è­‰é¸é …
    if args.disable_validation:
        os.environ['ENABLE_AI_VALIDATION'] = 'false'
        logger.info("ğŸš« AIé©—è­‰å·²ç¦ç”¨")
    
    generator = TestDatasetGenerator()
    
    try:
        await generator.initialize_api_connection()
        
        result = await generator.generate_test_dataset(
            target_document_ratio=args.document_ratio,
            output_path=args.output
        )
        
        # è¼¸å‡ºçµ±è¨ˆä¿¡æ¯
        stats = result['statistics']
        print(f"\nğŸ‰ æ¸¬è©¦æ•¸æ“šé›†ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"   - å¯ç”¨æ–‡æª”ç¸½æ•¸: {stats['total_documents_available']}")
        print(f"   - é¸æ“‡æ–‡æª”æ•¸é‡: {stats['documents_selected']} ({stats['document_selection_ratio']:.1%})")
        print(f"   - ä¸»é¡Œç´šå•é¡Œ: {stats['summary_questions_generated']}")
        print(f"   - ç´°ç¯€ç´šå•é¡Œ: {stats['detail_questions_generated']}")
        print(f"   - ç¸½å•ç­”å°æ•¸é‡: {stats['total_qa_pairs']}")
        print(f"   - æ¯æ–‡æª”å¯¦éš›å•é¡Œæ•¸: {stats['questions_per_document_actual']:.1f}")
        print(f"   - ç”ŸæˆæˆåŠŸç‡: {stats['generation_success_rate']:.2%}")
        print(f"   - é©—è­‰æˆåŠŸç‡: {stats['validation_success_rate']:.2%}")
        print(f"   - æ•´é«”æˆåŠŸç‡: {stats['overall_success_rate']:.2%}")
        print(f"   - ç¸½è€—æ™‚: {stats['total_generation_time_seconds']/60:.1f} åˆ†é˜")
        print(f"   - å¹³å‡æ™‚é–“/æ–‡æª”: {stats['average_time_per_document']:.1f} ç§’")
        print(f"ğŸ“ è©•ä¼°ç”¨æ–‡ä»¶: {args.output}")
        print(f"ğŸ“ è©³ç´°æ–‡ä»¶: {args.output.replace('.json', '_detailed.json')}")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¸¬è©¦æ•¸æ“šé›†å¤±æ•—: {e}")
        raise
    finally:
        await generator.close()

if __name__ == "__main__":
    asyncio.run(main()) 