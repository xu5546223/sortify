import json
import asyncio
import logging
import sys
import argparse
from typing import List, Dict, Any, Optional
from collections import defaultdict
import numpy as np
import pandas as pd
import aiohttp
from dotenv import load_dotenv
import os
# --- Foolproof .env loading ---
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
except NameError:
    script_dir = os.getcwd()

dotenv_path = os.path.join(script_dir, '.env')
if not os.path.exists(dotenv_path):
    project_root = os.path.dirname(script_dir)
    dotenv_path_alt = os.path.join(project_root, 'evaluation', '.env')
    if os.path.exists(dotenv_path_alt):
        dotenv_path = dotenv_path_alt
    else:
        dotenv_path_root = os.path.join(project_root, '.env')
        if os.path.exists(dotenv_path_root):
            dotenv_path = dotenv_path_root
        else:
            print(f"FATAL: '.env' file not found at various checked paths.")
            sys.exit(1)
load_dotenv(dotenv_path=dotenv_path, override=True)
print(f"Successfully loaded .env file from: {dotenv_path}")

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Dummy Rate Limiter ---
try:
    from api_rate_limiter import APIRateLimiter
except ImportError:
    logger.warning("api_rate_limiter.py not found. Using a dummy rate limiter.")
    class APIRateLimiter:
        def __init__(self, requests_per_minute: int):
            self.delay = 60.0 / requests_per_minute if requests_per_minute > 0 else 0
        async def wait_if_needed_async(self):
            if self.delay > 0:
                await asyncio.sleep(self.delay)

logger.info("è©•ä¼°ç³»çµ±ï¼šRRFèåˆæª¢ç´¢ vs å…©éšæ®µæ··åˆæª¢ç´¢ vs å‚³çµ±å‘é‡æª¢ç´¢ - å…¨é¢æº–ç¢ºåº¦å°æ¯”åˆ†æ")


class VectorRetrievalEvaluator:
    """å…©éšæ®µæ··åˆæª¢ç´¢æº–ç¢ºåº¦è©•ä¼°å™¨ - ä½¿ç”¨APIèª¿ç”¨ï¼Œæ”¯æ´å¤šç¨®æœç´¢æ¨¡å¼å°æ¯”"""
    
    def __init__(self):
        """åˆå§‹åŒ–è©•ä¼°å™¨"""
        self.api_base_url = os.getenv('API_URL', 'http://localhost:8000')
        self.api_username = os.getenv('USERNAME')
        self.api_password = os.getenv('PASSWORD')
        if not self.api_username or not self.api_password:
            raise ValueError("è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½® USERNAME å’Œ PASSWORD")
        self.session = None
        self.access_token = None
        
        # æ”¯æ´çš„æœç´¢æ¨¡å¼
        self.search_modes = {
            'rrf_fusion': 'ğŸš€ RRF èåˆæª¢ç´¢',
            'hybrid': 'å…©éšæ®µæ··åˆæª¢ç´¢',
            'summary_only': 'åƒ…æ‘˜è¦å‘é‡æœç´¢',
            'chunks_only': 'åƒ…å…§å®¹å¡Šæœç´¢',
            'legacy': 'å‚³çµ±å–®éšæ®µæœç´¢'
        }
        
        logger.info("æ··åˆæª¢ç´¢è©•ä¼°å™¨åˆå§‹åŒ–å®Œæˆ - æ”¯æ´å¤šç¨®æœç´¢æ¨¡å¼å°æ¯” (åŒ…å« RRF èåˆæª¢ç´¢)")

    async def initialize_services(self):
        """åˆå§‹åŒ–APIé€£æ¥"""
        try:
            self.session = aiohttp.ClientSession()
            if not await self._login_and_get_token(): # <<< MODIFIED: ä½¿ç”¨çµ±ä¸€çš„ aiohttp ç™»å…¥
                raise Exception("ç„¡æ³•ç²å–èªè­‰ token")
            await self._test_api_connection()
            logger.info("APIé€£æ¥åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"APIé€£æ¥åˆå§‹åŒ–å¤±æ•—: {e}", exc_info=True)
            raise
    
    # <<< MODIFIED: çµ±ä¸€ä½¿ç”¨ aiohttp é€²è¡Œç™»å…¥
    async def _login_and_get_token(self) -> bool:
        """ç™»å…¥ä¸¦ç²å– JWT tokenï¼Œçµ±ä¸€ä½¿ç”¨ aiohttp"""
        if not self.session:
            self.session = aiohttp.ClientSession()
        login_url = f"{self.api_base_url}/api/v1/auth/token"
        login_data = {"username": self.api_username, "password": self.api_password}
        logger.info(f"å˜—è©¦ç™»å…¥åˆ°: {login_url}")
        try:
            async with self.session.post(login_url, data=login_data) as response:
                response.raise_for_status() # å¦‚æœç‹€æ…‹ç¢¼ä¸æ˜¯ 2xxï¼Œå‰‡å¼•ç™¼ç•°å¸¸
                result = await response.json()
                self.access_token = result.get("access_token")
                if self.access_token:
                    logger.info("ç™»å…¥æˆåŠŸï¼Œç²å–åˆ° JWT token")
                    return True
                else:
                    logger.error("ç™»å…¥å¤±æ•—ï¼šéŸ¿æ‡‰ä¸­æ²’æœ‰ access_token")
                    return False
        except Exception as e:
            logger.error(f"ç™»å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return False
            
    def get_auth_headers(self) -> Dict[str, str]:
        if not self.access_token:
            raise ValueError("æœªç²å–åˆ° access_tokenï¼Œè«‹å…ˆç™»å…¥")
        return {"Authorization": f"Bearer {self.access_token}", "Content-Type": "application/json"}

    async def _test_api_connection(self):
        try:
            headers = self.get_auth_headers()
            async with self.session.get(f"{self.api_base_url}/", headers=headers) as response:
                response.raise_for_status()
                logger.info("APIé€£æ¥æ¸¬è©¦æˆåŠŸ")
        except Exception as e:
            logger.warning(f"APIé€£æ¥æ¸¬è©¦å¤±æ•—: {e}")

    async def _search_vectors_via_api(self, query: str, top_k: int, similarity_threshold: float, 
                                     search_mode: str = 'hybrid', enable_diversity: bool = True,
                                     rrf_weights: Optional[Dict[str, float]] = None,
                                     rrf_k_constant: Optional[int] = None) -> List[Dict]:
        """é€šéAPIåŸ·è¡Œå‘é‡æª¢ç´¢ - æ”¯æ´å¤šç¨®æœç´¢æ¨¡å¼å’Œå‹•æ…‹RRFæ¬Šé‡"""
        headers = self.get_auth_headers()
        url = f"{self.api_base_url}/api/v1/vector-db/semantic-search"
        
        # æ–°çš„APIè«‹æ±‚åƒæ•¸
        payload = {
            "query": query, 
            "top_k": top_k, 
            "similarity_threshold": similarity_threshold,
            "enable_hybrid_search": search_mode != 'legacy',  # éå‚³çµ±æ¨¡å¼éƒ½å•Ÿç”¨æ··åˆæœç´¢
            "enable_diversity_optimization": enable_diversity
        }
        
        # åªæœ‰éå‚³çµ±æ¨¡å¼æ‰è¨­ç½®search_type
        if search_mode != 'legacy':
            payload["search_type"] = search_mode
        
        # æ·»åŠ RRFæ¬Šé‡é…ç½®ï¼ˆåƒ…ç”¨æ–¼rrf_fusionæ¨¡å¼ï¼‰
        if search_mode == 'rrf_fusion':
            if rrf_weights:
                payload["rrf_weights"] = rrf_weights
            if rrf_k_constant:
                payload["rrf_k_constant"] = rrf_k_constant
        
        try:
            async with self.session.post(url, json=payload, headers=headers) as response:
                response.raise_for_status()
                results = await response.json()
                mode_name = self.search_modes.get(search_mode, search_mode)
                weight_info = f" (æ¬Šé‡: {rrf_weights}, k: {rrf_k_constant})" if rrf_weights else ""
                logger.info(f"[{mode_name}{weight_info}] æŸ¥è©¢ '{query[:30]}...' -> æª¢ç´¢åˆ° {len(results)} å€‹çµæœ")
                return results
        except Exception as e:
            logger.error(f"[{search_mode}] å‘é‡æª¢ç´¢APIèª¿ç”¨ç•°å¸¸ for query '{query[:30]}...': {e}")
            return []

    async def evaluate_all_search_modes(self, test_cases: List[Dict], eval_params: Dict) -> Dict[str, Any]:
        """è©•ä¼°æ‰€æœ‰æœç´¢æ¨¡å¼çš„æº–ç¢ºåº¦ä¸¦é€²è¡Œå°æ¯”"""
        logger.info(f"é–‹å§‹è©•ä¼°æ‰€æœ‰æœç´¢æ¨¡å¼ï¼Œåƒæ•¸: {eval_params}")
        
        results_by_mode = {}
        
        # è©•ä¼°æ¯ç¨®æœç´¢æ¨¡å¼
        for mode_key, mode_name in self.search_modes.items():
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ” è©•ä¼°æœç´¢æ¨¡å¼: {mode_name} ({mode_key})")
            logger.info(f"{'='*60}")
            
            mode_results = await self._evaluate_single_mode(test_cases, eval_params, mode_key)
            results_by_mode[mode_key] = mode_results
            
            # å³æ™‚é¡¯ç¤ºè©²æ¨¡å¼çš„çµæœ
            self._print_single_mode_results(mode_results, mode_name)
        
        # ç”Ÿæˆç¶œåˆå°æ¯”çµæœ
        comparison_results = self._generate_comparison_results(results_by_mode, eval_params)
        
        return comparison_results

    async def _evaluate_single_mode(self, test_cases: List[Dict], eval_params: Dict, search_mode: str) -> Dict[str, Any]:
        """è©•ä¼°å–®ä¸€æœç´¢æ¨¡å¼çš„æº–ç¢ºåº¦"""
        all_metrics = []
        verbose_mode = eval_params.get('verbose_mode', False)
        case_details = []  # å„²å­˜å€‹æ¡ˆè©³ç´°çµæœ
        
        for i, test_case in enumerate(test_cases):
            question = test_case.get('question') or test_case.get('user_query')
            expected_doc_ids = test_case.get('expected_relevant_doc_ids', [])
            
            if not question or not expected_doc_ids:
                logger.warning(f"è·³éæ¡ˆä¾‹ {i+1}ï¼Œç¼ºå°‘å¿…è¦æ¬„ä½")
                continue
            
            search_results = await self._search_vectors_via_api(
                question, eval_params['top_k'], eval_params['similarity_threshold'], search_mode
            )
            retrieved_doc_ids = [doc.get('document_id', '') for doc in search_results]

            metrics = {
                "hit_rate": self._calculate_hit_rate(expected_doc_ids, retrieved_doc_ids),
                "mrr": self._calculate_mrr(expected_doc_ids, retrieved_doc_ids),
                "ndcg": self._calculate_ndcg(expected_doc_ids, retrieved_doc_ids)
            }
            all_metrics.append(metrics)
            
            # è©³ç´°æ¨¡å¼ï¼šè¨˜éŒ„å€‹æ¡ˆçµæœ
            if verbose_mode:
                case_detail = {
                    "case_index": i + 1,
                    "question": question[:100] + "..." if len(question) > 100 else question,
                    "expected_doc_ids": expected_doc_ids,
                    "retrieved_doc_ids": retrieved_doc_ids,
                    "hit_found": bool(set(expected_doc_ids) & set(retrieved_doc_ids)),
                    "metrics": metrics
                }
                case_details.append(case_detail)
                
                # å³æ™‚é¡¯ç¤ºå€‹æ¡ˆçµæœ
                hit_found = "âœ…" if case_detail["hit_found"] else "âŒ"
                print(f"  æ¡ˆä¾‹ {i+1:2d}: {hit_found} MRR={metrics['mrr']:.3f} | {question[:60]}...")
        
        results = self._aggregate_results(all_metrics, len(test_cases), eval_params, search_mode)
        
        # å¦‚æœå•Ÿç”¨è©³ç´°æ¨¡å¼ï¼ŒåŠ å…¥å€‹æ¡ˆåˆ†æ
        if verbose_mode:
            results["case_details"] = case_details
            results["detailed_analysis"] = self._analyze_case_details(case_details)
        
        return results

    # ä¿ç•™åŸæœ‰çš„è©•ä¼°æ–¹æ³•ä½œç‚ºå‘å¾Œå…¼å®¹
    async def evaluate_retrieval_accuracy(self, test_cases: List[Dict], eval_params: Dict) -> Dict[str, Any]:
        """è©•ä¼°ç´”å‘é‡æª¢ç´¢çš„æº–ç¢ºåº¦ - å‘å¾Œå…¼å®¹æ–¹æ³•"""
        logger.info(f"é–‹å§‹è©•ä¼°ç´”å‘é‡æª¢ç´¢ï¼Œåƒæ•¸: {eval_params}")
        return await self._evaluate_single_mode(test_cases, eval_params, 'legacy')

    def _calculate_hit_rate(self, expected_ids: List[str], retrieved_ids: List[str]) -> Dict[str, float]:
        expected_set = set(expected_ids)
        return {f"@_k{k}": 1.0 if expected_set.intersection(set(retrieved_ids[:k])) else 0.0 for k in [1, 3, 5, 10]}

    def _calculate_mrr(self, expected_ids: List[str], retrieved_ids: List[str]) -> float:
        expected_set = set(expected_ids)
        for rank, doc_id in enumerate(retrieved_ids, 1):
            if doc_id in expected_set:
                return 1.0 / rank
        return 0.0
        
    def _calculate_ndcg(self, expected_ids: List[str], retrieved_ids: List[str]) -> Dict[str, float]:
        expected_set = set(expected_ids)
        ndcg_scores = {}
        for k in [1, 3, 5, 10]:
            relevance = [1 if doc_id in expected_set else 0 for doc_id in retrieved_ids[:k]]
            dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance))
            idcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(expected_ids))))
            ndcg_scores[f"@_k{k}"] = dcg / idcg if idcg > 0 else 0.0
        return ndcg_scores

    def _aggregate_results(self, all_metrics: List[Dict], total_cases: int, eval_params: Dict, search_mode: str = None) -> Dict:
        """åŒ¯ç¸½æ‰€æœ‰è©•ä¼°æŒ‡æ¨™"""
        if not all_metrics:
            return {"error": "No metrics were calculated."}

        df = pd.json_normalize(all_metrics)
        mean_scores = df.mean().to_dict()
        hit_rate_scores = {k.replace('hit_rate.@_k', '@'): v for k, v in mean_scores.items() if k.startswith('hit_rate')}
        ndcg_scores = {k.replace('ndcg.@_k', '@'): v for k, v in mean_scores.items() if k.startswith('ndcg')}

        evaluation_type = "hybrid_vector_retrieval" if search_mode == 'hybrid' else f"{search_mode}_vector_retrieval" if search_mode else "pure_vector_retrieval_baseline"

        return {
            "evaluation_type": evaluation_type,
            "search_mode": search_mode,
            "total_test_cases": total_cases,
            "processed_cases": len(all_metrics),
            "retrieval_metrics": {
                "hit_rate": hit_rate_scores,
                "mrr": mean_scores.get('mrr', 0.0),
                "ndcg": ndcg_scores
            },
            "evaluation_parameters": eval_params
        }
    
    def _generate_comparison_results(self, results_by_mode: Dict[str, Dict], eval_params: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæœç´¢æ¨¡å¼å°æ¯”çµæœ"""
        comparison = {
            "evaluation_type": "multi_mode_vector_retrieval_comparison",
            "evaluation_parameters": eval_params,
            "modes_compared": list(self.search_modes.keys()),
            "results_by_mode": results_by_mode,
            "performance_ranking": self._rank_modes_by_performance(results_by_mode),
            "improvement_analysis": self._analyze_improvements(results_by_mode)
        }
        
        return comparison

    def _rank_modes_by_performance(self, results_by_mode: Dict[str, Dict]) -> List[Dict]:
        """æ ¹æ“šMRRåˆ†æ•¸æ’åå„ç¨®æœç´¢æ¨¡å¼"""
        rankings = []
        
        for mode, results in results_by_mode.items():
            mrr_score = results.get('retrieval_metrics', {}).get('mrr', 0.0)
            hit_rate_at_5 = results.get('retrieval_metrics', {}).get('hit_rate', {}).get('@5', 0.0)
            
            rankings.append({
                "mode": mode,
                "mode_name": self.search_modes[mode],
                "mrr_score": mrr_score,
                "hit_rate_at_5": hit_rate_at_5,
                "composite_score": (mrr_score * 0.6) + (hit_rate_at_5 * 0.4)  # ç¶œåˆè©•åˆ†
            })
        
        # æŒ‰ç¶œåˆè©•åˆ†æ’åº
        rankings.sort(key=lambda x: x["composite_score"], reverse=True)
        
        return rankings

    def _analyze_improvements(self, results_by_mode: Dict[str, Dict]) -> Dict[str, Any]:
        """åˆ†æ RRF èåˆæª¢ç´¢å’Œå…©éšæ®µæ··åˆæª¢ç´¢ç›¸å°æ–¼å‚³çµ±æª¢ç´¢çš„æ”¹é€²"""
        rrf_results = results_by_mode.get("rrf_fusion", {}).get("retrieval_metrics", {})
        hybrid_results = results_by_mode.get("hybrid", {}).get("retrieval_metrics", {})
        legacy_results = results_by_mode.get("legacy", {}).get("retrieval_metrics", {})
        
        if not legacy_results:
            return {"error": "ç¼ºå°‘å‚³çµ±æª¢ç´¢åŸºæº–æ•¸æ“š"}
        
        improvements = {}
        
        # MRR æ”¹é€²åˆ†æ
        legacy_mrr = legacy_results.get("mrr", 0.0)
        
        # èˆ‡å‚³çµ±æª¢ç´¢å°æ¯”
        comparisons = {}
        
        # RRF èåˆæª¢ç´¢å°æ¯”
        if rrf_results:
            rrf_mrr = rrf_results.get("mrr", 0.0)
            comparisons["rrf_vs_legacy"] = {
                "rrf": rrf_mrr,
                "legacy": legacy_mrr,
                "improvement_percentage": ((rrf_mrr - legacy_mrr) / max(legacy_mrr, 0.001)) * 100
            }
        
        # å…©éšæ®µæ··åˆæª¢ç´¢å°æ¯”
        if hybrid_results:
            hybrid_mrr = hybrid_results.get("mrr", 0.0)
            comparisons["hybrid_vs_legacy"] = {
                "hybrid": hybrid_mrr,
                "legacy": legacy_mrr,
                "improvement_percentage": ((hybrid_mrr - legacy_mrr) / max(legacy_mrr, 0.001)) * 100
            }
        
        # RRF vs å…©éšæ®µæ··åˆæª¢ç´¢å°æ¯”
        if rrf_results and hybrid_results:
            rrf_mrr = rrf_results.get("mrr", 0.0)
            hybrid_mrr = hybrid_results.get("mrr", 0.0)
            comparisons["rrf_vs_hybrid"] = {
                "rrf": rrf_mrr,
                "hybrid": hybrid_mrr,
                "improvement_percentage": ((rrf_mrr - hybrid_mrr) / max(hybrid_mrr, 0.001)) * 100
            }
        
        improvements["mrr_comparisons"] = comparisons
        
        # Hit Rate æ”¹é€²åˆ†æ
        improvements["hit_rate_comparisons"] = {}
        legacy_hr = legacy_results.get("hit_rate", {})
        
        # å°æ¯å€‹ Hit Rate æŒ‡æ¨™é€²è¡Œæ¯”è¼ƒ
        for key in legacy_hr.keys():
            legacy_hr_val = legacy_hr.get(key, 0.0)
            key_comparisons = {"legacy": legacy_hr_val}
            
            # RRF vs Legacy
            if rrf_results:
                rrf_hr = rrf_results.get("hit_rate", {})
                rrf_hr_val = rrf_hr.get(key, 0.0)
                key_comparisons["rrf_vs_legacy"] = {
                    "rrf": rrf_hr_val,
                    "improvement_percentage": ((rrf_hr_val - legacy_hr_val) / max(legacy_hr_val, 0.001)) * 100
                }
            
            # Hybrid vs Legacy
            if hybrid_results:
                hybrid_hr = hybrid_results.get("hit_rate", {})
                hybrid_hr_val = hybrid_hr.get(key, 0.0)
                key_comparisons["hybrid_vs_legacy"] = {
                    "hybrid": hybrid_hr_val,
                    "improvement_percentage": ((hybrid_hr_val - legacy_hr_val) / max(legacy_hr_val, 0.001)) * 100
                }
            
            # RRF vs Hybrid
            if rrf_results and hybrid_results:
                rrf_hr = rrf_results.get("hit_rate", {})
                hybrid_hr = hybrid_results.get("hit_rate", {})
                rrf_hr_val = rrf_hr.get(key, 0.0)
                hybrid_hr_val = hybrid_hr.get(key, 0.0)
                key_comparisons["rrf_vs_hybrid"] = {
                    "rrf": rrf_hr_val,
                    "hybrid": hybrid_hr_val,
                    "improvement_percentage": ((rrf_hr_val - hybrid_hr_val) / max(hybrid_hr_val, 0.001)) * 100
                }
            
            improvements["hit_rate_comparisons"][key] = key_comparisons
        
        # nDCG æ”¹é€²åˆ†æ
        improvements["ndcg_comparisons"] = {}
        legacy_ndcg = legacy_results.get("ndcg", {})
        
        # å°æ¯å€‹ nDCG æŒ‡æ¨™é€²è¡Œæ¯”è¼ƒ
        for key in legacy_ndcg.keys():
            legacy_ndcg_val = legacy_ndcg.get(key, 0.0)
            key_comparisons = {"legacy": legacy_ndcg_val}
            
            # RRF vs Legacy
            if rrf_results:
                rrf_ndcg = rrf_results.get("ndcg", {})
                rrf_ndcg_val = rrf_ndcg.get(key, 0.0)
                key_comparisons["rrf_vs_legacy"] = {
                    "rrf": rrf_ndcg_val,
                    "improvement_percentage": ((rrf_ndcg_val - legacy_ndcg_val) / max(legacy_ndcg_val, 0.001)) * 100
                }
            
            # Hybrid vs Legacy
            if hybrid_results:
                hybrid_ndcg = hybrid_results.get("ndcg", {})
                hybrid_ndcg_val = hybrid_ndcg.get(key, 0.0)
                key_comparisons["hybrid_vs_legacy"] = {
                    "hybrid": hybrid_ndcg_val,
                    "improvement_percentage": ((hybrid_ndcg_val - legacy_ndcg_val) / max(legacy_ndcg_val, 0.001)) * 100
                }
            
            # RRF vs Hybrid
            if rrf_results and hybrid_results:
                rrf_ndcg = rrf_results.get("ndcg", {})
                hybrid_ndcg = hybrid_results.get("ndcg", {})
                rrf_ndcg_val = rrf_ndcg.get(key, 0.0)
                hybrid_ndcg_val = hybrid_ndcg.get(key, 0.0)
                key_comparisons["rrf_vs_hybrid"] = {
                    "rrf": rrf_ndcg_val,
                    "hybrid": hybrid_ndcg_val,
                    "improvement_percentage": ((rrf_ndcg_val - hybrid_ndcg_val) / max(hybrid_ndcg_val, 0.001)) * 100
                }
            
            improvements["ndcg_comparisons"][key] = key_comparisons
        
        # ç”Ÿæˆæ™ºèƒ½å»ºè­°
        recommendations = []
        
        # æ¯”è¼ƒ RRF èˆ‡å…¶ä»–æ–¹æ³•
        if rrf_results and hybrid_results:
            rrf_vs_legacy = comparisons.get("rrf_vs_legacy", {}).get("improvement_percentage", 0)
            rrf_vs_hybrid = comparisons.get("rrf_vs_hybrid", {}).get("improvement_percentage", 0)
            hybrid_vs_legacy = comparisons.get("hybrid_vs_legacy", {}).get("improvement_percentage", 0)
            
            # ç¢ºå®šæœ€ä½³ç­–ç•¥
            if rrf_vs_legacy > 15 and rrf_vs_hybrid > 5:
                recommendations.append("ğŸš€ å¼·çƒˆå»ºè­°ä½¿ç”¨ RRF èåˆæª¢ç´¢ï¼ç›¸æ¯”å‚³çµ±æª¢ç´¢æå‡é¡¯è‘—ï¼Œä¸”å„ªæ–¼å…©éšæ®µæ··åˆæª¢ç´¢")
            elif rrf_vs_legacy > 10:
                recommendations.append("ğŸ¯ å»ºè­°å„ªå…ˆè€ƒæ…® RRF èåˆæª¢ç´¢ï¼Œç›¸æ¯”å‚³çµ±æª¢ç´¢æœ‰æ˜é¡¯æ”¹é€²")
            elif hybrid_vs_legacy > 10:
                recommendations.append("âœ… å»ºè­°ä½¿ç”¨å…©éšæ®µæ··åˆæª¢ç´¢ï¼Œæ€§èƒ½ç©©å®šä¸”æœ‰è‰¯å¥½æ”¹é€²")
            elif rrf_vs_legacy > 0:
                recommendations.append("ğŸ’¡ RRF èåˆæª¢ç´¢æœ‰è¼•å¾®æ”¹é€²ï¼Œå¯æ ¹æ“šè¨ˆç®—è³‡æºæƒ…æ³é¸æ“‡")
            else:
                recommendations.append("âš ï¸  éœ€è¦é€²ä¸€æ­¥èª¿å„ªæª¢ç´¢åƒæ•¸æˆ–æª¢æŸ¥æ•¸æ“šè³ªé‡")
                
            # æ·»åŠ å…·é«”çš„æ€§èƒ½æ•¸æ“š
            recommendations.append(f"ğŸ“Š æ€§èƒ½æ•¸æ“š: RRF vs Legacy (+{rrf_vs_legacy:.1f}%), RRF vs Hybrid (+{rrf_vs_hybrid:.1f}%)")
        
        elif rrf_results:
            rrf_vs_legacy = comparisons.get("rrf_vs_legacy", {}).get("improvement_percentage", 0)
            if rrf_vs_legacy > 10:
                recommendations.append("ğŸš€ å»ºè­°æ¡ç”¨ RRF èåˆæª¢ç´¢ï¼Œç›¸æ¯”å‚³çµ±æª¢ç´¢æœ‰é¡¯è‘—æå‡")
            else:
                recommendations.append("ğŸ’¡ RRF èåˆæª¢ç´¢æœ‰æ”¹é€²ï¼Œå»ºè­°é€²ä¸€æ­¥è©•ä¼°")
        
        elif hybrid_results:
            hybrid_vs_legacy = comparisons.get("hybrid_vs_legacy", {}).get("improvement_percentage", 0)
            if hybrid_vs_legacy > 10:
                recommendations.append("âœ… å»ºè­°æ¡ç”¨å…©éšæ®µæ··åˆæª¢ç´¢ï¼Œç›¸æ¯”å‚³çµ±æª¢ç´¢æœ‰é¡¯è‘—æ”¹é€²")
            else:
                recommendations.append("âš ï¸  å…©éšæ®µæ··åˆæª¢ç´¢æ”¹é€²æœ‰é™ï¼Œéœ€è¦èª¿å„ªåƒæ•¸")
        
        improvements["recommendations"] = recommendations
        
        return improvements

    def _analyze_case_details(self, case_details: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå€‹æ¡ˆè©³ç´°çµæœï¼Œæä¾›æ´å¯Ÿ"""
        if not case_details:
            return {}
        
        total_cases = len(case_details)
        successful_cases = [case for case in case_details if case["hit_found"]]
        failed_cases = [case for case in case_details if not case["hit_found"]]
        
        analysis = {
            "ç¸½æ¡ˆä¾‹æ•¸": total_cases,
            "æˆåŠŸæ¡ˆä¾‹æ•¸": len(successful_cases),
            "å¤±æ•—æ¡ˆä¾‹æ•¸": len(failed_cases),
            "æˆåŠŸç‡": len(successful_cases) / total_cases if total_cases > 0 else 0,
            "å¹³å‡MRR": sum(case["metrics"]["mrr"] for case in case_details) / total_cases if total_cases > 0 else 0
        }
        
        # åˆ†æå¤±æ•—æ¡ˆä¾‹çš„ç‰¹å¾µ
        if failed_cases:
            failed_questions = [case["question"] for case in failed_cases[:5]]  # åªé¡¯ç¤ºå‰5å€‹
            analysis["å¤±æ•—æ¡ˆä¾‹ç¯„ä¾‹"] = failed_questions
        
        # åˆ†æé«˜åˆ†æ¡ˆä¾‹
        high_score_cases = [case for case in case_details if case["metrics"]["mrr"] >= 0.5]
        if high_score_cases:
            analysis["é«˜åˆ†æ¡ˆä¾‹æ•¸"] = len(high_score_cases)
            analysis["é«˜åˆ†æ¡ˆä¾‹ä½”æ¯”"] = len(high_score_cases) / total_cases
        
        return analysis

    def _print_single_mode_results(self, results: Dict[str, Any], mode_name: str):
        """æ‰“å°å–®ä¸€æ¨¡å¼çš„è©•ä¼°çµæœ"""
        metrics = results.get("retrieval_metrics", {})
        
        hr_str = ", ".join([f"{k}:{v:.4f}" for k, v in metrics.get("hit_rate", {}).items()])
        ndcg_str = ", ".join([f"{k}:{v:.4f}" for k, v in metrics.get("ndcg", {}).items()])
        
        print(f"\nğŸ“Š {mode_name} è©•ä¼°çµæœ:")
        print(f"   - MRR: {metrics.get('mrr', 0.0):.4f}")
        print(f"   - Hit Rate: {hr_str}")
        print(f"   - nDCG: {ndcg_str}")

    def print_comparison_results(self, results: Dict[str, Any]):
        """ä»¥å°äººé¡å‹å¥½çš„æ ¼å¼è¼¸å‡ºå°æ¯”è©•ä¼°çµæœ"""
        print("\n" + "="*100)
        print("ğŸ† RRF èåˆæª¢ç´¢ vs å…©éšæ®µæ··åˆæª¢ç´¢ vs å‚³çµ±å‘é‡æª¢ç´¢ - å…¨é¢æ€§èƒ½å°æ¯”")
        print("="*100)
        
        # é¡¯ç¤ºæ¯å€‹æ¨¡å¼çš„è©³ç´°æŒ‡æ¨™
        results_by_mode = results.get("detailed_results_by_mode", {})
        
        print("\nğŸ“Š å„æœç´¢æ¨¡å¼è©³ç´°æŒ‡æ¨™:")
        for mode_name, mode_results in results_by_mode.items():
            metrics = mode_results.get("retrieval_metrics", {})
            hr_str = ", ".join([f"@{k}:{v:.4f}" for k, v in metrics.get("hit_rate", {}).items()])
            ndcg_str = ", ".join([f"@{k}:{v:.4f}" for k, v in metrics.get("ndcg", {}).items()])
            
            print(f"\n   ğŸ” {mode_name}:")
            print(f"      - MRR: {metrics.get('mrr', 0.0):.4f}")
            print(f"      - Hit Rate: {hr_str}")
            print(f"      - nDCG: {ndcg_str}")
        
        # é¡¯ç¤ºæ€§èƒ½æ’å
        rankings = results.get("performance_ranking", [])
        print("\nğŸ¥‡ æœç´¢æ¨¡å¼æ€§èƒ½æ’å:")
        for i, rank in enumerate(rankings, 1):
            print(f"   {i}. {rank['mode_name']:<25} | MRR: {rank['mrr_score']:.4f} | Hit@5: {rank['hit_rate_at_5']:.4f} | ç¶œåˆè©•åˆ†: {rank['composite_score']:.4f}")
        
        # é¡¯ç¤ºæ”¹é€²åˆ†æ
        improvements = results.get("improvement_analysis", {})
        if "error" not in improvements:
            print(f"\nğŸ“ˆ æ€§èƒ½æ”¹é€²åˆ†æ:")
            
            # MRR å°æ¯”åˆ†æ
            mrr_comparisons = improvements.get("mrr_comparisons", {})
            if mrr_comparisons:
                print(f"   ğŸ¯ MRR å°æ¯”:")
                for comparison_type, comparison_data in mrr_comparisons.items():
                    if comparison_type.endswith("_vs_legacy"):
                        method = comparison_type.split("_vs_")[0].upper()
                        print(f"     {method} vs Legacy: {comparison_data.get('legacy', 0.0):.4f} â†’ {comparison_data.get(comparison_type.split('_vs_')[0], 0.0):.4f} ({comparison_data.get('improvement_percentage', 0.0):+.2f}%)")
                    elif comparison_type == "rrf_vs_hybrid":
                        print(f"     RRF vs Hybrid: {comparison_data.get('hybrid', 0.0):.4f} â†’ {comparison_data.get('rrf', 0.0):.4f} ({comparison_data.get('improvement_percentage', 0.0):+.2f}%)")
            
            # Hit Rate å°æ¯”åˆ†æ
            hr_comparisons = improvements.get("hit_rate_comparisons", {})
            if hr_comparisons:
                print(f"   ğŸ“Š Hit Rate å°æ¯”:")
                for metric, comparisons in hr_comparisons.items():
                    legacy_val = comparisons.get("legacy", 0.0)
                    print(f"     {metric} (åŸºæº–: {legacy_val:.4f}):")
                    
                    if "rrf_vs_legacy" in comparisons:
                        rrf_data = comparisons["rrf_vs_legacy"]
                        print(f"       RRF: {rrf_data.get('rrf', 0.0):.4f} ({rrf_data.get('improvement_percentage', 0.0):+.2f}%)")
                    
                    if "hybrid_vs_legacy" in comparisons:
                        hybrid_data = comparisons["hybrid_vs_legacy"]
                        print(f"       Hybrid: {hybrid_data.get('hybrid', 0.0):.4f} ({hybrid_data.get('improvement_percentage', 0.0):+.2f}%)")
            
            # nDCG å°æ¯”åˆ†æ
            ndcg_comparisons = improvements.get("ndcg_comparisons", {})
            if ndcg_comparisons:
                print(f"   ğŸ“ˆ nDCG å°æ¯”:")
                for metric, comparisons in ndcg_comparisons.items():
                    legacy_val = comparisons.get("legacy", 0.0)
                    print(f"     {metric} (åŸºæº–: {legacy_val:.4f}):")
                    
                    if "rrf_vs_legacy" in comparisons:
                        rrf_data = comparisons["rrf_vs_legacy"]
                        print(f"       RRF: {rrf_data.get('rrf', 0.0):.4f} ({rrf_data.get('improvement_percentage', 0.0):+.2f}%)")
                    
                    if "hybrid_vs_legacy" in comparisons:
                        hybrid_data = comparisons["hybrid_vs_legacy"]
                        print(f"       Hybrid: {hybrid_data.get('hybrid', 0.0):.4f} ({hybrid_data.get('improvement_percentage', 0.0):+.2f}%)")
            
            # é¡¯ç¤ºå»ºè­°
            recommendations = improvements.get("recommendations", [])
            if recommendations:
                print(f"\nğŸ’¡ æ™ºèƒ½å»ºè­°:")
                for rec in recommendations:
                    print(f"   {rec}")
        
        print("="*100 + "\n")

    # ä¿ç•™åŸæœ‰çš„çµæœè¼¸å‡ºå‡½å¼ä½œç‚ºå‘å¾Œå…¼å®¹
    def print_results(self, results: Dict[str, Any]):
        """ä»¥å°äººé¡å‹å¥½çš„æ ¼å¼è¼¸å‡ºè©•ä¼°çµæœ - å‘å¾Œå…¼å®¹"""
        params = results.get("evaluation_parameters", {})
        metrics = results.get("retrieval_metrics", {})
        
        # æº–å‚™ Hit Rate å’Œ NDCG çš„å­—ä¸²ä»¥ä¾¿å°é½Šæ‰“å°
        hr_str = ", ".join([f"{k}:{v:.4f}" for k, v in metrics.get("hit_rate", {}).items()])
        ndcg_str = ", ".join([f"{k}:{v:.4f}" for k, v in metrics.get("ndcg", {}).items()])
        
        print("\n" + "="*80)
        print("ğŸ“Š ç´”å‘é‡æª¢ç´¢åŸºæº–ç·š (Baseline) è©•ä¼°çµæœ")
        print("="*80)
        print(f"ğŸ¯ ç¸½æ¡ˆä¾‹æ•¸: {results.get('total_test_cases', 0):<5} | "
              f"å·²è™•ç†æ¡ˆä¾‹: {results.get('processed_cases', 0):<5}")
        print(f"âš™ï¸  è©•ä¼°åƒæ•¸: Top-K={params.get('top_k')}, Threshold={params.get('similarity_threshold')}")
        print("-"*80)
        print("ğŸ“ˆ æª¢ç´¢æ€§èƒ½æŒ‡æ¨™:")
        print(f"   - MRR (Mean Reciprocal Rank): {metrics.get('mrr', 0.0):.4f}")
        print(f"   - Hit Rate:                 {hr_str}")
        print(f"   - nDCG:                     {ndcg_str}")
        print("="*80 + "\n")

    def save_results_to_json(self, results: Dict[str, Any], output_path: str):
        """å°‡è©³ç´°è©•ä¼°çµæœä¿å­˜åˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            logger.info(f"è©³ç´°è©•ä¼°çµæœå·²ä¿å­˜è‡³: {output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜è©•ä¼°çµæœå¤±æ•—: {e}")

    async def run_evaluation_flow(self, dataset_path: str, eval_params: Dict, comparison_mode: bool = True):
        """åŸ·è¡Œå®Œæ•´çš„è©•ä¼°æµç¨‹ - æ”¯æ´å–®æ¨¡å¼æˆ–å¤šæ¨¡å¼å°æ¯”"""
        mode_desc = "å¤šæ¨¡å¼å°æ¯”è©•ä¼°" if comparison_mode else "å–®æ¨¡å¼è©•ä¼°"
        logger.info(f"é–‹å§‹{mode_desc}æµç¨‹...")
        
        if not os.path.isabs(dataset_path):
            dataset_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), dataset_path)
        logger.info(f"ä½¿ç”¨è³‡æ–™é›†: {dataset_path}")

        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
            logger.info(f"æˆåŠŸè¼‰å…¥ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¸¬è©¦æ•¸æ“šå¤±æ•—: {e}")
            return
        
        try:
            await self.initialize_services()
            
            if comparison_mode:
                # å¤šæ¨¡å¼å°æ¯”è©•ä¼°
                results = await self.evaluate_all_search_modes(test_cases, eval_params)
                self.print_comparison_results(results)
                output_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "rrf_hybrid_legacy_retrieval_comparison.json")
            else:
                # å–®æ¨¡å¼è©•ä¼°ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                results = await self.evaluate_retrieval_accuracy(test_cases, eval_params)
                self.print_results(results)
                output_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "vector_retrieval_baseline_results.json")
            
            self.save_results_to_json(results, output_path)
        finally:
            if self.session and not self.session.closed:
                await self.session.close()
                logger.info("HTTPæœƒè©±å·²é—œé–‰")

    async def _evaluate_rrf_with_weights(self, test_cases: List[Dict], eval_params: Dict, 
                                        rrf_weights: Dict[str, float], rrf_k_constant: int,
                                        description: str = "") -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šçš„RRFæ¬Šé‡å’ŒKå€¼è©•ä¼°æª¢ç´¢æ€§èƒ½"""
        all_metrics = []
        verbose_mode = eval_params.get('verbose_mode', False)
        
        for i, test_case in enumerate(test_cases):
            question = test_case.get('question') or test_case.get('user_query')
            expected_doc_ids = test_case.get('expected_relevant_doc_ids', [])
            
            if not question or not expected_doc_ids:
                continue
            
            search_results = await self._search_vectors_via_api(
                question, eval_params['top_k'], eval_params['similarity_threshold'], 
                'rrf_fusion', True, rrf_weights, rrf_k_constant
            )
            retrieved_doc_ids = [doc.get('document_id', '') for doc in search_results]

            metrics = {
                "hit_rate": self._calculate_hit_rate(expected_doc_ids, retrieved_doc_ids),
                "mrr": self._calculate_mrr(expected_doc_ids, retrieved_doc_ids),
                "ndcg": self._calculate_ndcg(expected_doc_ids, retrieved_doc_ids)
            }
            all_metrics.append(metrics)
            
            if verbose_mode:
                hit_found = "âœ…" if set(expected_doc_ids) & set(retrieved_doc_ids) else "âŒ"
                weights_str = f"S:{rrf_weights['summary']:.1f}/C:{rrf_weights['chunks']:.1f}/k:{rrf_k_constant}"
                print(f"  [{weights_str}] æ¡ˆä¾‹ {i+1:2d}: {hit_found} MRR={metrics['mrr']:.3f}")
        
        return self._aggregate_results(all_metrics, len(test_cases), eval_params, 'rrf_fusion')

    async def optimize_rrf_weights_and_k(self, test_cases: List[Dict], eval_params: Dict) -> Dict[str, Any]:
        """
        ğŸ¯ RRF æ¬Šé‡+Kå€¼ è¯åˆèª¿å„ªï¼šå››æ­¥èµ°ç­–ç•¥
        
        æ­¥é©Ÿä¸€ï¼šå…¬å¹³çš„èµ·é» (1.0, 1.0, k=60)
        æ­¥é©ŸäºŒï¼šKå€¼æ•æ„Ÿæ€§åˆ†æ
        æ­¥é©Ÿä¸‰ï¼šåŸºæ–¼å‡è¨­çš„æ¬Šé‡å¾®èª¿
        æ­¥é©Ÿå››ï¼šè¯åˆç¶²æ ¼æœç´¢å„ªåŒ–
        """
        logger.info("ğŸš€ é–‹å§‹ RRF æ¬Šé‡+Kå€¼ è¯åˆèª¿å„ª - å››æ­¥èµ°ç­–ç•¥")
        
        # å®šç¾©æ¸¬è©¦ç¯„åœ
        k_values_to_test = [20, 30, 45, 60, 80, 100, 120]  # Kå€¼æ¸¬è©¦ç¯„åœ
        optimization_results = {}
        
        # === æ­¥é©Ÿä¸€ï¼šå…¬å¹³çš„èµ·é» ===
        logger.info("\n" + "="*70)
        logger.info("ğŸ“Š æ­¥é©Ÿä¸€ï¼šå…¬å¹³çš„èµ·é» (The Baseline)")
        logger.info("="*70)
        
        baseline_weights = {"summary": 1.0, "chunks": 1.0}
        baseline_k = 60
        baseline_result = await self._evaluate_rrf_with_weights(
            test_cases, eval_params, baseline_weights, baseline_k, description="å…¬å¹³èµ·é» (1.0,1.0,k=60)"
        )
        optimization_results["baseline"] = {
            "weights": baseline_weights,
            "k_constant": baseline_k,
            "results": baseline_result,
            "description": "å…¬å¹³çš„èµ·é» - ç„¡åè¦‹çš„åŸºæº–ç·š"
        }
        
        baseline_mrr = baseline_result.get("retrieval_metrics", {}).get("mrr", 0.0)
        logger.info(f"âœ… åŸºæº–ç·š MRR (1.0,1.0,k=60): {baseline_mrr:.4f}")
        
        # === æ­¥é©ŸäºŒï¼šKå€¼æ•æ„Ÿæ€§åˆ†æ ===
        logger.info("\n" + "="*70)
        logger.info("ğŸ”¬ æ­¥é©ŸäºŒï¼šKå€¼æ•æ„Ÿæ€§åˆ†æ (K-Value Sensitivity)")
        logger.info("="*70)
        
        best_k_mrr = baseline_mrr
        best_k_value = baseline_k
        k_analysis_results = {}
        
        logger.info(f"ğŸ§ª æ¸¬è©¦ {len(k_values_to_test)} å€‹ä¸åŒçš„ K å€¼")
        
        for i, k_val in enumerate(k_values_to_test):
            if k_val == baseline_k:  # è·³éåŸºæº–ç·š
                continue
                
            logger.info(f"ğŸ” Kå€¼æ¸¬è©¦ {i+1}/{len(k_values_to_test)}: k={k_val}")
            
            result = await self._evaluate_rrf_with_weights(
                test_cases, eval_params, baseline_weights, k_val, 
                description=f"Kå€¼æ•æ„Ÿæ€§æ¸¬è©¦ k={k_val}"
            )
            
            optimization_results[f"k_test_{k_val}"] = {
                "weights": baseline_weights,
                "k_constant": k_val,
                "results": result,
                "description": f"Kå€¼æ¸¬è©¦ k={k_val}"
            }
            
            mrr_score = result.get("retrieval_metrics", {}).get("mrr", 0.0)
            improvement = ((mrr_score - baseline_mrr) / max(baseline_mrr, 0.001)) * 100
            k_analysis_results[k_val] = {"mrr": mrr_score, "improvement": improvement}
            
            logger.info(f"   MRR: {mrr_score:.4f} (ç›¸å°åŸºæº– {improvement:+.1f}%)")
            
            if mrr_score > best_k_mrr:
                best_k_mrr = mrr_score
                best_k_value = k_val
        
        logger.info(f"âœ¨ Kå€¼åˆ†æå®Œæˆï¼Œæœ€ä½³ k={best_k_value}, MRR={best_k_mrr:.4f}")
        
        # === æ­¥é©Ÿä¸‰ï¼šåŸºæ–¼å‡è¨­çš„æ¬Šé‡å¾®èª¿ ===
        logger.info("\n" + "="*70)
        logger.info("ğŸ§  æ­¥é©Ÿä¸‰ï¼šåŸºæ–¼å‡è¨­çš„æ¬Šé‡å¾®èª¿ (Hypothesis-Driven)")
        logger.info("="*70)
        
        hypothesis_configs = [
            # å‡è¨­Aï¼šæ‘˜è¦ç‚ºç‹ (ä½¿ç”¨æœ€ä½³Kå€¼)
            {"summary": 1.2, "chunks": 1.0, "desc": "è¼•å¾®åé‡æ‘˜è¦"},
            {"summary": 1.5, "chunks": 1.0, "desc": "æ˜é¡¯åé‡æ‘˜è¦"},
            {"summary": 2.0, "chunks": 1.0, "desc": "å¼·çƒˆåé‡æ‘˜è¦"},
            
            # å‡è¨­Bï¼šç´°ç¯€ç‚ºç‹ (ä½¿ç”¨æœ€ä½³Kå€¼)
            {"summary": 1.0, "chunks": 1.2, "desc": "è¼•å¾®åé‡å…§å®¹å¡Š"},
            {"summary": 1.0, "chunks": 1.5, "desc": "æ˜é¡¯åé‡å…§å®¹å¡Š"},
            {"summary": 1.0, "chunks": 2.0, "desc": "å¼·çƒˆåé‡å…§å®¹å¡Š"},
            
            # å‡è¨­Cï¼šå¹³è¡¡ä½†æœ‰å¾®èª¿
            {"summary": 0.8, "chunks": 1.0, "desc": "è¼•å¾®æ‡²ç½°æ‘˜è¦"},
            {"summary": 1.0, "chunks": 0.8, "desc": "è¼•å¾®æ‡²ç½°å…§å®¹å¡Š"},
            {"summary": 1.3, "chunks": 1.3, "desc": "åŒæ­¥æå‡"},
        ]
        
        best_hypothesis = None
        best_hypothesis_mrr = best_k_mrr
        best_hypothesis_k = best_k_value
        
        logger.info(f"ğŸ§ª ä½¿ç”¨æœ€ä½³ k={best_k_value} æ¸¬è©¦ {len(hypothesis_configs)} å€‹å‡è¨­")
        
        for i, config in enumerate(hypothesis_configs):
            weights = {"summary": config["summary"], "chunks": config["chunks"]}
            logger.info(f"ğŸ§ª æ¸¬è©¦å‡è¨­ {i+1}/{len(hypothesis_configs)}: {config['desc']} {weights} (k={best_k_value})")
            
            result = await self._evaluate_rrf_with_weights(
                test_cases, eval_params, weights, best_k_value, description=config['desc']
            )
            
            optimization_results[f"hypothesis_{i+1}"] = {
                "weights": weights,
                "k_constant": best_k_value,
                "results": result,
                "description": config['desc']
            }
            
            mrr_score = result.get("retrieval_metrics", {}).get("mrr", 0.0)
            improvement = ((mrr_score - baseline_mrr) / max(baseline_mrr, 0.001)) * 100
            
            logger.info(f"   MRR: {mrr_score:.4f} (ç›¸å°åŸºæº– {improvement:+.1f}%)")
            
            if mrr_score > best_hypothesis_mrr:
                best_hypothesis = {**config, "k": best_k_value}
                best_hypothesis_mrr = mrr_score
                best_hypothesis_k = best_k_value
        
        logger.info(f"âœ¨ æœ€ä½³å‡è¨­: {best_hypothesis['desc'] if best_hypothesis else 'åŸºæº–ç·š'}")
        logger.info(f"   MRR: {best_hypothesis_mrr:.4f}, k={best_hypothesis_k}")
        
        # === æ­¥é©Ÿå››ï¼šè¯åˆç¶²æ ¼æœç´¢å„ªåŒ– ===
        logger.info("\n" + "="*70)
        logger.info("ğŸ”¬ æ­¥é©Ÿå››ï¼šè¯åˆç¶²æ ¼æœç´¢ (Joint Grid Search)")
        logger.info("="*70)
        
        # åŸºæ–¼å‰é¢çš„çµæœé¸æ“‡ç²¾ç´°æœç´¢ç¯„åœ
        if best_hypothesis:
            base_summary = best_hypothesis["summary"]
            base_chunks = best_hypothesis["chunks"]
            fine_k_values = self._get_fine_k_range(best_k_value)
        else:
            base_summary = 1.0
            base_chunks = 1.0
            fine_k_values = [best_k_value - 10, best_k_value, best_k_value + 10]
        
        # ç”Ÿæˆè¯åˆæœç´¢é…ç½®
        joint_configs = []
        for k_val in fine_k_values:
            if k_val <= 0:
                continue
            for s_delta in [-0.2, -0.1, 0, 0.1, 0.2]:
                for c_delta in [-0.2, -0.1, 0, 0.1, 0.2]:
                    new_summary = max(0.1, base_summary + s_delta)
                    new_chunks = max(0.1, base_chunks + c_delta)
                    joint_configs.append({
                        "summary": round(new_summary, 1),
                        "chunks": round(new_chunks, 1),
                        "k": k_val,
                        "desc": f"è¯åˆæœç´¢ S:{new_summary:.1f} C:{new_chunks:.1f} k:{k_val}"
                    })
        
        # å»é‡ä¸¦é™åˆ¶æ•¸é‡
        unique_joint_configs = []
        seen_configs = set()
        for config in joint_configs:
            config_key = (config["summary"], config["chunks"], config["k"])
            baseline_key = (1.0, 1.0, 60)
            if config_key not in seen_configs and config_key != baseline_key:
                seen_configs.add(config_key)
                unique_joint_configs.append(config)
                if len(unique_joint_configs) >= 15:  # é™åˆ¶æœç´¢æ•¸é‡
                    break
        
        best_joint_mrr = best_hypothesis_mrr
        best_joint_config = None
        
        logger.info(f"ğŸ” å°‡æ¸¬è©¦ {len(unique_joint_configs)} å€‹è¯åˆé…ç½®")
        
        for i, config in enumerate(unique_joint_configs):
            weights = {"summary": config["summary"], "chunks": config["chunks"]}
            logger.info(f"ğŸ”¬ è¯åˆæœç´¢ {i+1}/{len(unique_joint_configs)}: {weights} k={config['k']}")
            
            result = await self._evaluate_rrf_with_weights(
                test_cases, eval_params, weights, config['k'], description=config['desc']
            )
            
            optimization_results[f"joint_{i+1}"] = {
                "weights": weights,
                "k_constant": config['k'],
                "results": result,
                "description": config['desc']
            }
            
            mrr_score = result.get("retrieval_metrics", {}).get("mrr", 0.0)
            improvement = ((mrr_score - baseline_mrr) / max(baseline_mrr, 0.001)) * 100
            
            logger.info(f"   MRR: {mrr_score:.4f} (ç›¸å°åŸºæº– {improvement:+.1f}%)")
            
            if mrr_score > best_joint_mrr:
                best_joint_config = config
                best_joint_mrr = mrr_score
        
        # ç”Ÿæˆæœ€çµ‚è¯åˆå„ªåŒ–å ±å‘Š
        joint_summary = self._generate_joint_optimization_summary(
            optimization_results, baseline_mrr, best_joint_mrr, best_joint_config,
            k_analysis_results, best_k_value
        )
        
        return {
            "optimization_type": "rrf_weight_and_k_joint_optimization",
            "baseline_mrr": baseline_mrr,
            "best_mrr": best_joint_mrr,
            "improvement_percentage": ((best_joint_mrr - baseline_mrr) / max(baseline_mrr, 0.001)) * 100,
            "best_configuration": best_joint_config,
            "best_k_from_sensitivity": best_k_value,
            "k_sensitivity_analysis": k_analysis_results,
            "all_results": optimization_results,
            "joint_optimization_summary": joint_summary,
            "evaluation_parameters": eval_params
        }

    def _get_fine_k_range(self, best_k: int) -> List[int]:
        """åŸºæ–¼æœ€ä½³Kå€¼ç”Ÿæˆç²¾ç´°æœç´¢ç¯„åœ"""
        fine_range = []
        
        # åœ¨æœ€ä½³Kå€¼å‘¨åœç”Ÿæˆç²¾ç´°ç¯„åœ
        for delta in [-15, -10, -5, 0, 5, 10, 15]:
            new_k = best_k + delta
            if 10 <= new_k <= 150:  # åˆç†çš„Kå€¼ç¯„åœ
                fine_range.append(new_k)
        
        return sorted(list(set(fine_range)))

    def _generate_joint_optimization_summary(self, all_results: Dict, baseline_mrr: float, 
                                           best_mrr: float, best_config: Optional[Dict],
                                           k_analysis: Dict, best_k: int) -> Dict[str, Any]:
        """ç”Ÿæˆè¯åˆå„ªåŒ–ç¸½çµ"""
        
        summary = {
            "baseline_performance": baseline_mrr,
            "best_performance": best_mrr,
            "total_improvement": ((best_mrr - baseline_mrr) / max(baseline_mrr, 0.001)) * 100,
            "best_weights": {"summary": best_config["summary"], "chunks": best_config["chunks"]} if best_config else {"summary": 1.0, "chunks": 1.0},
            "best_k_constant": best_config["k"] if best_config else 60,
            "optimization_stages": {
                "stage1_baseline": {"mrr": baseline_mrr, "description": "å…¬å¹³èµ·é» (1.0, 1.0, k=60)"},
                "stage2_k_sensitivity": self._analyze_k_sensitivity(k_analysis, baseline_mrr, best_k),
                "stage3_hypothesis": self._analyze_hypothesis_stage_with_k(all_results, baseline_mrr),
                "stage4_joint_search": self._analyze_joint_stage(all_results, baseline_mrr)
            }
        }
        
        # ç”Ÿæˆæ™ºèƒ½å»ºè­°
        total_improvement = summary["total_improvement"]
        k_improvement = k_analysis.get(best_k, {}).get("improvement", 0)
        
        if total_improvement > 20:
            summary["recommendation"] = f"ğŸš€ å¼·çƒˆå»ºè­°æ¡ç”¨è¯åˆå„ªåŒ–é…ç½®ï¼æ€§èƒ½æå‡ {total_improvement:.1f}%"
        elif total_improvement > 10:
            summary["recommendation"] = f"âœ… å»ºè­°æ¡ç”¨è¯åˆå„ªåŒ–é…ç½®ï¼Œæœ‰é¡¯è‘—æ”¹é€² {total_improvement:.1f}%"
        elif k_improvement > 5:
            summary["recommendation"] = f"ğŸ’¡ å»ºè­°è‡³å°‘èª¿æ•´Kå€¼åˆ° {best_k}ï¼ŒKå€¼å„ªåŒ–å¸¶ä¾† {k_improvement:.1f}% æ”¹é€²"
        elif total_improvement > 0:
            summary["recommendation"] = f"ğŸ’¡ è¯åˆå„ªåŒ–æœ‰è¼•å¾®æ”¹é€² {total_improvement:.1f}%ï¼Œå¯æ ¹æ“šå¯¦éš›æƒ…æ³é¸æ“‡"
        else:
            summary["recommendation"] = "âš ï¸ é è¨­é…ç½® (1.0, 1.0, k=60) å·²ç¶“æ˜¯è¼ƒä½³é¸æ“‡"
        
        return summary

    def _analyze_k_sensitivity(self, k_analysis: Dict, baseline_mrr: float, best_k: int) -> Dict[str, Any]:
        """åˆ†æKå€¼æ•æ„Ÿæ€§çµæœ"""
        if not k_analysis:
            return {"best_k": 60, "improvement": 0.0, "sensitivity": "ç„¡æ•¸æ“š"}
        
        best_k_data = k_analysis.get(best_k, {})
        
        # è¨ˆç®—Kå€¼æ•æ„Ÿæ€§ç¨‹åº¦
        improvements = [data.get("improvement", 0) for data in k_analysis.values()]
        max_improvement = max(improvements) if improvements else 0
        min_improvement = min(improvements) if improvements else 0
        sensitivity_range = max_improvement - min_improvement
        
        if sensitivity_range > 10:
            sensitivity_level = "é«˜æ•æ„Ÿæ€§"
        elif sensitivity_range > 5:
            sensitivity_level = "ä¸­ç­‰æ•æ„Ÿæ€§"
        else:
            sensitivity_level = "ä½æ•æ„Ÿæ€§"
        
        return {
            "best_k": best_k,
            "best_mrr": best_k_data.get("mrr", baseline_mrr),
            "improvement": best_k_data.get("improvement", 0.0),
            "sensitivity_level": sensitivity_level,
            "sensitivity_range": sensitivity_range,
            "tested_k_values": len(k_analysis)
        }

    def _analyze_hypothesis_stage_with_k(self, all_results: Dict, baseline_mrr: float) -> Dict[str, Any]:
        """åˆ†æå‡è¨­éšæ®µçš„çµæœï¼ˆåŒ…å«Kå€¼ä¿¡æ¯ï¼‰"""
        hypothesis_results = {k: v for k, v in all_results.items() if k.startswith('hypothesis_')}
        
        if not hypothesis_results:
            return {"best_mrr": baseline_mrr, "improvement": 0.0, "best_hypothesis": "ç„¡"}
        
        best_hyp = max(hypothesis_results.items(), 
                      key=lambda x: x[1]["results"].get("retrieval_metrics", {}).get("mrr", 0.0))
        
        best_mrr = best_hyp[1]["results"].get("retrieval_metrics", {}).get("mrr", 0.0)
        improvement = ((best_mrr - baseline_mrr) / max(baseline_mrr, 0.001)) * 100
        
        return {
            "best_mrr": best_mrr,
            "improvement": improvement,
            "best_hypothesis": best_hyp[1]["description"],
            "best_weights": best_hyp[1]["weights"],
            "best_k": best_hyp[1]["k_constant"]
        }

    def _analyze_joint_stage(self, all_results: Dict, baseline_mrr: float) -> Dict[str, Any]:
        """åˆ†æè¯åˆæœç´¢éšæ®µçš„çµæœ"""
        joint_results = {k: v for k, v in all_results.items() if k.startswith('joint_')}
        
        if not joint_results:
            return {"best_mrr": baseline_mrr, "improvement": 0.0, "configurations_tested": 0}
        
        best_joint = max(joint_results.items(), 
                        key=lambda x: x[1]["results"].get("retrieval_metrics", {}).get("mrr", 0.0))
        
        best_mrr = best_joint[1]["results"].get("retrieval_metrics", {}).get("mrr", 0.0)
        improvement = ((best_mrr - baseline_mrr) / max(baseline_mrr, 0.001)) * 100
        
        return {
            "best_mrr": best_mrr,
            "improvement": improvement,
            "configurations_tested": len(joint_results),
            "best_weights": best_joint[1]["weights"],
            "best_k": best_joint[1]["k_constant"]
        }

    async def run_joint_optimization_flow(self, dataset_path: str, eval_params: Dict):
        """åŸ·è¡Œ RRF æ¬Šé‡+Kå€¼ è¯åˆèª¿å„ªæµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹ RRF æ¬Šé‡+Kå€¼ è¯åˆèª¿å„ªæµç¨‹ - å››æ­¥èµ°ç­–ç•¥")
        
        if not os.path.isabs(dataset_path):
            dataset_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), dataset_path)
        logger.info(f"ä½¿ç”¨è³‡æ–™é›†: {dataset_path}")

        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
            logger.info(f"æˆåŠŸè¼‰å…¥ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¸¬è©¦æ•¸æ“šå¤±æ•—: {e}")
            return
        
        try:
            await self.initialize_services()
            
            # åŸ·è¡Œè¯åˆèª¿å„ª
            optimization_results = await self.optimize_rrf_weights_and_k(test_cases, eval_params)
            
            # è¼¸å‡ºèª¿å„ªçµæœ
            self.print_joint_optimization_results(optimization_results)
            
            # ä¿å­˜è©³ç´°çµæœ
            output_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "rrf_joint_optimization_results.json")
            self.save_results_to_json(optimization_results, output_path)
            
        finally:
            if self.session and not self.session.closed:
                await self.session.close()
                logger.info("HTTPæœƒè©±å·²é—œé–‰")

    def print_joint_optimization_results(self, results: Dict[str, Any]):
        """è¼¸å‡ºè¯åˆå„ªåŒ–çµæœï¼ˆæ¬Šé‡+Kå€¼ï¼‰"""
        print("\n" + "ğŸ”¥" + "="*90)
        print("ğŸš€ RRF æ¬Šé‡+Kå€¼ è¯åˆèª¿å„ªçµæœ - å››æ­¥èµ°ç­–ç•¥")
        print("ğŸ”¥" + "="*90)
        
        baseline_mrr = results.get("baseline_mrr", 0.0)
        best_mrr = results.get("best_mrr", 0.0)
        improvement = results.get("improvement_percentage", 0.0)
        best_config = results.get("best_configuration", {})
        best_k_sensitivity = results.get("best_k_from_sensitivity", 60)
        k_analysis = results.get("k_sensitivity_analysis", {})
        
        print(f"\nğŸ“Š æ•´é«”è¯åˆå„ªåŒ–çµæœ:")
        print(f"   ğŸ¯ åŸºæº–ç·š MRR (1.0, 1.0, k=60): {baseline_mrr:.4f}")
        print(f"   ğŸ† æœ€ä½³ MRR: {best_mrr:.4f}")
        print(f"   ğŸ“ˆ ç¸½æ€§èƒ½æå‡: {improvement:+.2f}%")
        
        if best_config:
            print(f"   ğŸ¯ æœ€ä½³è¯åˆé…ç½®:")
            print(f"      - æ‘˜è¦æ¬Šé‡: {best_config.get('summary', 1.0):.2f}")
            print(f"      - å…§å®¹å¡Šæ¬Šé‡: {best_config.get('chunks', 1.0):.2f}")
            print(f"      - Kå¸¸æ•¸: {best_config.get('k', 60)}")
            print(f"      - é…ç½®æè¿°: {best_config.get('desc', 'æœªçŸ¥é…ç½®')}")
        
        # é¡¯ç¤ºå„éšæ®µçµæœ
        summary = results.get("joint_optimization_summary", {})
        stages = summary.get("optimization_stages", {})
        
        print(f"\nğŸ“ˆ å››æ­¥è¯åˆèª¿å„ªè©³ç´°çµæœ:")
        
        # æ­¥é©Ÿä¸€ï¼šåŸºæº–ç·š
        stage1 = stages.get("stage1_baseline", {})
        print(f"   ğŸ“Š æ­¥é©Ÿä¸€ - å…¬å¹³èµ·é»:")
        print(f"      MRR: {stage1.get('mrr', 0.0):.4f} | {stage1.get('description', 'æœªçŸ¥')}")
        
        # æ­¥é©ŸäºŒï¼šKå€¼æ•æ„Ÿæ€§åˆ†æ
        stage2 = stages.get("stage2_k_sensitivity", {})
        if stage2.get("tested_k_values", 0) > 0:
            print(f"   ğŸ”¬ æ­¥é©ŸäºŒ - Kå€¼æ•æ„Ÿæ€§åˆ†æ:")
            print(f"      æ¸¬è©¦Kå€¼æ•¸é‡: {stage2.get('tested_k_values', 0)}")
            print(f"      æœ€ä½³Kå€¼: {stage2.get('best_k', 60)}")
            print(f"      æœ€ä½³Kå€¼MRR: {stage2.get('best_mrr', 0.0):.4f} (æ”¹é€² {stage2.get('improvement', 0.0):+.1f}%)")
            print(f"      Kå€¼æ•æ„Ÿæ€§: {stage2.get('sensitivity_level', 'æœªçŸ¥')}")
            print(f"      æ•æ„Ÿæ€§ç¯„åœ: {stage2.get('sensitivity_range', 0.0):.1f}%")
        
        # æ­¥é©Ÿä¸‰ï¼šå‡è¨­é©…å‹•ï¼ˆåŸºæ–¼æœ€ä½³Kå€¼ï¼‰
        stage3 = stages.get("stage3_hypothesis", {})
        if stage3.get("best_mrr", 0.0) > 0:
            print(f"   ğŸ§  æ­¥é©Ÿä¸‰ - å‡è¨­é©…å‹•å¾®èª¿ (ä½¿ç”¨æœ€ä½³Kå€¼):")
            print(f"      æœ€ä½³ MRR: {stage3.get('best_mrr', 0.0):.4f} (æ”¹é€² {stage3.get('improvement', 0.0):+.1f}%)")
            print(f"      æœ€ä½³å‡è¨­: {stage3.get('best_hypothesis', 'æœªçŸ¥')}")
            best_hyp_weights = stage3.get("best_weights", {})
            print(f"      æœ€ä½³æ¬Šé‡: æ‘˜è¦ {best_hyp_weights.get('summary', 1.0):.2f}, å…§å®¹å¡Š {best_hyp_weights.get('chunks', 1.0):.2f}")
            print(f"      ä½¿ç”¨Kå€¼: {stage3.get('best_k', 60)}")
        
        # æ­¥é©Ÿå››ï¼šè¯åˆç¶²æ ¼æœç´¢
        stage4 = stages.get("stage4_joint_search", {})
        if stage4.get("configurations_tested", 0) > 0:
            print(f"   ğŸ”¬ æ­¥é©Ÿå›› - è¯åˆç¶²æ ¼æœç´¢:")
            print(f"      æ¸¬è©¦é…ç½®æ•¸: {stage4.get('configurations_tested', 0)}")
            print(f"      æœ€ä½³ MRR: {stage4.get('best_mrr', 0.0):.4f} (æ”¹é€² {stage4.get('improvement', 0.0):+.1f}%)")
            best_joint_weights = stage4.get("best_weights", {})
            print(f"      æœ€ä½³æ¬Šé‡: æ‘˜è¦ {best_joint_weights.get('summary', 1.0):.2f}, å…§å®¹å¡Š {best_joint_weights.get('chunks', 1.0):.2f}")
            print(f"      æœ€ä½³Kå€¼: {stage4.get('best_k', 60)}")
        
        # Kå€¼åˆ†æè©³æƒ…
        if k_analysis:
            print(f"\nğŸ” Kå€¼æ•æ„Ÿæ€§è©³ç´°åˆ†æ:")
            for k_val, k_data in sorted(k_analysis.items()):
                mrr_val = k_data.get("mrr", 0.0)
                improvement_val = k_data.get("improvement", 0.0)
                marker = "ğŸ†" if k_val == best_k_sensitivity else "   "
                print(f"      {marker} k={k_val}: MRR={mrr_val:.4f} (æ”¹é€² {improvement_val:+.1f}%)")
        
        # æ™ºèƒ½å»ºè­°
        recommendation = summary.get("recommendation", "ç„¡å»ºè­°")
        print(f"\nğŸ’¡ æ™ºèƒ½å»ºè­°:")
        print(f"   {recommendation}")
        
        # å¯¦ç”¨é…ç½®è¼¸å‡º
        if best_config:
            print(f"\nğŸ”§ ç”Ÿç”¢ç’°å¢ƒè¯åˆé…ç½®å»ºè­°:")
            print(f"   # åœ¨ backend/app/core/config.py ä¸­è¨­ç½®ï¼š")
            print(f"   RRF_WEIGHTS = {{'summary': {best_config.get('summary', 1.0):.2f}, 'chunks': {best_config.get('chunks', 1.0):.2f}}}")
            print(f"   RRF_K_CONSTANT = {best_config.get('k', 60)}  # ç¶“éè¯åˆå„ªåŒ–çš„Kå€¼")
        
        print("ğŸ”¥" + "="*90 + "\n")

    async def optimize_rrf_weights(self, test_cases: List[Dict], eval_params: Dict) -> Dict[str, Any]:
        """
        ğŸ¯ RRF æ¬Šé‡èª¿å„ªï¼šä¸‰æ­¥èµ°ç­–ç•¥ï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰
        
        æ­¥é©Ÿä¸€ï¼šå…¬å¹³çš„èµ·é» (1.0, 1.0)
        æ­¥é©ŸäºŒï¼šåŸºæ–¼å‡è¨­çš„å¾®èª¿
        æ­¥é©Ÿä¸‰ï¼šæ•¸æ“šé©…å‹•çš„ç¶²æ ¼æœç´¢
        """
        logger.info("ğŸš€ é–‹å§‹ RRF æ¬Šé‡èª¿å„ª - ä¸‰æ­¥èµ°ç­–ç•¥ï¼ˆç¶“å…¸ç‰ˆæœ¬ï¼‰")
        
        # å®šç¾©æ¬Šé‡ç¶²æ ¼ - ä¸‰æ­¥ç­–ç•¥
        weight_optimization_results = {}
        default_k = 60  # å›ºå®šKå€¼
        
        # === æ­¥é©Ÿä¸€ï¼šå…¬å¹³çš„èµ·é» ===
        logger.info("\n" + "="*70)
        logger.info("ğŸ“Š æ­¥é©Ÿä¸€ï¼šå…¬å¹³çš„èµ·é»")
        logger.info("="*70)
        
        baseline_weights = {"summary": 1.0, "chunks": 1.0}
        baseline_result = await self._evaluate_rrf_with_weights(
            test_cases, eval_params, baseline_weights, default_k, "å…¬å¹³èµ·é» (1.0,1.0)"
        )
        weight_optimization_results["baseline"] = {
            "weights": baseline_weights,
            "results": baseline_result,
            "description": "å…¬å¹³çš„èµ·é»"
        }
        
        baseline_mrr = baseline_result.get("retrieval_metrics", {}).get("mrr", 0.0)
        logger.info(f"âœ… åŸºæº–ç·š MRR (1.0,1.0): {baseline_mrr:.4f}")
        
        # === æ­¥é©ŸäºŒï¼šåŸºæ–¼å‡è¨­çš„å¾®èª¿ ===
        logger.info("\n" + "="*70)
        logger.info("ğŸ§  æ­¥é©ŸäºŒï¼šåŸºæ–¼å‡è¨­çš„å¾®èª¿")
        logger.info("="*70)
        
        hypothesis_configs = [
            {"summary": 1.2, "chunks": 1.0, "desc": "è¼•å¾®åé‡æ‘˜è¦"},
            {"summary": 1.5, "chunks": 1.0, "desc": "æ˜é¡¯åé‡æ‘˜è¦"},
            {"summary": 2.0, "chunks": 1.0, "desc": "å¼·çƒˆåé‡æ‘˜è¦"},
            {"summary": 1.0, "chunks": 1.2, "desc": "è¼•å¾®åé‡å…§å®¹å¡Š"},
            {"summary": 1.0, "chunks": 1.5, "desc": "æ˜é¡¯åé‡å…§å®¹å¡Š"},
            {"summary": 1.0, "chunks": 2.0, "desc": "å¼·çƒˆåé‡å…§å®¹å¡Š"},
            {"summary": 0.8, "chunks": 1.0, "desc": "è¼•å¾®æ‡²ç½°æ‘˜è¦"},
            {"summary": 1.0, "chunks": 0.8, "desc": "è¼•å¾®æ‡²ç½°å…§å®¹å¡Š"},
        ]
        
        best_hypothesis_mrr = baseline_mrr
        
        for i, config in enumerate(hypothesis_configs):
            weights = {"summary": config["summary"], "chunks": config["chunks"]}
            logger.info(f"ğŸ§ª æ¸¬è©¦å‡è¨­ {i+1}: {config['desc']} {weights}")
            
            result = await self._evaluate_rrf_with_weights(
                test_cases, eval_params, weights, default_k, config['desc']
            )
            
            weight_optimization_results[f"hypothesis_{i+1}"] = {
                "weights": weights,
                "results": result,
                "description": config['desc']
            }
            
            mrr_score = result.get("retrieval_metrics", {}).get("mrr", 0.0)
            improvement = ((mrr_score - baseline_mrr) / max(baseline_mrr, 0.001)) * 100
            
            logger.info(f"   MRR: {mrr_score:.4f} (æ”¹é€² {improvement:+.1f}%)")
            
            if mrr_score > best_hypothesis_mrr:
                best_hypothesis_mrr = mrr_score
        
        # === æ­¥é©Ÿä¸‰ï¼šç¶²æ ¼æœç´¢ ===
        logger.info("\n" + "="*70)
        logger.info("ğŸ”¬ æ­¥é©Ÿä¸‰ï¼šç¶²æ ¼æœç´¢")
        logger.info("="*70)
        
        grid_configs = []
        for s_weight in [0.5, 0.8, 1.0, 1.2, 1.5, 2.0]:
            for c_weight in [0.5, 0.8, 1.0, 1.2, 1.5, 2.0]:
                if s_weight == 1.0 and c_weight == 1.0:  # è·³éåŸºæº–ç·š
                    continue
                grid_configs.append({
                    "summary": s_weight,
                    "chunks": c_weight,
                    "desc": f"ç¶²æ ¼æœç´¢ S:{s_weight} C:{c_weight}"
                })
        
        # é™åˆ¶æœç´¢æ•¸é‡
        grid_configs = grid_configs[:15]
        best_grid_mrr = best_hypothesis_mrr
        best_config = None
        
        for i, config in enumerate(grid_configs):
            weights = {"summary": config["summary"], "chunks": config["chunks"]}
            logger.info(f"ğŸ”¬ ç¶²æ ¼æœç´¢ {i+1}/{len(grid_configs)}: {weights}")
            
            result = await self._evaluate_rrf_with_weights(
                test_cases, eval_params, weights, default_k, config['desc']
            )
            
            weight_optimization_results[f"grid_{i+1}"] = {
                "weights": weights,
                "results": result,
                "description": config['desc']
            }
            
            mrr_score = result.get("retrieval_metrics", {}).get("mrr", 0.0)
            improvement = ((mrr_score - baseline_mrr) / max(baseline_mrr, 0.001)) * 100
            
            logger.info(f"   MRR: {mrr_score:.4f} (æ”¹é€² {improvement:+.1f}%)")
            
            if mrr_score > best_grid_mrr:
                best_grid_mrr = mrr_score
                best_config = config
        
        return {
            "optimization_type": "rrf_weight_optimization",
            "baseline_mrr": baseline_mrr,
            "best_mrr": best_grid_mrr,
            "improvement_percentage": ((best_grid_mrr - baseline_mrr) / max(baseline_mrr, 0.001)) * 100,
            "best_configuration": best_config,
            "all_results": weight_optimization_results,
            "evaluation_parameters": eval_params
        }

    async def run_weight_optimization_flow(self, dataset_path: str, eval_params: Dict):
        """åŸ·è¡Œ RRF æ¬Šé‡èª¿å„ªæµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹ RRF æ¬Šé‡èª¿å„ªæµç¨‹")
        
        if not os.path.isabs(dataset_path):
            dataset_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), dataset_path)
        logger.info(f"ä½¿ç”¨è³‡æ–™é›†: {dataset_path}")

        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
            logger.info(f"æˆåŠŸè¼‰å…¥ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¸¬è©¦æ•¸æ“šå¤±æ•—: {e}")
            return
        
        try:
            await self.initialize_services()
            
            # åŸ·è¡Œæ¬Šé‡èª¿å„ª
            optimization_results = await self.optimize_rrf_weights(test_cases, eval_params)
            
            # è¼¸å‡ºèª¿å„ªçµæœ
            self.print_weight_optimization_results(optimization_results)
            
            # ä¿å­˜è©³ç´°çµæœ
            output_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "rrf_weight_optimization_results.json")
            self.save_results_to_json(optimization_results, output_path)
            
        finally:
            if self.session and not self.session.closed:
                await self.session.close()
                logger.info("HTTPæœƒè©±å·²é—œé–‰")

    def print_weight_optimization_results(self, results: Dict[str, Any]):
        """è¼¸å‡ºæ¬Šé‡èª¿å„ªçµæœ"""
        print("\n" + "ğŸ”¥" + "="*80)
        print("ğŸš€ RRF æ¬Šé‡èª¿å„ªçµæœ")
        print("ğŸ”¥" + "="*80)
        
        baseline_mrr = results.get("baseline_mrr", 0.0)
        best_mrr = results.get("best_mrr", 0.0)
        improvement = results.get("improvement_percentage", 0.0)
        best_config = results.get("best_configuration", {})
        
        print(f"\nğŸ“Š æ¬Šé‡èª¿å„ªçµæœ:")
        print(f"   ğŸ¯ åŸºæº–ç·š MRR (1.0, 1.0): {baseline_mrr:.4f}")
        print(f"   ğŸ† æœ€ä½³ MRR: {best_mrr:.4f}")
        print(f"   ğŸ“ˆ æ€§èƒ½æå‡: {improvement:+.2f}%")
        
        if best_config:
            print(f"   ğŸ¯ æœ€ä½³æ¬Šé‡é…ç½®:")
            print(f"      - æ‘˜è¦æ¬Šé‡: {best_config.get('summary', 1.0):.2f}")
            print(f"      - å…§å®¹å¡Šæ¬Šé‡: {best_config.get('chunks', 1.0):.2f}")
            print(f"      - é…ç½®æè¿°: {best_config.get('desc', 'æœªçŸ¥é…ç½®')}")
        
        # å»ºè­°
        if improvement > 10:
            recommendation = "ğŸš€ å¼·çƒˆå»ºè­°æ¡ç”¨èª¿å„ªå¾Œçš„æ¬Šé‡é…ç½®ï¼"
        elif improvement > 5:
            recommendation = "âœ… å»ºè­°æ¡ç”¨èª¿å„ªå¾Œçš„æ¬Šé‡é…ç½®"
        elif improvement > 0:
            recommendation = "ğŸ’¡ èª¿å„ªæœ‰è¼•å¾®æ”¹é€²ï¼Œå¯æ ¹æ“šå¯¦éš›æƒ…æ³é¸æ“‡"
        else:
            recommendation = "âš ï¸ é è¨­æ¬Šé‡ (1.0, 1.0) å·²ç¶“æ˜¯è¼ƒä½³é¸æ“‡"
        
        print(f"\nğŸ’¡ å»ºè­°: {recommendation}")
        print("ğŸ”¥" + "="*80 + "\n")

async def main():
    """ä¸»å‡½æ•¸ï¼Œè§£æåƒæ•¸ä¸¦å•Ÿå‹•è©•ä¼°"""
    parser = argparse.ArgumentParser(description='RRFèåˆæª¢ç´¢ vs å…©éšæ®µæ··åˆæª¢ç´¢ vs å‚³çµ±å‘é‡æª¢ç´¢ - å…¨é¢æ€§èƒ½å°æ¯”è©•ä¼°')
    parser.add_argument('--dataset', type=str, required=True, help='æ¸¬è©¦è³‡æ–™é›†æª”æ¡ˆè·¯å¾‘ (å¿…é ˆ)')
    parser.add_argument('--top-k', type=int, default=10, help='æª¢ç´¢çµæœæ•¸é‡ (é è¨­: 10)')
    parser.add_argument('--threshold', type=float, default=0.3, help='ç›¸ä¼¼åº¦é–¾å€¼ (é è¨­: 0.3)')
    parser.add_argument(
        '--mode', 
        default='compare', 
        choices=['compare', 'individual', 'optimize_weights', 'optimize_weights_k'],
        help='åŸ·è¡Œæ¨¡å¼ï¼šcompareï¼ˆå°æ¯”ï¼‰, individualï¼ˆå–®ä¸€è©•ä¼°ï¼‰, optimize_weightsï¼ˆæ¬Šé‡èª¿å„ªï¼‰, optimize_weights_kï¼ˆæ¬Šé‡+Kå€¼è¯åˆèª¿å„ªï¼‰'
    )
    
    # æ–°å¢ï¼šè©³ç´°è©•ä¼°é¸é …
    parser.add_argument('--verbose', action='store_true', help='é¡¯ç¤ºæ¯å€‹æ¸¬è©¦æ¡ˆä¾‹çš„è©³ç´°çµæœ')
    parser.add_argument('--save-detailed', action='store_true', help='ä¿å­˜è©³ç´°çš„å€‹åˆ¥æ¡ˆä¾‹åˆ†æçµæœ')
    
    args = parser.parse_args()
    
    eval_params = {
        "top_k": args.top_k,
        "similarity_threshold": args.threshold,
        "verbose_mode": args.verbose,
        "save_detailed_analysis": args.save_detailed
    }
    
    for var in ['USERNAME', 'PASSWORD', 'API_URL']:
        if not os.getenv(var):
            logger.error(f"ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸: {var}ï¼Œè«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½®ã€‚")
            return
            
    evaluator = VectorRetrievalEvaluator()
    try:
        if args.mode == 'optimize_weights':
            # æ¬Šé‡èª¿å„ªæ¨¡å¼
            await evaluator.run_weight_optimization_flow(
                dataset_path=args.dataset,
                eval_params=eval_params
            )
        elif args.mode == 'optimize_weights_k':
            # æ¬Šé‡+Kå€¼è¯åˆèª¿å„ªæ¨¡å¼
            await evaluator.run_joint_optimization_flow(
                dataset_path=args.dataset,
                eval_params=eval_params
            )
        elif args.mode == 'compare':
            # å¤šæ¨¡å¼å°æ¯”
            await evaluator.run_evaluation_flow(
                dataset_path=args.dataset,
                eval_params=eval_params,
                comparison_mode=True
            )
        elif args.mode == 'individual':
            # å–®ä¸€ç­–ç•¥è©•ä¼° (å‚³çµ±å‘é‡æª¢ç´¢)
            await evaluator.run_evaluation_flow(
                dataset_path=args.dataset,
                eval_params=eval_params,
                comparison_mode=False
            )
    except Exception as e:
        logger.error(f"è©•ä¼°è…³æœ¬åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)

if __name__ == "__main__":
    # <<< MODIFIED: ç§»é™¤äº† nest_asyncioï¼Œå› åœ¨æ¨™æº–è…³æœ¬ä¸­ asyncio.run() æ˜¯æ›´å¥½çš„é¸æ“‡
    try:
        asyncio.run(main())
        logger.info("ç¨‹å¼åŸ·è¡Œå®Œæˆï¼Œæ­£å¸¸é€€å‡ºã€‚")
    except SystemExit as e:
        if e.code != 0:
             logger.error("å› åƒæ•¸éŒ¯èª¤å°è‡´ç¨‹å¼é€€å‡ºã€‚")
    except Exception as e:
        logger.error(f"ç¨‹å¼åœ¨é ‚å±¤åŸ·è¡Œæ™‚å´©æ½°: {e}", exc_info=True)