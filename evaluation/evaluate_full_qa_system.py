import json
import asyncio
import logging
import sys
import argparse
import time
from typing import List, Dict, Optional, Any
import numpy as np
import pandas as pd
import aiohttp
from dotenv import load_dotenv
import os

# --- Foolproof .env loading ---
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
except NameError:
    script_dir = os.getcwd()

dotenv_path = os.path.join(script_dir, '.env')
if not os.path.exists(dotenv_path):
    project_root = os.path.dirname(script_dir)
    dotenv_path_alt = os.path.join(project_root, 'evaluation', '.env')
    if os.path.exists(dotenv_path_alt):
        dotenv_path = dotenv_path_alt
    else:
        dotenv_path_root = os.path.join(project_root, '.env')
        if os.path.exists(dotenv_path_root):
            dotenv_path = dotenv_path_root
        else:
            print(f"FATAL: '.env' file not found at various checked paths.")
            sys.exit(1)
load_dotenv(dotenv_path=dotenv_path, override=True)
print(f"Successfully loaded .env file from: {dotenv_path}")

# --- Ragas & LangChain Imports ---
try:
    from ragas import evaluate
    from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy, answer_correctness
    from datasets import Dataset
    use_google = os.getenv('USE_GOOGLE_MODELS', 'true').lower() == 'true'
    if use_google:
        from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
        google_api_key = os.getenv('GOOGLE_API_KEY')
        if not google_api_key: raise ValueError("è«‹è¨­ç½® GOOGLE_API_KEY")
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0, google_api_key=google_api_key)
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=google_api_key)
    else:
        from langchain_openai import ChatOpenAI, OpenAIEmbeddings
        llm = ChatOpenAI(model="gpt-4", temperature=0)
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
except ImportError as e:
    print(f"Critical Import Error: {e}. Please ensure dependencies are correctly installed.", file=sys.stderr)
    sys.exit(1)

# --- Logging & Other Setups ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    from api_rate_limiter import APIRateLimiter
except ImportError:
    logger.warning("api_rate_limiter.py not found. Using a dummy rate limiter.")
    class APIRateLimiter:
        def __init__(self, requests_per_minute: int):
            self.delay = 60.0 / requests_per_minute if requests_per_minute > 0 else 0
        async def wait_if_needed_async(self):
            if self.delay > 0: await asyncio.sleep(self.delay)

# --- AIQA Settings for High Precision Mode ---
AIQA_HIGH_PRECISION_SETTINGS = {
    "use_ai_detailed_query": True, "use_semantic_search": True, "use_structured_filter": True,
    "context_limit": 20, "similarity_threshold": 0.2, "max_documents_for_selection": 12,
    "ai_selection_limit": 5, "query_rewrite_count": 5, "detailed_text_max_length": 15000,
    "max_chars_per_doc": 5000, "enable_query_expansion": True, "context_window_overlap": 0.2,
    "prompt_input_max_length": 8000
}


class FullQASystemEvaluator:
    """å®Œæ•´AIå•ç­”ç³»çµ±æº–ç¢ºåº¦è©•ä¼°å™¨ - ä½¿ç”¨Ragasæ¡†æ¶å’ŒAPIèª¿ç”¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è©•ä¼°å™¨"""
        self.api_base_url = os.getenv('API_URL', 'http://localhost:8000')
        self.api_username = os.getenv('USERNAME')
        self.api_password = os.getenv('PASSWORD')
        if not self.api_username or not self.api_password:
            raise ValueError("è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½® USERNAME å’Œ PASSWORD")
        self.session = None
        self.access_token = None
        
        # AIQA APIé€Ÿç‡é™åˆ¶å™¨ (æ¯æ¬¡AIQAç´„6æ¬¡Google AIèª¿ç”¨ï¼Œæ¯åˆ†é˜10æ¬¡é™åˆ¶ â†’ æœ€å¤š1.5æ¬¡AIQA)
        self.aiqa_rate_limiter = APIRateLimiter(requests_per_minute=1)
        
        # Ragasè©•ä¼°é€Ÿç‡é™åˆ¶å™¨ (æ¯åˆ†é˜15æ¬¡AIèª¿ç”¨)
        self.ragas_rate_limiter = APIRateLimiter(requests_per_minute=15)
        
        logger.info(f"AIQA APIé€Ÿç‡é™åˆ¶å™¨å·²åˆå§‹åŒ– (æ¯åˆ†é˜æœ€å¤š 1 æ¬¡è«‹æ±‚ - é…åˆGoogle AIé™åˆ¶)")
        logger.info(f"Ragasè©•ä¼°é€Ÿç‡é™åˆ¶å™¨å·²åˆå§‹åŒ– (æ¯åˆ†é˜æœ€å¤š 15 æ¬¡AIèª¿ç”¨)")
    
    async def initialize_services(self):
        """åˆå§‹åŒ–APIé€£æ¥ä¸¦ç™»å…¥"""
        try:
            self.session = aiohttp.ClientSession()
            if not await self._login_and_get_token():
                raise Exception("ç„¡æ³•ç²å–èªè­‰ token")
            await self._test_api_connection()
            logger.info("APIé€£æ¥åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"APIé€£æ¥åˆå§‹åŒ–å¤±æ•—: {e}", exc_info=True)
            raise

    async def _login_and_get_token(self) -> bool:
        """ç™»å…¥ä¸¦ç²å–JWT tokenï¼Œçµ±ä¸€ä½¿ç”¨aiohttp"""
        login_url = f"{self.api_base_url}/api/v1/auth/token"
        login_data = {"username": self.api_username, "password": self.api_password}
        logger.info(f"å˜—è©¦ç™»å…¥åˆ°: {login_url}")
        try:
            async with self.session.post(login_url, data=login_data) as response:
                response.raise_for_status()
                result = await response.json()
                self.access_token = result.get("access_token")
                if self.access_token:
                    logger.info("ç™»å…¥æˆåŠŸï¼Œç²å–åˆ°JWT token")
                    return True
                else:
                    logger.error("ç™»å…¥å¤±æ•—ï¼šéŸ¿æ‡‰ä¸­æ²’æœ‰access_token")
                    return False
        except Exception as e:
            logger.error(f"ç™»å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return False

    def get_auth_headers(self) -> Dict[str, str]:
        if not self.access_token:
            raise ValueError("æœªç²å–åˆ°access_token")
        return {"Authorization": f"Bearer {self.access_token}", "Content-Type": "application/json"}

    async def _test_api_connection(self):
        try:
            async with self.session.get(f"{self.api_base_url}/", headers=self.get_auth_headers()) as r:
                r.raise_for_status()
                logger.info("APIé€£æ¥æ¸¬è©¦æˆåŠŸ")
        except Exception as e:
            logger.warning(f"APIé€£æ¥æ¸¬è©¦å¤±æ•—: {e}")

    async def _qa_via_api(self, question: str, model_preference: Optional[str]) -> Dict[str, Any]:
        """é€šéAPIåŸ·è¡Œå®Œæ•´å•ç­” - ä½¿ç”¨AIQAå°ˆç”¨é€Ÿç‡é™åˆ¶"""
        await self.aiqa_rate_limiter.wait_if_needed_async()
        start_time = time.time()
        
        # æ­£ç¢ºçš„API endpointå’Œpayloadæ ¼å¼
        url = f"{self.api_base_url}/api/v1/unified-ai/qa"
        payload = {
            "question": question,  # ä¿®æ­£ï¼šAPIè¦æ±‚çš„å­—æ®µæ˜¯ questionï¼Œä¸æ˜¯ user_query
            **AIQA_HIGH_PRECISION_SETTINGS
        }
        if model_preference:
            payload["model_preference"] = model_preference
        
        try:
            async with self.session.post(url, json=payload, headers=self.get_auth_headers()) as response:
                logger.info(f"APIéŸ¿æ‡‰ç‹€æ…‹ç¢¼: {response.status}")
                
                if response.status != 200:
                    response_text = await response.text()
                    logger.error(f"APIè«‹æ±‚å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status}, éŸ¿æ‡‰: {response_text}")
                    return {"error": f"APIè¿”å›éŒ¯èª¤ç‹€æ…‹ç¢¼: {response.status}"}
                
                response_data = await response.json()
                end_time = time.time()
                response_data['processing_time'] = end_time - start_time
                
                # èª¿è©¦ï¼šè¨˜éŒ„éŸ¿æ‡‰æ•¸æ“šçš„ä¸»è¦éµ
                main_keys = list(response_data.keys()) if isinstance(response_data, dict) else "éå­—å…¸éŸ¿æ‡‰"
                logger.info(f"APIéŸ¿æ‡‰ä¸»è¦éµ: {main_keys}")
                
                return response_data
                
        except Exception as e:
            end_time = time.time()
            logger.error(f"å•ç­”APIèª¿ç”¨ç•°å¸¸ for query '{question[:30]}...': {e}")
            return {
                "error": str(e),
                "processing_time": end_time - start_time
            }

    async def _run_ragas_evaluation(self, ragas_data: List[Dict], answer_only_mode: bool = True) -> Dict:
        """åŸ·è¡ŒRagasè©•ä¼° - è€ƒæ…®AIèª¿ç”¨é€Ÿç‡é™åˆ¶"""
        if not ragas_data:
            logger.warning("æ²’æœ‰å¯ç”¨æ–¼Ragasè©•ä¼°çš„æ•¸æ“šã€‚")
            return {}
        
        # æ ¹æ“šè©•ä¼°æ¨¡å¼é¸æ“‡æŒ‡æ¨™
        if answer_only_mode:
            # åƒ…ç­”æ¡ˆè©•ä¼°æ¨¡å¼ï¼šå°ˆç‚ºæ‘˜è¦æ¶æ§‹è¨­è¨ˆ
            metrics = [answer_relevancy, answer_correctness]  # ç§»é™¤ faithfulness
            ai_calls_per_case = 2  # æ¯æ¡ˆä¾‹2æ¬¡AIèª¿ç”¨
            logger.info("ğŸ¯ æ‘˜è¦æ¶æ§‹å°ˆç”¨è©•ä¼°æ¨¡å¼ - è©•ä¼°ç­”æ¡ˆç›¸é—œæ€§å’Œæ­£ç¢ºæ€§")
            logger.info("â„¹ï¸  è·³é Faithfulness (æ‘˜è¦vsåŸæ–‡ç„¡æ³•ç›´æ¥æ¯”è¼ƒ)")
            logger.info("â„¹ï¸  è·³é Context Recall (æ‘˜è¦æ¶æ§‹ä¸é©ç”¨)")
        else:
            # å®Œæ•´è©•ä¼°æ¨¡å¼ï¼šåƒ…åŒ…å«é©ç”¨çš„æª¢ç´¢æŒ‡æ¨™
            metrics = [context_precision, answer_relevancy, answer_correctness]  # ç§»é™¤ context_recall å’Œ faithfulness
            ai_calls_per_case = 3  # æ¯æ¡ˆä¾‹3æ¬¡AIèª¿ç”¨  
            logger.info("ğŸ“Š æ‘˜è¦æ¶æ§‹å®Œæ•´è©•ä¼°æ¨¡å¼")
            logger.info("â„¹ï¸  åŒ…å« Context Precision (æª¢ç´¢ç²¾ç¢ºåº¦)")
            logger.info("â„¹ï¸  è·³é Context Recall (æ‘˜è¦æ¶æ§‹ä¸é©ç”¨)")
            logger.info("â„¹ï¸  è·³é Faithfulness (æ‘˜è¦vsåŸæ–‡ä¸é©ç”¨)")
        
        # è¨ˆç®—Ragasè©•ä¼°æ™‚é–“é ä¼°
        estimated_ai_calls = len(ragas_data) * ai_calls_per_case
        estimated_time_minutes = max(1, estimated_ai_calls / 15)  # æ¯åˆ†é˜æœ€å¤š15æ¬¡
        logger.info(f"â±ï¸  é ä¼°Ragasè©•ä¼°æ™‚é–“: è‡³å°‘ {estimated_time_minutes:.1f} åˆ†é˜ (ç´„ {estimated_ai_calls} æ¬¡AIèª¿ç”¨)")
        
        logger.info(f"æº–å‚™åŸ·è¡ŒRagasè©•ä¼°ï¼Œå…± {len(ragas_data)} å€‹é …ç›®...")
        dataset = Dataset.from_list(ragas_data)
        
        try:
            # ç”±æ–¼Ragaså…§éƒ¨çš„AIèª¿ç”¨æˆ‘å€‘ç„¡æ³•ç›´æ¥æ§åˆ¶é€Ÿç‡ï¼Œ
            # æˆ‘å€‘é€šéæ¸›å°‘ä½µç™¼æˆ–åˆ†æ‰¹è™•ç†ä¾†é–“æ¥æ§åˆ¶
            if len(ragas_data) > 5:
                logger.warning(f"âš ï¸  å¤§å‹æ•¸æ“šé›† ({len(ragas_data)} é …)ï¼ŒRagasè©•ä¼°å¯èƒ½éœ€è¦å¾ˆé•·æ™‚é–“")
                logger.warning(f"âš ï¸  å»ºè­°è€ƒæ…®åˆ†æ‰¹è©•ä¼°æˆ–ä½¿ç”¨æ›´å°çš„æ¸¬è©¦é›†")
            
            # ç‚ºäº†éµå®ˆé€Ÿç‡é™åˆ¶ï¼Œæˆ‘å€‘å¯ä»¥æ·»åŠ ä¸€å€‹é å»¶é²
            if estimated_ai_calls > 15:
                pre_delay = (estimated_ai_calls - 15) * 4  # æ¯è¶…å‡ºçš„èª¿ç”¨å»¶é²4ç§’
                logger.info(f"â±ï¸  ç‚ºé¿å…è¶…éé€Ÿç‡é™åˆ¶ï¼Œå…ˆç­‰å¾… {pre_delay} ç§’...")
                await asyncio.sleep(pre_delay)
            
            result = await asyncio.to_thread(evaluate, dataset, metrics=metrics, llm=llm, embeddings=embeddings, raise_exceptions=False)
            logger.info("Ragasè©•ä¼°å®Œæˆï¼")
            return result.to_pandas().mean(numeric_only=True).to_dict()
        except Exception as e:
            logger.error(f"Ragasè©•ä¼°å¤±æ•—: {e}", exc_info=True)
            return {"error": str(e)}

    async def evaluate_full_qa_system(self, test_cases: List[Dict], model_preference: Optional[str], answer_only_mode: bool = True) -> Dict[str, Any]:
        """è©•ä¼°å®Œæ•´å•ç­”ç³»çµ±çš„æº–ç¢ºåº¦"""
        # è¨ˆç®—ç¸½é«”è©•ä¼°æ™‚é–“é ä¼°
        total_aiqa_calls = len(test_cases)
        aiqa_time_minutes = max(1, total_aiqa_calls / 1)  # æ¯åˆ†é˜æœ€å¤š1æ¬¡AIQA
        
        # æ ¹æ“šè©•ä¼°æ¨¡å¼è¨ˆç®—Ragas AIèª¿ç”¨æ¬¡æ•¸
        ragas_calls_per_case = 2 if answer_only_mode else 3  # åƒ…ç­”æ¡ˆæ¨¡å¼2æ¬¡ï¼Œå®Œæ•´æ¨¡å¼3æ¬¡
        ragas_ai_calls = len(test_cases) * ragas_calls_per_case
        ragas_time_minutes = max(1, ragas_ai_calls / 15)  # æ¯åˆ†é˜æœ€å¤š15æ¬¡
        total_estimated_time = aiqa_time_minutes + ragas_time_minutes
        
        logger.info(f"â±ï¸  é ä¼°ç¸½è©•ä¼°æ™‚é–“: è‡³å°‘ {total_estimated_time:.1f} åˆ†é˜")
        logger.info(f"   - AIQAéšæ®µ: {aiqa_time_minutes:.1f} åˆ†é˜ ({total_aiqa_calls} æ¬¡è«‹æ±‚)")
        logger.info(f"   - Ragasè©•ä¼°éšæ®µ: {ragas_time_minutes:.1f} åˆ†é˜ ({ragas_ai_calls} æ¬¡AIèª¿ç”¨)")
        
        ragas_data = []
        system_performance_data = []
        
        for i, test_case in enumerate(test_cases):
            question = test_case.get('question') or test_case.get('user_query')
            ground_truth = test_case.get('ground_truth')
            logger.info(f"--- [æ¡ˆä¾‹ {i+1}/{len(test_cases)}] å•é¡Œ: {question[:50]}... ---")
            
            if not all([question, ground_truth]):
                logger.warning("è·³éæ¡ˆä¾‹ï¼šç¼ºå°‘ 'question' æˆ– 'ground_truth'")
                continue

            qa_response = await self._qa_via_api(question, model_preference)
            
            # èª¿è©¦ï¼šé¡¯ç¤ºAPIéŸ¿æ‡‰çµæ§‹
            logger.info(f"QA API éŸ¿æ‡‰çµæ§‹: {list(qa_response.keys()) if qa_response else 'Empty response'}")
            
            answer = qa_response.get('answer', '').strip()
            
            # === å¤šå±¤æ¬¡ä¸Šä¸‹æ–‡åˆ†æ ===
            semantic_search_contexts = qa_response.get('semantic_search_contexts', [])
            llm_context_documents = qa_response.get('llm_context_documents', [])
            detailed_document_data = qa_response.get('detailed_document_data_from_ai_query', [])
            
            # æ–°å¢ï¼šæŸ¥è©¢é‡å¯«åˆ†æ
            query_rewrite_result = qa_response.get('query_rewrite_result', {})
            ai_query_reasoning = qa_response.get('ai_generated_query_reasoning', '')
            
            # æå–ä¸åŒå±¤æ¬¡çš„ä¸Šä¸‹æ–‡ç”¨æ–¼è©•ä¼°
            contexts = []
            context_analysis = {
                "semantic_search_count": len(semantic_search_contexts),
                "llm_context_count": len(llm_context_documents),
                "detailed_data_count": len(detailed_document_data) if detailed_document_data else 0,
                "ai_selection_effectiveness": 0.0,
                "context_source_types": [],
                # æ–°å¢ï¼šæŸ¥è©¢é‡å¯«åˆ†æ
                "query_rewrite_count": len(query_rewrite_result.get('rewritten_queries', [])),
                "has_intent_analysis": bool(query_rewrite_result.get('intent_analysis', '')),
                "has_extracted_parameters": bool(query_rewrite_result.get('extracted_parameters', {})),
                "has_ai_query_reasoning": bool(ai_query_reasoning),
                # æ–°å¢ï¼šç›¸ä¼¼åº¦åˆ†æ
                "avg_similarity_score": 0.0,
                "max_similarity_score": 0.0,
                "min_similarity_score": 0.0,
                "similarity_score_std": 0.0
            }
            
            # è¨ˆç®—èªç¾©æœç´¢çš„ç›¸ä¼¼åº¦çµ±è¨ˆ
            if semantic_search_contexts:
                similarity_scores = [doc.get('similarity_score', 0) for doc in semantic_search_contexts]
                if similarity_scores:
                    context_analysis["avg_similarity_score"] = np.mean(similarity_scores)
                    context_analysis["max_similarity_score"] = np.max(similarity_scores)
                    context_analysis["min_similarity_score"] = np.min(similarity_scores)
                    context_analysis["similarity_score_std"] = np.std(similarity_scores)
            
            # å„ªå…ˆä½¿ç”¨ LLM å¯¦éš›ä½¿ç”¨çš„ä¸Šä¸‹æ–‡ï¼ˆæœ€æº–ç¢ºï¼‰
            if llm_context_documents:
                contexts = [doc.get('content_used', '') for doc in llm_context_documents if doc.get('content_used')]
                context_analysis["context_source_types"] = [doc.get('source_type', 'unknown') for doc in llm_context_documents]
                logger.info(f"âœ… ä½¿ç”¨ LLM å¯¦éš›ä¸Šä¸‹æ–‡: {len(contexts)} å€‹ç‰‡æ®µ")
                logger.info(f"   ä¸Šä¸‹æ–‡ä¾†æºé¡å‹: {set(context_analysis['context_source_types'])}")
                
                # è¨ˆç®— AI é¸æ“‡æ•ˆæœï¼ˆLLM ä¸Šä¸‹æ–‡æ•¸ vs èªç¾©æœç´¢çµæœæ•¸ï¼‰
                if semantic_search_contexts:
                    context_analysis["ai_selection_effectiveness"] = len(llm_context_documents) / len(semantic_search_contexts)
                    logger.info(f"   AI ç¯©é¸æ•ˆæœ: {len(llm_context_documents)}/{len(semantic_search_contexts)} = {context_analysis['ai_selection_effectiveness']:.2f}")
            
            # å‚™é¸ï¼šä½¿ç”¨èªç¾©æœç´¢çš„æ‘˜è¦
            elif semantic_search_contexts:
                contexts = [doc.get('summary_or_chunk_text', '') for doc in semantic_search_contexts if doc.get('summary_or_chunk_text')]
                logger.info(f"âš ï¸ å›é€€ä½¿ç”¨èªç¾©æœç´¢ä¸Šä¸‹æ–‡: {len(contexts)} å€‹æ‘˜è¦")
                
            # æœ€å¾Œå‚™é¸ï¼šä½¿ç”¨ç­”æ¡ˆæœ¬èº«
            else:
                contexts = [answer] if answer else ["ç„¡ä¸Šä¸‹æ–‡"]
                logger.warning("âš ï¸ ç„¡ä¸Šä¸‹æ–‡æ•¸æ“šï¼Œä½¿ç”¨ç­”æ¡ˆæœ¬èº«ä½œç‚ºè™›æ“¬ä¸Šä¸‹æ–‡")
            
            # å¢å¼·ç‰ˆæª¢ç´¢åˆ†ææ—¥èªŒ
            logger.info("ğŸ“Š å¢å¼·ç‰ˆæª¢ç´¢æµç¨‹åˆ†æ:")
            
            # Step 0: æŸ¥è©¢é‡å¯«åˆ†æ
            logger.info(f"   0ï¸âƒ£ æŸ¥è©¢é‡å¯«éšæ®µ:")
            logger.info(f"      åŸå§‹æŸ¥è©¢: {question[:50]}...")
            if query_rewrite_result:
                rewritten_queries = query_rewrite_result.get('rewritten_queries', [])
                logger.info(f"      é‡å¯«æŸ¥è©¢æ•¸: {len(rewritten_queries)} å€‹")
                if rewritten_queries:
                    logger.info(f"      é‡å¯«ç¤ºä¾‹: {rewritten_queries[0][:50]}..." if rewritten_queries[0] else "ç„¡")
                intent_analysis = query_rewrite_result.get('intent_analysis', '')
                logger.info(f"      æ„åœ–åˆ†æ: {'æœ‰' if intent_analysis else 'ç„¡'} ({len(intent_analysis)} å­—ç¬¦)")
                extracted_params = query_rewrite_result.get('extracted_parameters', {})
                logger.info(f"      æå–åƒæ•¸: {len(extracted_params)} å€‹ - {list(extracted_params.keys())[:3]}")
            
            # Step 1: èªç¾©æœç´¢åˆ†æ
            logger.info(f"   1ï¸âƒ£ èªç¾©æœç´¢éšæ®µ: {len(semantic_search_contexts)} å€‹å€™é¸")
            if semantic_search_contexts:
                logger.info(f"      å¹³å‡ç›¸ä¼¼åº¦: {context_analysis['avg_similarity_score']:.3f}")
                logger.info(f"      ç›¸ä¼¼åº¦ç¯„åœ: {context_analysis['min_similarity_score']:.3f} - {context_analysis['max_similarity_score']:.3f}")
                logger.info(f"      ç›¸ä¼¼åº¦æ¨™æº–å·®: {context_analysis['similarity_score_std']:.3f}")
                doc_ids = [doc.get('document_id', 'unknown')[:8] for doc in semantic_search_contexts[:3]]
                logger.info(f"      Top-3 æ–‡æª”ID: {doc_ids}")
            
            # Step 2: AI æ™ºæ…§ç¯©é¸åˆ†æ
            logger.info(f"   2ï¸âƒ£ AI æ™ºæ…§ç¯©é¸éšæ®µ: {len(llm_context_documents)} å€‹")
            if llm_context_documents:
                source_types = [doc.get('source_type', 'unknown') for doc in llm_context_documents]
                source_distribution = dict(zip(*np.unique(source_types, return_counts=True))) if source_types else {}
                logger.info(f"      ä¾†æºé¡å‹åˆ†å¸ƒ: {source_distribution}")
                
                # åˆ†æä¸Šä¸‹æ–‡å“è³ª
                avg_content_length = np.mean([len(doc.get('content_used', '')) for doc in llm_context_documents])
                logger.info(f"      å¹³å‡ä¸Šä¸‹æ–‡é•·åº¦: {avg_content_length:.0f} å­—ç¬¦")
            
            # Step 3: è©³ç´°æŸ¥è©¢åˆ†æ
            logger.info(f"   3ï¸âƒ£ è©³ç´°æŸ¥è©¢éšæ®µ: {context_analysis['detailed_data_count']} å€‹")
            if ai_query_reasoning:
                logger.info(f"      AI æŸ¥è©¢æ¨ç†: {'æœ‰' if ai_query_reasoning else 'ç„¡'} ({len(ai_query_reasoning)} å­—ç¬¦)")
                logger.info(f"      æ¨ç†æ‘˜è¦: {ai_query_reasoning[:100]}..." if len(ai_query_reasoning) > 100 else ai_query_reasoning)
            
            # è¨ˆç®—æª¢ç´¢ç®¡é“æ•ˆç‡æŒ‡æ¨™
            pipeline_efficiency = {
                "retrieval_funnel_ratio": 0.0,  # æœ€çµ‚ä½¿ç”¨æ•¸ / åˆå§‹å€™é¸æ•¸
                "ai_filtering_precision": 0.0,  # AIç¯©é¸çš„ç²¾ç¢ºåº¦
                "detailed_query_success_rate": 0.0  # è©³ç´°æŸ¥è©¢æˆåŠŸç‡
            }
            
            if semantic_search_contexts and llm_context_documents:
                pipeline_efficiency["retrieval_funnel_ratio"] = len(llm_context_documents) / len(semantic_search_contexts)
                pipeline_efficiency["ai_filtering_precision"] = context_analysis["ai_selection_effectiveness"]
                
            if detailed_document_data and llm_context_documents:
                # æª¢æŸ¥æœ‰å¤šå°‘LLMä¸Šä¸‹æ–‡ä¾†è‡ªè©³ç´°æŸ¥è©¢
                detailed_source_count = sum(1 for doc in llm_context_documents if doc.get('source_type') == 'ai_detailed_query')
                if len(llm_context_documents) > 0:
                    pipeline_efficiency["detailed_query_success_rate"] = detailed_source_count / len(llm_context_documents)
            
            # å°‡ç®¡é“æ•ˆç‡åŠ å…¥ä¸Šä¸‹æ–‡åˆ†æ
            context_analysis.update(pipeline_efficiency)
            
            logger.info(f"   ğŸ“ˆ æª¢ç´¢ç®¡é“æ•ˆç‡:")
            logger.info(f"      æª¢ç´¢æ¼æ–—æ¯”ç‡: {pipeline_efficiency['retrieval_funnel_ratio']:.2f}")
            logger.info(f"      AIç¯©é¸ç²¾ç¢ºåº¦: {pipeline_efficiency['ai_filtering_precision']:.2f}")
            logger.info(f"      è©³ç´°æŸ¥è©¢æˆåŠŸç‡: {pipeline_efficiency['detailed_query_success_rate']:.2f}")
            
            system_performance_data.append({
                "response_time": qa_response.get('processing_time', 0.0),
                "tokens_used": qa_response.get('tokens_used', 0),
                **context_analysis  # æ·»åŠ ä¸Šä¸‹æ–‡åˆ†ææ•¸æ“š
            })

            # æº–å‚™ Ragas è©•ä¼°æ•¸æ“š
            if answer:
                ragas_item = {
                    "question": question,
                    "answer": answer,
                    "contexts": contexts,
                    "ground_truth": ground_truth
                }
                
                # æ·»åŠ å¢å¼·ç‰ˆæª¢ç´¢åˆ†æå…ƒæ•¸æ“šï¼ˆç”¨æ–¼å¾ŒçºŒåˆ†æï¼‰
                ragas_item["_metadata"] = {
                    # åŸºæœ¬æª¢ç´¢æŒ‡æ¨™
                    "semantic_search_count": context_analysis["semantic_search_count"],
                    "llm_context_count": context_analysis["llm_context_count"],
                    "ai_selection_effectiveness": context_analysis["ai_selection_effectiveness"],
                    "context_source_types": context_analysis["context_source_types"],
                    "has_detailed_query_data": context_analysis["detailed_data_count"] > 0,
                    
                    # æŸ¥è©¢é‡å¯«æŒ‡æ¨™
                    "query_rewrite_count": context_analysis["query_rewrite_count"],
                    "has_intent_analysis": context_analysis["has_intent_analysis"],
                    "has_extracted_parameters": context_analysis["has_extracted_parameters"],
                    "has_ai_query_reasoning": context_analysis["has_ai_query_reasoning"],
                    
                    # ç›¸ä¼¼åº¦åˆ†ææŒ‡æ¨™
                    "avg_similarity_score": context_analysis["avg_similarity_score"],
                    "max_similarity_score": context_analysis["max_similarity_score"],
                    "min_similarity_score": context_analysis["min_similarity_score"],
                    "similarity_score_std": context_analysis["similarity_score_std"],
                    
                    # æª¢ç´¢ç®¡é“æ•ˆç‡æŒ‡æ¨™
                    "retrieval_funnel_ratio": context_analysis["retrieval_funnel_ratio"],
                    "ai_filtering_precision": context_analysis["ai_filtering_precision"],
                    "detailed_query_success_rate": context_analysis["detailed_query_success_rate"]
                }
                
                ragas_data.append(ragas_item)
                logger.info(f"âœ… æˆåŠŸæ·»åŠ æ¡ˆä¾‹åˆ°Ragasè©•ä¼°æ•¸æ“šï¼ˆä¸Šä¸‹æ–‡å“è³ª: {len(contexts)} å€‹æœ‰æ•ˆç‰‡æ®µï¼‰")
            else:
                logger.warning(f"âŒ APIæœªè¿”å›æœ‰æ•ˆç­”æ¡ˆï¼Œæ­¤æ¡ˆä¾‹å°‡ä¸è¢«Ragasè©•ä¼°")
                logger.warning(f"  - ç­”æ¡ˆ: {'æœ‰' if answer else 'ç„¡'} ({len(answer) if answer else 0} å­—ç¬¦)")
                logger.warning(f"  - æª¢ç´¢æµç¨‹: èªç¾©æœç´¢ {context_analysis['semantic_search_count']} â†’ AIç¯©é¸ {context_analysis['llm_context_count']} â†’ è©³ç´°æŸ¥è©¢ {context_analysis['detailed_data_count']}")

        # æ ¹æ“šåƒæ•¸é¸æ“‡è©•ä¼°æ¨¡å¼
        ragas_scores = await self._run_ragas_evaluation(ragas_data, answer_only_mode=answer_only_mode)
        
        # åŒ¯ç¸½çµæœ
        sys_perf_df = pd.DataFrame(system_performance_data)
        
        # è¨ˆç®—å¢å¼·ç‰ˆæª¢ç´¢æµç¨‹åˆ†æçµ±è¨ˆ
        retrieval_analysis = {}
        if len(sys_perf_df) > 0 and 'semantic_search_count' in sys_perf_df.columns:
            retrieval_analysis = {
                # åŸæœ‰æŒ‡æ¨™
                "average_semantic_search_candidates": sys_perf_df['semantic_search_count'].mean(),
                "average_llm_context_used": sys_perf_df['llm_context_count'].mean(),
                "average_ai_selection_effectiveness": sys_perf_df['ai_selection_effectiveness'].mean(),
                "detailed_query_usage_rate": (sys_perf_df['detailed_data_count'] > 0).mean(),
                "context_source_distribution": {},
                
                # æ–°å¢ï¼šæŸ¥è©¢é‡å¯«åˆ†æ
                "query_rewrite_analysis": {
                    "average_rewrite_count": sys_perf_df['query_rewrite_count'].mean() if 'query_rewrite_count' in sys_perf_df.columns else 0,
                    "intent_analysis_success_rate": sys_perf_df['has_intent_analysis'].mean() if 'has_intent_analysis' in sys_perf_df.columns else 0,
                    "parameter_extraction_success_rate": sys_perf_df['has_extracted_parameters'].mean() if 'has_extracted_parameters' in sys_perf_df.columns else 0,
                    "ai_reasoning_availability_rate": sys_perf_df['has_ai_query_reasoning'].mean() if 'has_ai_query_reasoning' in sys_perf_df.columns else 0
                },
                
                # æ–°å¢ï¼šç›¸ä¼¼åº¦åˆ†æ
                "similarity_analysis": {
                    "average_similarity_score": sys_perf_df['avg_similarity_score'].mean() if 'avg_similarity_score' in sys_perf_df.columns else 0,
                    "average_max_similarity": sys_perf_df['max_similarity_score'].mean() if 'max_similarity_score' in sys_perf_df.columns else 0,
                    "average_min_similarity": sys_perf_df['min_similarity_score'].mean() if 'min_similarity_score' in sys_perf_df.columns else 0,
                    "average_similarity_variance": sys_perf_df['similarity_score_std'].mean() if 'similarity_score_std' in sys_perf_df.columns else 0
                },
                
                # æ–°å¢ï¼šæª¢ç´¢ç®¡é“æ•ˆç‡
                "pipeline_efficiency": {
                    "average_retrieval_funnel_ratio": sys_perf_df['retrieval_funnel_ratio'].mean() if 'retrieval_funnel_ratio' in sys_perf_df.columns else 0,
                    "average_ai_filtering_precision": sys_perf_df['ai_filtering_precision'].mean() if 'ai_filtering_precision' in sys_perf_df.columns else 0,
                    "average_detailed_query_success_rate": sys_perf_df['detailed_query_success_rate'].mean() if 'detailed_query_success_rate' in sys_perf_df.columns else 0
                }
            }
            
            # çµ±è¨ˆä¸Šä¸‹æ–‡ä¾†æºé¡å‹åˆ†å¸ƒ
            all_source_types = []
            for context_types in sys_perf_df['context_source_types']:
                if isinstance(context_types, list):
                    all_source_types.extend(context_types)
            
            if all_source_types:
                from collections import Counter
                source_counter = Counter(all_source_types)
                total_sources = sum(source_counter.values())
                retrieval_analysis["context_source_distribution"] = {
                    source: count/total_sources for source, count in source_counter.items()
                }
        
        results = {
            "evaluation_type": "end_to_end_qa_system_with_ragas_enhanced",
            "total_test_cases": len(test_cases),
            "ragas_evaluated_cases": len(ragas_data),
            "ragas_metrics": {k: (v if pd.notna(v) else 0.0) for k, v in ragas_scores.items()},
            "system_performance": {
                "average_response_time_seconds": sys_perf_df['response_time'].mean(),
                "median_response_time_seconds": sys_perf_df['response_time'].median(),
                "average_tokens_used": sys_perf_df['tokens_used'].mean()
            },
            "retrieval_pipeline_analysis": retrieval_analysis,
            "evaluation_parameters": {
                "model_preference": model_preference or "default",
                "mode": "high_precision",
                **AIQA_HIGH_PRECISION_SETTINGS
            }
        }
        return results

    def print_results(self, results: Dict[str, Any]):
        """ä»¥å°äººé¡å‹å¥½çš„æ ¼å¼è¼¸å‡ºè©•ä¼°çµæœ"""
        params = results.get("evaluation_parameters", {})
        ragas_metrics = results.get("ragas_metrics", {})
        sys_perf = results.get("system_performance", {})
        
        print("\n" + "="*80)
        print("ğŸ¤– ç«¯åˆ°ç«¯ (End-to-End) AI å•ç­”ç³»çµ±è©•ä¼°å ±å‘Š")
        print("="*80)
        print(f"ç¸½æ¡ˆä¾‹æ•¸: {results.get('total_test_cases', 0):<5} | "
              f"æœ‰æ•ˆ Ragas è©•ä¼°æ¡ˆä¾‹: {results.get('ragas_evaluated_cases', 0):<5}")
        print(f"âš™ï¸  è©•ä¼°æ¨¡å¼: {params.get('mode', 'N/A')}, "
              f"åå¥½æ¨¡å‹: {params.get('model_preference', 'default')}")
        
        # --- ç­”æ¡ˆå“è³ªè©•ä¼° ---
        print("\n" + "â”€"*25 + " ğŸ¯ æ‘˜è¦æ¶æ§‹ç­”æ¡ˆå“è³ªè©•ä¼° " + "â”€"*25)
        print(f"  - Answer Correctness (æ­£ç¢ºæ€§):  {ragas_metrics.get('answer_correctness', 0.0):.4f}")
        print(f"  - Answer Relevancy (ç›¸é—œæ€§):    {ragas_metrics.get('answer_relevancy', 0.0):.4f}")
        
        # é¡¯ç¤ºè·³éçš„æŒ‡æ¨™
        skipped_metrics = []
        if ragas_metrics.get('faithfulness') is None:
            skipped_metrics.append("Faithfulness (æ‘˜è¦vsåŸæ–‡ä¸é©ç”¨)")
        
        # --- æª¢ç´¢å“è³ª (åƒ…åœ¨å®Œæ•´æ¨¡å¼ä¸‹é¡¯ç¤º) ---
        if ragas_metrics.get('context_precision') is not None:
            print("\n" + "â”€"*25 + " ğŸ“Š æ‘˜è¦æ¶æ§‹æª¢ç´¢å“è³ªè©•ä¼° " + "â”€"*25)
            print(f"  - Context Precision (ç²¾ç¢ºåº¦):   {ragas_metrics.get('context_precision', 0.0):.4f}")
            skipped_metrics.append("Context Recall (æ‘˜è¦æ¶æ§‹ä¸é©ç”¨)")
        else:
            print("\n" + "â”€"*20 + " â„¹ï¸  æ‘˜è¦æ¶æ§‹å°ˆç”¨è©•ä¼°èªªæ˜ " + "â”€"*20)
            skipped_metrics.extend([
                "Context Precision (å®Œæ•´æ¨¡å¼æ‰è©•ä¼°)",
                "Context Recall (æ‘˜è¦æ¶æ§‹ä¸é©ç”¨)"
            ])
        
        # é¡¯ç¤ºè·³éçš„æŒ‡æ¨™èªªæ˜
        if skipped_metrics:
            print("\n" + "â”€"*25 + " âš ï¸  è·³éçš„è©•ä¼°æŒ‡æ¨™ " + "â”€"*25)
            for metric in skipped_metrics:
                print(f"  - {metric}")
            print("  ğŸ“ åŸå› ï¼šæ‚¨çš„å‘é‡åŒ–ä½¿ç”¨æ‘˜è¦ï¼ŒéåŸå§‹æ–‡æœ¬")
        
        # --- å¢å¼·ç‰ˆæª¢ç´¢æµç¨‹åˆ†æ ---
        retrieval_analysis = results.get("retrieval_pipeline_analysis", {})
        if retrieval_analysis:
            print("\n" + "â”€"*20 + " ğŸ” å®Œæ•´æª¢ç´¢æµç¨‹æ•ˆèƒ½åˆ†æ " + "â”€"*20)
            
            # Step 0: æŸ¥è©¢é‡å¯«æ•ˆèƒ½
            query_rewrite = retrieval_analysis.get('query_rewrite_analysis', {})
            if query_rewrite:
                print("  ğŸ“ æŸ¥è©¢é‡å¯«éšæ®µ:")
                print(f"    - å¹³å‡é‡å¯«æŸ¥è©¢æ•¸: {query_rewrite.get('average_rewrite_count', 0):.1f} å€‹")
                print(f"    - æ„åœ–åˆ†ææˆåŠŸç‡: {query_rewrite.get('intent_analysis_success_rate', 0):.1%}")
                print(f"    - åƒæ•¸æå–æˆåŠŸç‡: {query_rewrite.get('parameter_extraction_success_rate', 0):.1%}")
                print(f"    - AIæ¨ç†å¯ç”¨ç‡: {query_rewrite.get('ai_reasoning_availability_rate', 0):.1%}")
            
            # Step 1: èªç¾©æœç´¢æ•ˆèƒ½
            similarity_analysis = retrieval_analysis.get('similarity_analysis', {})
            print("  ğŸ” èªç¾©æœç´¢éšæ®µ:")
            print(f"    - å¹³å‡å€™é¸æ•¸: {retrieval_analysis.get('average_semantic_search_candidates', 0):.1f} å€‹")
            if similarity_analysis:
                print(f"    - å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•¸: {similarity_analysis.get('average_similarity_score', 0):.3f}")
                print(f"    - ç›¸ä¼¼åº¦å“è³ªç¯„åœ: {similarity_analysis.get('average_min_similarity', 0):.3f} - {similarity_analysis.get('average_max_similarity', 0):.3f}")
                print(f"    - ç›¸ä¼¼åº¦è®Šç•°åº¦: {similarity_analysis.get('average_similarity_variance', 0):.3f}")
            
            # Step 2: AI æ™ºæ…§ç¯©é¸æ•ˆèƒ½
            pipeline_eff = retrieval_analysis.get('pipeline_efficiency', {})
            print("  ğŸ¤– AI æ™ºæ…§ç¯©é¸éšæ®µ:")
            print(f"    - å¹³å‡ç¯©é¸å¾Œæ•¸é‡: {retrieval_analysis.get('average_llm_context_used', 0):.1f} å€‹")
            print(f"    - AIç¯©é¸æ•ˆæœ: {retrieval_analysis.get('average_ai_selection_effectiveness', 0):.1%}")
            if pipeline_eff:
                print(f"    - æª¢ç´¢æ¼æ–—æ¯”ç‡: {pipeline_eff.get('average_retrieval_funnel_ratio', 0):.2f}")
                print(f"    - AIç¯©é¸ç²¾ç¢ºåº¦: {pipeline_eff.get('average_ai_filtering_precision', 0):.2f}")
            
            # Step 3: è©³ç´°æŸ¥è©¢æ•ˆèƒ½
            print("  ğŸ“‹ è©³ç´°æŸ¥è©¢éšæ®µ:")
            print(f"    - è©³ç´°æŸ¥è©¢ä½¿ç”¨ç‡: {retrieval_analysis.get('detailed_query_usage_rate', 0):.1%}")
            if pipeline_eff:
                print(f"    - è©³ç´°æŸ¥è©¢æˆåŠŸç‡: {pipeline_eff.get('average_detailed_query_success_rate', 0):.1%}")
            
            # ä¸Šä¸‹æ–‡ä¾†æºåˆ†å¸ƒ
            source_dist = retrieval_analysis.get('context_source_distribution', {})
            if source_dist:
                print("  ğŸ“Š æœ€çµ‚ä¸Šä¸‹æ–‡ä¾†æºåˆ†å¸ƒ:")
                for source, ratio in sorted(source_dist.items(), key=lambda x: x[1], reverse=True):
                    source_name_map = {
                        "ai_detailed_query": "AIè©³ç´°æŸ¥è©¢",
                        "general_ai_summary": "AIé€šç”¨æ‘˜è¦", 
                        "general_extracted_text": "åŸå§‹æ–‡æœ¬",
                        "general_placeholder": "ä½”ä½ç¬¦"
                    }
                    display_name = source_name_map.get(source, source)
                    print(f"    â€¢ {display_name}: {ratio:.1%}")
        
        # --- ç³»çµ±æ€§èƒ½ ---
        print("\n" + "â”€"*32 + " âš¡ ç³»çµ±æ€§èƒ½ " + "â”€"*32)
        print(f"  - å¹³å‡å›æ‡‰æ™‚é–“: {sys_perf.get('average_response_time_seconds', 0.0):.2f} ç§’")
        print(f"  - ä¸­ä½å›æ‡‰æ™‚é–“: {sys_perf.get('median_response_time_seconds', 0.0):.2f} ç§’")
        print(f"  - å¹³å‡Tokenç”¨é‡: {sys_perf.get('average_tokens_used', 0.0):.0f}")
        
        # --- æ™ºæ…§è©•ä¼°ç¸½çµ ---
        print("\n" + "="*20 + " ğŸ“‹ æ™ºæ…§æª¢ç´¢ç³»çµ±è©•ä¼°ç¸½çµ " + "="*20)
        if retrieval_analysis:
            # æŸ¥è©¢é‡å¯«æ•ˆèƒ½è©•ä¼°
            query_rewrite = retrieval_analysis.get('query_rewrite_analysis', {})
            if query_rewrite:
                intent_rate = query_rewrite.get('intent_analysis_success_rate', 0)
                reasoning_rate = query_rewrite.get('ai_reasoning_availability_rate', 0)
                if intent_rate > 0.8 and reasoning_rate > 0.8:
                    print("âœ… æŸ¥è©¢ç†è§£å„ªç§€ï¼šAIæˆåŠŸç†è§£ç”¨æˆ¶æ„åœ–ä¸¦æä¾›æ¨ç†")
                elif intent_rate > 0.5:
                    print("âš ï¸ æŸ¥è©¢ç†è§£ä¸­ç­‰ï¼šå»ºè­°å„ªåŒ–æ„åœ–åˆ†ææˆ–æ¨ç†ç”Ÿæˆ")
                else:
                    print("âŒ æŸ¥è©¢ç†è§£ä¸ä½³ï¼šæŸ¥è©¢é‡å¯«ç³»çµ±éœ€è¦æ”¹å–„")
            
            # èªç¾©æœç´¢å“è³ªè©•ä¼°
            similarity_analysis = retrieval_analysis.get('similarity_analysis', {})
            if similarity_analysis:
                avg_similarity = similarity_analysis.get('average_similarity_score', 0)
                similarity_variance = similarity_analysis.get('average_similarity_variance', 0)
                if avg_similarity > 0.7:
                    print("âœ… èªç¾©æœç´¢å„ªç§€ï¼šå€™é¸æ–‡æª”èˆ‡æŸ¥è©¢é«˜åº¦ç›¸é—œ")
                elif avg_similarity > 0.5:
                    print("âš ï¸ èªç¾©æœç´¢ä¸­ç­‰ï¼šéƒ¨åˆ†æª¢ç´¢çµæœç›¸é—œæ€§å¾…æå‡")
                else:
                    print("âŒ èªç¾©æœç´¢ä¸ä½³ï¼šå‘é‡æ¨¡å‹æˆ–embeddingå“è³ªéœ€æ”¹å–„")
                
                if similarity_variance < 0.1:
                    print("âœ… æª¢ç´¢ç©©å®šæ€§å¥½ï¼šç›¸ä¼¼åº¦åˆ†æ•¸è®Šç•°åº¦ä½")
                elif similarity_variance > 0.2:
                    print("âš ï¸ æª¢ç´¢ç©©å®šæ€§å·®ï¼šç›¸ä¼¼åº¦åˆ†æ•¸è®Šç•°åº¦éé«˜")
            
            # AIç¯©é¸æ•ˆèƒ½è©•ä¼°
            effectiveness = retrieval_analysis.get('average_ai_selection_effectiveness', 0)
            pipeline_eff = retrieval_analysis.get('pipeline_efficiency', {})
            if effectiveness > 0.8:
                print("âœ… AIç¯©é¸æ•ˆæœå„ªç§€ï¼šæˆåŠŸå¾å€™é¸ä¸­ç²¾é¸å‡ºé«˜å“è³ªä¸Šä¸‹æ–‡")
            elif effectiveness > 0.5:
                print("âš ï¸ AIç¯©é¸æ•ˆæœä¸­ç­‰ï¼šå»ºè­°å„ªåŒ–ç¯©é¸ç­–ç•¥æˆ–æç¤ºè©")
            else:
                print("âŒ AIç¯©é¸æ•ˆæœä¸ä½³ï¼šAIæ–‡æª”é¸æ“‡é‚è¼¯éœ€è¦èª¿æ•´")
            
            # è©³ç´°æŸ¥è©¢ç³»çµ±è©•ä¼°
            detailed_usage = retrieval_analysis.get('detailed_query_usage_rate', 0)
            detailed_success = pipeline_eff.get('average_detailed_query_success_rate', 0) if pipeline_eff else 0
            if detailed_usage > 0.7 and detailed_success > 0.7:
                print("âœ… è©³ç´°æŸ¥è©¢ç³»çµ±å„ªç§€ï¼šé«˜ä½¿ç”¨ç‡ä¸”æˆåŠŸç‡ä½³")
            elif detailed_usage > 0.3:
                print("âš ï¸ è©³ç´°æŸ¥è©¢ç³»çµ±ä¸­ç­‰ï¼šå¯è€ƒæ…®æå‡ä½¿ç”¨ç‡æˆ–æˆåŠŸç‡")
            else:
                print("âŒ è©³ç´°æŸ¥è©¢ç³»çµ±ä¸ä½³ï¼šä¸»è¦ä¾è³´é€šç”¨æ‘˜è¦ï¼Œæ™ºæ…§æŸ¥è©¢æœªå……åˆ†ç™¼æ®")
            
            # æ•´é«”æª¢ç´¢ç®¡é“æ•ˆç‡è©•ä¼°
            funnel_ratio = pipeline_eff.get('average_retrieval_funnel_ratio', 0) if pipeline_eff else 0
            if funnel_ratio > 0.3:
                print("âœ… æª¢ç´¢ç®¡é“æ•ˆç‡ä½³ï¼šè‰¯å¥½çš„å€™é¸ç¯©é¸æ¯”ä¾‹")
            elif funnel_ratio > 0.1:
                print("âš ï¸ æª¢ç´¢ç®¡é“æ•ˆç‡ä¸­ç­‰ï¼šç¯©é¸æ¯”ä¾‹åˆç†ä½†æœ‰æ”¹å–„ç©ºé–“")
            else:
                print("âŒ æª¢ç´¢ç®¡é“æ•ˆç‡ä½ï¼šç¯©é¸éæ–¼åš´æ ¼æˆ–èªç¾©æœç´¢å“è³ªä¸ä½³")
        
        print("ğŸ“ å»ºè­°ï¼šå°ˆæ³¨å„ªåŒ–æ•´å€‹æª¢ç´¢ç®¡é“çš„å”åŒæ•ˆæœï¼Œæ‘˜è¦æ¶æ§‹å·²è·³éä¸é©ç”¨çš„æª¢ç´¢æŒ‡æ¨™")
        print("ğŸ¯ é‡é»ï¼šè©•ä¼°æ¶µè“‹å¾æŸ¥è©¢é‡å¯«åˆ°è©³ç´°æŸ¥è©¢çš„å®Œæ•´æ™ºæ…§æª¢ç´¢æµç¨‹")
        print("="*80 + "\n")

    def save_results_to_json(self, results: Dict[str, Any], output_path: str):
        """å°‡è©³ç´°è©•ä¼°çµæœä¿å­˜åˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            logger.info(f"è©³ç´°è©•ä¼°çµæœå·²ä¿å­˜è‡³: {output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜è©•ä¼°çµæœå¤±æ•—: {e}")

    async def run_evaluation_flow(self, dataset_path: str, model_preference: Optional[str], answer_only_mode: bool = True):
        """åŸ·è¡Œå®Œæ•´çš„è©•ä¼°æµç¨‹"""
        logger.info("é–‹å§‹ç«¯åˆ°ç«¯è©•ä¼°æµç¨‹...")
        if not os.path.isabs(dataset_path):
            dataset_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), dataset_path)
        
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
            logger.info(f"æˆåŠŸè¼‰å…¥ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¸¬è©¦æ•¸æ“šå¤±æ•—: {e}")
            return
        
        try:
            await self.initialize_services()
            results = await self.evaluate_full_qa_system(test_cases, model_preference, answer_only_mode=answer_only_mode)
            self.print_results(results)
            output_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "full_qa_system_evaluation_results.json")
            self.save_results_to_json(results, output_path)
        finally:
            if self.session and not self.session.closed:
                await self.session.close()
                logger.info("HTTPæœƒè©±å·²é—œé–‰")

async def main():
    """ä¸»å‡½æ•¸ï¼Œè§£æåƒæ•¸ä¸¦å•Ÿå‹•è©•ä¼°"""
    parser = argparse.ArgumentParser(description='ç«¯åˆ°ç«¯AIå•ç­”ç³»çµ±è©•ä¼° (ä½¿ç”¨Ragas)')
    parser.add_argument('--dataset', type=str, required=True, help='æ¸¬è©¦è³‡æ–™é›†æª”æ¡ˆè·¯å¾‘ (å¿…é ˆ)')
    parser.add_argument('--model', type=str, help='æŒ‡å®šä½¿ç”¨çš„AIæ¨¡å‹ (å¯é¸)')
    parser.add_argument('--full-context', action='store_true', help='å•Ÿç”¨å®Œæ•´è©•ä¼°æ¨¡å¼ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡æª¢ç´¢å“è³ªï¼‰')
    args = parser.parse_args()
    
    # æ±ºå®šè©•ä¼°æ¨¡å¼
    answer_only_mode = not args.full_context
    
    # é¡¯ç¤ºGoogle AIé€Ÿç‡é™åˆ¶è­¦å‘Š
    logger.warning("âš ï¸  æ³¨æ„ï¼šæœ¬è©•ä¼°ä½¿ç”¨Google AIå…è²»API")
    logger.warning("âš ï¸  AIQAé™åˆ¶ï¼šæ¯åˆ†é˜æœ€å¤š1æ¬¡è«‹æ±‚ (æ¯æ¬¡ç´„6æ¬¡Google AIèª¿ç”¨)")
    logger.warning("âš ï¸  Ragasé™åˆ¶ï¼šæ¯åˆ†é˜æœ€å¤š15æ¬¡AIèª¿ç”¨")
    logger.warning("âš ï¸  å¤§å‹è³‡æ–™é›†è©•ä¼°å¯èƒ½éœ€è¦æ•¸å°æ™‚å®Œæˆ")
    
    for var in ['USERNAME', 'PASSWORD', 'API_URL']:
        if not os.getenv(var):
            logger.error(f"ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸: {var}ï¼Œè«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®ã€‚")
            return
            
    evaluator = FullQASystemEvaluator()
    try:
        await evaluator.run_evaluation_flow(
            dataset_path=args.dataset,
            model_preference=args.model,
            answer_only_mode=answer_only_mode
        )
    except Exception as e:
        logger.error(f"è©•ä¼°è…³æœ¬åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)

if __name__ == "__main__":
    # ç§»é™¤äº† nest_asyncioï¼Œå› åœ¨æ¨™æº–è…³æœ¬ä¸­ asyncio.run() æ˜¯æ›´å¥½çš„é¸æ“‡
    try:
        asyncio.run(main())
        logger.info("ç¨‹å¼åŸ·è¡Œå®Œæˆï¼Œæ­£å¸¸é€€å‡ºã€‚")
    except SystemExit as e:
        if e.code != 0:
             logger.error("å› åƒæ•¸éŒ¯èª¤å°è‡´ç¨‹å¼é€€å‡ºã€‚")
    except Exception as e:
        logger.error(f"ç¨‹å¼åœ¨é ‚å±¤åŸ·è¡Œæ™‚å´©æ½°: {e}", exc_info=True)