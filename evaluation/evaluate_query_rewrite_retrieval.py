import json
import asyncio
import logging
import uuid
import os
import sys
import argparse
from typing import List, Dict, Any
from collections import defaultdict
import numpy as np
import pandas as pd
import aiohttp
from dotenv import load_dotenv

# --- Foolproof .env loading ---
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
except NameError:
    script_dir = os.getcwd()

dotenv_path = os.path.join(script_dir, '.env')
if not os.path.exists(dotenv_path):
    project_root = os.path.dirname(script_dir)
    dotenv_path_alt = os.path.join(project_root, 'evaluation', '.env')
    if os.path.exists(dotenv_path_alt):
        dotenv_path = dotenv_path_alt
    else:
        dotenv_path_root = os.path.join(project_root, '.env')
        if os.path.exists(dotenv_path_root):
            dotenv_path = dotenv_path_root
        else:
            print(f"FATAL: '.env' file not found at various checked paths.")
            sys.exit(1)
load_dotenv(dotenv_path=dotenv_path, override=True)
print(f"Successfully loaded .env file from: {dotenv_path}")

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Dummy Rate Limiter ---
try:
    from api_rate_limiter import APIRateLimiter
except ImportError:
    logger.warning("api_rate_limiter.py not found. Using a dummy rate limiter.")
    class APIRateLimiter:
        def __init__(self, requests_per_minute: int):
            self.delay = 60.0 / requests_per_minute if requests_per_minute > 0 else 0
        async def wait_if_needed_async(self):
            if self.delay > 0:
                await asyncio.sleep(self.delay)

logger.info("è©•ä¼°ç³»çµ±ï¼šæŸ¥è©¢å„ªåŒ–ç³»çµ± - å°ˆæ³¨æ–¼æŸ¥è©¢å„ªåŒ–å°æª¢ç´¢æº–ç¢ºæ€§çš„æ”¹å–„æ•ˆæœ")


class QueryOptimizationRetrievalEvaluator:
    """æŸ¥è©¢å„ªåŒ–å¾Œæª¢ç´¢æº–ç¢ºåº¦è©•ä¼°å™¨ - ä½¿ç”¨APIèª¿ç”¨ï¼Œæ”¯æ´æ™ºèƒ½è§¸ç™¼è©•ä¼°"""

    def __init__(self):
        self.api_base_url = os.getenv('API_URL', 'http://localhost:8000')
        self.api_username = os.getenv('USERNAME')
        self.api_password = os.getenv('PASSWORD')
        if not self.api_username or not self.api_password:
            raise ValueError("è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½® USERNAME å’Œ PASSWORD")
        self.session = None
        self.access_token = None
        
        self.rewrite_rate_limiter = APIRateLimiter(requests_per_minute=10)
        
        # æ™ºèƒ½è§¸ç™¼é…ç½®
        self.confidence_threshold = 0.75  # å¯èª¿æ•´çš„è§¸ç™¼é–€æª»
        
        logger.info(f"æŸ¥è©¢å„ªåŒ–APIé€Ÿç‡é™åˆ¶å™¨å·²åˆå§‹åŒ– (æ¯åˆ†é˜æœ€å¤š 10 æ¬¡è«‹æ±‚ - é…åˆå¾Œç«¯æœå‹™é™åˆ¶)")
        logger.info(f"å‘é‡æœç´¢APIç„¡é€Ÿç‡é™åˆ¶ï¼Œå¯ç›´æ¥èª¿ç”¨")
        logger.info(f"æ™ºèƒ½è§¸ç™¼é–€æª»è¨­å®šç‚º: {self.confidence_threshold}")

    async def initialize_services(self):
        try:
            self.session = aiohttp.ClientSession()
            if not await self.login_and_get_token():
                raise Exception("ç„¡æ³•ç²å–èªè­‰ token")
            await self._test_api_connection()
            logger.info("APIé€£æ¥åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"APIé€£æ¥åˆå§‹åŒ–å¤±æ•—: {e}", exc_info=True)
            raise

    async def login_and_get_token(self) -> bool:
        if not self.session:
            self.session = aiohttp.ClientSession()
        login_url = f"{self.api_base_url}/api/v1/auth/token"
        login_data = {"username": self.api_username, "password": self.api_password}
        logger.info(f"å˜—è©¦ç™»å…¥åˆ°: {login_url}")
        try:
            async with self.session.post(login_url, data=login_data) as response:
                response.raise_for_status()
                result = await response.json()
                self.access_token = result.get("access_token")
                if self.access_token:
                    logger.info("ç™»å…¥æˆåŠŸï¼Œç²å–åˆ° JWT token")
                    return True
                else:
                    logger.error("ç™»å…¥å¤±æ•—ï¼šéŸ¿æ‡‰ä¸­æ²’æœ‰ access_token")
                    return False
        except Exception as e:
            logger.error(f"ç™»å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            return False

    def get_auth_headers(self) -> Dict[str, str]:
        if not self.access_token:
            raise ValueError("æœªç²å–åˆ° access_tokenï¼Œè«‹å…ˆç™»å…¥")
        return {"Authorization": f"Bearer {self.access_token}", "Content-Type": "application/json"}

    async def _test_api_connection(self):
        try:
            headers = self.get_auth_headers()
            async with self.session.get(f"{self.api_base_url}/", headers=headers) as response:
                response.raise_for_status()
                logger.info("APIé€£æ¥æ¸¬è©¦æˆåŠŸ")
        except Exception as e:
            logger.warning(f"APIé€£æ¥æ¸¬è©¦å¤±æ•—: {e}")

    async def _rewrite_query_via_api(self, question: str, rewrite_count: int) -> List[str]:
        await self.rewrite_rate_limiter.wait_if_needed_async()
        headers = self.get_auth_headers()
        url = f"{self.api_base_url}/api/v1/unified-ai/rewrite-query"
        payload = {"query": question}
        try:
            async with self.session.post(url, json=payload, headers=headers) as response:
                response.raise_for_status()
                result = await response.json()
                
                rewritten_result_json = result.get('rewritten_result')
                if not rewritten_result_json:
                    logger.warning(f"æŸ¥è©¢é‡å¯« API æœªå›å‚³ 'rewritten_result' for query '{question[:30]}...'")
                    return [question]
                
                rewritten_data = json.loads(rewritten_result_json)
                rewritten_queries = rewritten_data.get('rewritten_queries', [])
                
                if not rewritten_queries:
                    logger.info(f"æŸ¥è©¢ '{question[:30]}...' æœªç”Ÿæˆä»»ä½•é‡å¯«æŸ¥è©¢ï¼Œä½¿ç”¨åŸå§‹æŸ¥è©¢")
                    return [question]

                final_queries = rewritten_queries[:rewrite_count]
                
                logger.info(f"æŸ¥è©¢ '{question[:30]}...' é€šé API ç²å¾— {len(final_queries)} å€‹é‡å¯«æŸ¥è©¢")
                return final_queries
        except Exception as e:
            logger.error(f"æŸ¥è©¢é‡å¯« API èª¿ç”¨ç•°å¸¸ for query '{question[:30]}...': {e}", exc_info=True)
            return [question]

    async def _search_semantic_via_api(self, query: str, top_k: int, similarity_threshold: float) -> List[Dict]:
        headers = self.get_auth_headers()
        url = f"{self.api_base_url}/api/v1/vector-db/semantic-search"
        payload = {
            "query": query, 
            "top_k": top_k, 
            "similarity_threshold": similarity_threshold,
            "enable_hybrid_search": True,
            "search_type": "rrf_fusion",
            "rrf_weights": { "summary": 0.4, "chunks": 0.6 }
        }
        try:
            async with self.session.post(url, json=payload, headers=headers) as response:
                response.raise_for_status()
                results = await response.json()
                logger.info(f"èªç¾©æœç´¢ (RRF Fusion) for query '{query[:30]}...' -> æª¢ç´¢åˆ° {len(results)} å€‹çµæœ")
                
                # èª¿è©¦ï¼šæª¢æŸ¥APIè¿”å›æ ¼å¼
                if results and logger.isEnabledFor(logging.DEBUG):
                    first_result = results[0]
                    logger.debug(f"ç¬¬ä¸€å€‹çµæœçš„å­—æ®µ: {list(first_result.keys())}")
                
                return results
        except Exception as e:
            logger.error(f"èªç¾©æœç´¢ (RRF Fusion) API èª¿ç”¨ç•°å¸¸ for query '{query[:30]}...': {e}", exc_info=True)
            return []

    async def _search_probe_via_api(self, query: str, top_k: int, similarity_threshold: float) -> List[Dict]:
        """åŸ·è¡Œæ¢é‡æœç´¢ - åƒ…å°æ‘˜è¦é€²è¡Œå¿«é€Ÿå‘é‡æœç´¢"""
        headers = self.get_auth_headers()
        url = f"{self.api_base_url}/api/v1/vector-db/semantic-search"
        payload = {
            "query": query, 
            "top_k": top_k, 
            "similarity_threshold": similarity_threshold,
            "enable_hybrid_search": False, # é—œéµï¼šç¦ç”¨æ··åˆæœç´¢ä»¥ç²å–åŸå§‹åˆ†æ•¸
            "search_type": "vector_only", # é—œéµï¼šåƒ…å‘é‡æœç´¢
            "collection_weights": { "summaries": 1.0, "chunks": 0.0 } # é—œéµï¼šåƒ…æœç´¢æ‘˜è¦
        }
        try:
            async with self.session.post(url, json=payload, headers=headers) as response:
                response.raise_for_status()
                results = await response.json()
                logger.info(f"ğŸ”¬ æ¢é‡æœç´¢ for query '{query[:30]}...' -> æª¢ç´¢åˆ° {len(results)} å€‹çµæœ")
                
                return results
        except Exception as e:
            logger.error(f"æ¢é‡æœç´¢ API èª¿ç”¨ç•°å¸¸ for query '{query[:30]}...': {e}", exc_info=True)
            return []

    async def evaluate_smart_trigger_strategy(self, test_cases: List[Dict], eval_params: Dict) -> Dict[str, Any]:
        """è©•ä¼°æ™ºèƒ½è§¸ç™¼ç­–ç•¥ vs ç¸½æ˜¯é‡å¯« vs å¾ä¸é‡å¯«çš„æ€§èƒ½æ¯”è¼ƒ
        
        æ³¨æ„ï¼šæ­¤è©•ä¼°æ–¹æ³•çœŸå¯¦æ¨¡æ“¬äº†æ™ºèƒ½è§¸ç™¼çš„æµç¨‹ï¼š
        - é«˜ç½®ä¿¡åº¦æ™‚ï¼šè·³éAIé‡å¯«ï¼Œåƒ…åŸ·è¡ŒåŸºæº–æª¢ç´¢ï¼ˆç¯€çœæˆæœ¬ï¼‰
        - ä½ç½®ä¿¡åº¦æ™‚ï¼šåŸ·è¡ŒAIé‡å¯«å’Œå„ªåŒ–æª¢ç´¢
        - ç‚ºäº†è©•ä¼°æ±ºç­–å“è³ªï¼Œæˆ‘å€‘æœƒåœ¨è·³éé‡å¯«æ™‚é¡å¤–åŸ·è¡Œé‡å¯«ä¾†æ¯”è¼ƒçµæœï¼Œ
          ä½†åœ¨å¯¦éš›ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œè·³éçš„é‡å¯«èª¿ç”¨ä¸æœƒè¢«åŸ·è¡Œã€‚
        """
        top_k = eval_params['top_k']
        similarity_threshold = eval_params['similarity_threshold']
        query_rewrite_count = eval_params['query_rewrite_count']
        confidence_threshold = eval_params.get('confidence_threshold', self.confidence_threshold)

        logger.info(f"é–‹å§‹è©•ä¼°æ™ºèƒ½è§¸ç™¼ç­–ç•¥ï¼Œåƒæ•¸: {eval_params}")
        logger.info(f"æ™ºèƒ½è§¸ç™¼é–€æª»: {confidence_threshold}")
        
        # é ä¼°æ™‚é–“ (æ™ºèƒ½è§¸ç™¼æœƒç¯€çœéƒ¨åˆ†AIé‡å¯«èª¿ç”¨)
        total_test_cases = len(test_cases)
        estimated_skip_rate = 0.3  # é ä¼°30%çš„æ¡ˆä¾‹æœƒè·³éé‡å¯«
        estimated_rewrite_calls = total_test_cases * (1 + estimated_skip_rate)  # åŸºæº– + éƒ¨åˆ†é‡å¯«
        estimated_time_minutes = max(1, estimated_rewrite_calls / 10)
        logger.info(f"â±ï¸  é ä¼°è©•ä¼°æ™‚é–“: ç´„ {estimated_time_minutes:.1f} åˆ†é˜")
        logger.info(f"ğŸ’° æ™ºèƒ½è§¸ç™¼é æœŸç¯€çœç´„ {estimated_skip_rate*100:.0f}% çš„AIé‡å¯«èª¿ç”¨")
        
        probe_search_metrics = []    # æ¢é‡æœç´¢ (åƒ…æ‘˜è¦ï¼Œç”¨æ–¼è§¸ç™¼)
        no_rewrite_metrics = []      # åŸºæº–æª¢ç´¢ (åŸå§‹æŸ¥è©¢ + RRF)
        always_rewrite_metrics = []  # ç¸½æ˜¯é‡å¯« (é‡å¯«æŸ¥è©¢ + RRF)
        smart_trigger_metrics = []   # æ™ºèƒ½è§¸ç™¼ç­–ç•¥
        
        trigger_stats = {
            "skip_rewrite_cases": 0,    # è·³éé‡å¯«çš„æ¡ˆä¾‹æ•¸
            "trigger_rewrite_cases": 0,  # è§¸ç™¼é‡å¯«çš„æ¡ˆä¾‹æ•¸
            "skip_correct_decisions": 0,  # è·³éé‡å¯«ä¸”çµæœæ›´å¥½çš„æ±ºç­–
            "skip_wrong_decisions": 0,   # è·³éé‡å¯«ä½†é‡å¯«æœƒæ›´å¥½çš„æ±ºç­–
            "trigger_correct_decisions": 0,  # è§¸ç™¼é‡å¯«ä¸”çµæœæ›´å¥½çš„æ±ºç­–
            "trigger_wrong_decisions": 0    # è§¸ç™¼é‡å¯«ä½†ä¸é‡å¯«æœƒæ›´å¥½çš„æ±ºç­–
        }
        
        detailed_case_results = []
        verbose_mode = eval_params.get('verbose_mode', False)
        
        for i, test_case in enumerate(test_cases):
            question = test_case.get('question') or test_case.get('user_query')
            expected_doc_ids = test_case.get('expected_relevant_doc_ids', [])
            logger.info(f"--- [æ¡ˆä¾‹ {i+1}/{len(test_cases)}] æ™ºèƒ½è§¸ç™¼è©•ä¼°: {question[:50]}... ---")
            
            if not question or not expected_doc_ids:
                logger.warning(f"è·³éæ¡ˆä¾‹ {i+1}ï¼Œç¼ºå°‘ 'question' æˆ– 'expected_relevant_doc_ids'")
                continue

            # 1. æ¢é‡æœç´¢ (Probe Search) - åƒ…ç”¨æ–¼æ™ºèƒ½è§¸ç™¼æ±ºç­–
            logger.info("1ï¸âƒ£  åŸ·è¡Œæ¢é‡æœç´¢ (ç”¨æ–¼æ±ºç­–)...")
            probe_results = await self._search_probe_via_api(question, top_k, 0.0)
            probe_retrieved_ids = [doc['document_id'] for doc in probe_results]
            
            # å¾æ¢é‡çµæœä¸­ç²å–çœŸå¯¦çš„ similarity_score
            if probe_results:
                first_result = probe_results[0]
                top_probe_score = first_result.get('similarity_score', 0.0)
                logger.info(f"   æ¢é‡æœç´¢æœ€é«˜åˆ†: {top_probe_score:.4f}")
            else:
                top_probe_score = 0.0

            # è¨ˆç®—æ¢é‡æœç´¢çš„æŒ‡æ¨™ (åƒ…ä¾›åƒè€ƒ)
            probe_case_metrics = {
                "hit_rate": self._calculate_hit_rate(expected_doc_ids, probe_retrieved_ids),
                "mrr": self._calculate_mrr(expected_doc_ids, probe_retrieved_ids),
                "ndcg": self._calculate_ndcg(expected_doc_ids, probe_retrieved_ids)
            }
            probe_search_metrics.append(probe_case_metrics)

            # 2. åŸºæº–æª¢ç´¢ (No Rewrite) - ä½¿ç”¨åŸå§‹æŸ¥è©¢å’ŒRRFï¼Œä½œç‚ºæ€§èƒ½æ¯”è¼ƒåŸºæº–
            logger.info("2ï¸âƒ£  åŸ·è¡ŒåŸºæº–æª¢ç´¢ (åŸå§‹æŸ¥è©¢ + RRF)...")
            no_rewrite_results = await self._search_semantic_via_api(question, top_k, similarity_threshold)
            no_rewrite_retrieved_ids = [doc['document_id'] for doc in no_rewrite_results]
            no_rewrite_case_metrics = {
                "hit_rate": self._calculate_hit_rate(expected_doc_ids, no_rewrite_retrieved_ids),
                "mrr": self._calculate_mrr(expected_doc_ids, no_rewrite_retrieved_ids),
                "ndcg": self._calculate_ndcg(expected_doc_ids, no_rewrite_retrieved_ids)
            }
            no_rewrite_metrics.append(no_rewrite_case_metrics)

            # 3. ç¸½æ˜¯é‡å¯« (Always Rewrite) - ç‚ºäº†é€²è¡Œå…¨é¢çš„æ¯”è¼ƒï¼Œæˆ‘å€‘ç¸½æ˜¯éœ€è¦è¨ˆç®—"ç¸½æ˜¯é‡å¯«"çš„çµæœ
            logger.info("3ï¸âƒ£  åŸ·è¡Œ'ç¸½æ˜¯é‡å¯«'æµç¨‹ä»¥ä¾›æ¯”è¼ƒ...")
            rewritten_queries = await self._rewrite_query_via_api(question, query_rewrite_count)
            
            search_tasks = [self._search_semantic_via_api(rq, top_k, similarity_threshold) for rq in rewritten_queries]
            list_of_results_per_query = await asyncio.gather(*search_tasks)

            rrf_input = {}
            for j, single_query_results in enumerate(list_of_results_per_query):
                rrf_input[f"q_{j}"] = [
                    {"doc_id": r.get('document_id'), "score": r.get('score', r.get('similarity_score', 0.5))} 
                    for r in single_query_results if r.get('document_id')
                ]

            merged_rewrite_results = self._reciprocal_rank_fusion(rrf_input)
            rewrite_retrieved_ids = [item['doc_id'] for item in merged_rewrite_results[:top_k]]

            always_rewrite_case_metrics = {
                "hit_rate": self._calculate_hit_rate(expected_doc_ids, rewrite_retrieved_ids),
                "mrr": self._calculate_mrr(expected_doc_ids, rewrite_retrieved_ids),
                "ndcg": self._calculate_ndcg(expected_doc_ids, rewrite_retrieved_ids)
            }
            always_rewrite_metrics.append(always_rewrite_case_metrics)

            # 4. æ™ºèƒ½è§¸ç™¼æ±ºç­– (åŸºæ–¼æ¢é‡åˆ†æ•¸)
            logger.info(f"4ï¸âƒ£  æ™ºèƒ½è§¸ç™¼æ±ºç­– (æ¢é‡åˆ†æ•¸: {top_probe_score:.4f}, é–€æª»: {confidence_threshold})...")
            
            if top_probe_score > confidence_threshold:
                # æ±ºç­–ï¼šè·³éé‡å¯«ï¼Œä½¿ç”¨åŸºæº–æª¢ç´¢çµæœ
                smart_trigger_result_ids = no_rewrite_retrieved_ids
                smart_trigger_case_metrics = no_rewrite_case_metrics.copy()
                trigger_decision = "SKIP_REWRITE"
                trigger_stats["skip_rewrite_cases"] += 1
                
                logger.info(f"   æ±ºç­–: âœ… è·³éAIé‡å¯« (ä½¿ç”¨åŸºæº–RRFæª¢ç´¢çµæœ)")
                
                # åˆ¤æ–·é€™å€‹æ±ºç­–æ˜¯å¦æ­£ç¢º (èˆ‡"ç¸½æ˜¯é‡å¯«"æ¯”è¼ƒ)
                if no_rewrite_case_metrics["mrr"] >= always_rewrite_case_metrics["mrr"]:
                    trigger_stats["skip_correct_decisions"] += 1
                    decision_quality = "CORRECT"
                else:
                    trigger_stats["skip_wrong_decisions"] += 1
                    decision_quality = "WRONG"
                
                logger.info(f"   æ±ºç­–å“è³ª: {decision_quality} (åŸºæº–MRR={no_rewrite_case_metrics['mrr']:.3f} vs é‡å¯«MRR={always_rewrite_case_metrics['mrr']:.3f})")
                
            else:
                # æ±ºç­–ï¼šè§¸ç™¼é‡å¯«ï¼Œä½¿ç”¨RRFèåˆçµæœ
                smart_trigger_result_ids = rewrite_retrieved_ids
                smart_trigger_case_metrics = always_rewrite_case_metrics.copy()
                trigger_decision = "TRIGGER_REWRITE"
                trigger_stats["trigger_rewrite_cases"] += 1
                
                logger.info(f"   æ±ºç­–: ğŸ”„ è§¸ç™¼AIé‡å¯« (ä½¿ç”¨RRFèåˆçµæœ)")

                # åˆ¤æ–·é€™å€‹æ±ºç­–æ˜¯å¦æ­£ç¢º (èˆ‡"åŸºæº–æª¢ç´¢"æ¯”è¼ƒ)
                if always_rewrite_case_metrics["mrr"] >= no_rewrite_case_metrics["mrr"]:
                    trigger_stats["trigger_correct_decisions"] += 1
                    decision_quality = "CORRECT"
                else:
                    trigger_stats["trigger_wrong_decisions"] += 1
                    decision_quality = "WRONG"
                
                logger.info(f"   æ±ºç­–å“è³ª: {decision_quality} (é‡å¯«MRR={always_rewrite_case_metrics['mrr']:.3f} vs åŸºæº–MRR={no_rewrite_case_metrics['mrr']:.3f})")

            smart_trigger_metrics.append(smart_trigger_case_metrics)
            
            # è©³ç´°æ¡ˆä¾‹çµæœ
            if verbose_mode:
                detailed_case = {
                    "question": question,
                    "expected_doc_ids": expected_doc_ids,
                    "probe_top_score": top_probe_score,
                    "trigger_decision": trigger_decision,
                    "decision_quality": decision_quality,
                    "probe_metrics": probe_case_metrics,
                    "no_rewrite_metrics": no_rewrite_case_metrics,
                    "always_rewrite_metrics": always_rewrite_case_metrics,
                    "smart_trigger_metrics": smart_trigger_case_metrics,
                    "probe_retrieved_ids": probe_retrieved_ids,
                    "no_rewrite_retrieved_ids": no_rewrite_retrieved_ids,
                    "rewrite_retrieved_ids": rewrite_retrieved_ids,
                    "smart_trigger_retrieved_ids": smart_trigger_result_ids,
                    "rewritten_queries": rewritten_queries
                }
                detailed_case_results.append(detailed_case)
                
                # å³æ™‚é¡¯ç¤ºæ¯”è¼ƒçµæœ
                probe_mrr = probe_case_metrics['mrr']
                no_rewrite_mrr = no_rewrite_case_metrics['mrr']
                rewrite_mrr = always_rewrite_case_metrics['mrr']
                smart_mrr = smart_trigger_case_metrics['mrr']
                
                print(f"  æ¡ˆä¾‹ {i+1:2d}: æ¢é‡={probe_mrr:.3f} | åŸºæº–={no_rewrite_mrr:.3f} | é‡å¯«={rewrite_mrr:.3f} | æ™ºèƒ½={smart_mrr:.3f} | æ±ºç­–={trigger_decision[:4]} ({decision_quality})")
            
            logger.info(f"  åŸºæº– MRR: {no_rewrite_case_metrics['mrr']:.3f}, é‡å¯« MRR: {always_rewrite_case_metrics['mrr']:.3f}, æ™ºèƒ½ MRR: {smart_trigger_case_metrics['mrr']:.3f}")

        # è¨ˆç®—å¯¦éš›çš„æˆæœ¬ç¯€çœ
        processed_cases = len(no_rewrite_metrics)
        # åªæœ‰åœ¨è§¸ç™¼é‡å¯«æ™‚æ‰æ¶ˆè€—AIèª¿ç”¨
        actual_rewrite_calls = trigger_stats["trigger_rewrite_cases"]
        total_possible_calls = len(test_cases)
        cost_saving_percentage = ((total_possible_calls - actual_rewrite_calls) / max(total_possible_calls, 1)) * 100

        logger.info(f"ğŸ“Š æ™ºèƒ½è§¸ç™¼çµ±è¨ˆ:")
        logger.info(f"   å¯¦éš›è·³éç‡: {cost_saving_percentage:.1f}% ({trigger_stats['skip_rewrite_cases']}/{processed_cases})")
        logger.info(f"   ç¯€çœAIèª¿ç”¨: {trigger_stats['skip_rewrite_cases']} æ¬¡")
        
        # åŒ¯ç¸½çµæœ
        results = self._aggregate_smart_trigger_results(
            probe_search_metrics, no_rewrite_metrics, always_rewrite_metrics, smart_trigger_metrics, 
            trigger_stats, len(test_cases), eval_params
        )
        
        if verbose_mode:
            results["detailed_case_results"] = detailed_case_results
        
        return results

    async def evaluate_retrieval_accuracy(self, test_cases: List[Dict], eval_params: Dict) -> Dict[str, Any]:
        top_k = eval_params['top_k']
        similarity_threshold = eval_params['similarity_threshold']
        query_rewrite_count = eval_params['query_rewrite_count']

        logger.info(f"é–‹å§‹è©•ä¼°æª¢ç´¢ç³»çµ±ï¼Œåƒæ•¸: {eval_params}")
        
        total_rewrite_calls = len(test_cases)
        estimated_rewrite_time_minutes = max(1, total_rewrite_calls / 10)
        logger.info(f"â±ï¸  é ä¼°è©•ä¼°æ™‚é–“: è‡³å°‘ {estimated_rewrite_time_minutes:.1f} åˆ†é˜ (å—å¾Œç«¯æœå‹™é€Ÿç‡é™åˆ¶å½±éŸ¿)")
        logger.info(f"ğŸ“Š å°‡è™•ç† {total_rewrite_calls} å€‹æŸ¥è©¢å„ªåŒ–è«‹æ±‚")
        
        baseline_metrics = []
        optimized_metrics = []
        comparison_stats = defaultdict(int)
        detailed_case_results = []  # æ–°å¢ï¼šè©³ç´°æ¡ˆä¾‹çµæœ
        verbose_mode = eval_params.get('verbose_mode', False)
        
        for i, test_case in enumerate(test_cases):
            question = test_case.get('question') or test_case.get('user_query')
            expected_doc_ids = test_case.get('expected_relevant_doc_ids', [])
            logger.info(f"--- [æ¡ˆä¾‹ {i+1}/{len(test_cases)}] åŸå§‹å•é¡Œ: {question[:50]}... ---")
            
            if not question or not expected_doc_ids:
                logger.warning(f"è·³éæ¡ˆä¾‹ {i+1}ï¼Œç¼ºå°‘ 'question' æˆ– 'expected_relevant_doc_ids'")
                continue

            logger.info("åŸ·è¡ŒåŸºæº–æ¸¬è©¦ (åŸå§‹æŸ¥è©¢)...")
            baseline_results = await self._search_semantic_via_api(question, top_k, similarity_threshold)
            baseline_retrieved_ids = [doc['document_id'] for doc in baseline_results]
            
            current_baseline_metrics = {
                "hit_rate": self._calculate_hit_rate(expected_doc_ids, baseline_retrieved_ids),
                "mrr": self._calculate_mrr(expected_doc_ids, baseline_retrieved_ids),
                "ndcg": self._calculate_ndcg(expected_doc_ids, baseline_retrieved_ids)
            }
            baseline_metrics.append(current_baseline_metrics)

            logger.info("åŸ·è¡Œå„ªåŒ–æ¸¬è©¦ (AIé‡å¯«æŸ¥è©¢)...")
            rewritten_queries = await self._rewrite_query_via_api(question, query_rewrite_count)
            
            search_tasks = [self._search_semantic_via_api(rq, top_k, similarity_threshold) for rq in rewritten_queries]
            list_of_results_per_query = await asyncio.gather(*search_tasks)

            rrf_input = {}
            for i, single_query_results in enumerate(list_of_results_per_query):
                rrf_input[f"q_{i}"] = [
                    {"doc_id": r.get('document_id'), "score": (r.get('score') or r.get('similarity_score', 0.5))} 
                    for r in single_query_results if r.get('document_id')
                ]

            merged_optimized_results = self._reciprocal_rank_fusion(rrf_input)
            optimized_retrieved_ids = [item['doc_id'] for item in merged_optimized_results[:top_k]]

            current_optimized_metrics = {
                "hit_rate": self._calculate_hit_rate(expected_doc_ids, optimized_retrieved_ids),
                "mrr": self._calculate_mrr(expected_doc_ids, optimized_retrieved_ids),
                "ndcg": self._calculate_ndcg(expected_doc_ids, optimized_retrieved_ids)
            }
            optimized_metrics.append(current_optimized_metrics)
            
            optimized_results_for_comparison = [{"document_id": doc_id} for doc_id in optimized_retrieved_ids]
            case_status = self._compare_performance(expected_doc_ids, baseline_results, optimized_results_for_comparison)
            comparison_stats[case_status] += 1
            
            # è©³ç´°æ¡ˆä¾‹çµæœï¼ˆç”¨æ–¼è©³ç´°æ¨¡å¼è¼¸å‡ºï¼‰
            if verbose_mode:
                detailed_case = {
                    "question": question,
                    "expected_doc_ids": expected_doc_ids,
                    "baseline_metrics": current_baseline_metrics,
                    "optimized_metrics": current_optimized_metrics,
                    "baseline_retrieved_ids": baseline_retrieved_ids,
                    "optimized_retrieved_ids": optimized_retrieved_ids,
                    "rewritten_queries": rewritten_queries,
                    "case_status": case_status
                }
                detailed_case_results.append(detailed_case)
                
                # å³æ™‚é¡¯ç¤ºè©³ç´°çµæœ
                improvement = current_optimized_metrics['mrr'] - current_baseline_metrics['mrr']
                status_icon = "ğŸš€" if improvement > 0.1 else "âœ…" if improvement > 0.01 else "âŒ" if improvement < -0.01 else "âš–ï¸"
                print(f"  æ¡ˆä¾‹ {i+1:2d}: {status_icon} åŸºæº–MRR={current_baseline_metrics['mrr']:.3f} | å„ªåŒ–MRR={current_optimized_metrics['mrr']:.3f} | è®ŠåŒ–={improvement:+.3f}")
            
            logger.info(f"  åŸºæº–æ¸¬è©¦ Hit Rate: {current_baseline_metrics['hit_rate']['hit_rate']:.2f}, MRR: {current_baseline_metrics['mrr']:.2f}")
            logger.info(f"  å„ªåŒ–æ¸¬è©¦ Hit Rate: {current_optimized_metrics['hit_rate']['hit_rate']:.2f}, MRR: {current_optimized_metrics['mrr']:.2f}")

        # åŒ¯ç¸½çµæœ
        results = self._aggregate_results(baseline_metrics, optimized_metrics, comparison_stats, len(test_cases), eval_params)
        
        # æ·»åŠ è©³ç´°æ¡ˆä¾‹çµæœ
        if verbose_mode:
            results["detailed_case_results"] = detailed_case_results
        
        return results

    def _reciprocal_rank_fusion(self, search_results_dict: Dict[str, List[Dict]], k: int = 60) -> List[Dict]:
        ranked_lists = defaultdict(dict)
        for query_id, results in search_results_dict.items():
            for i, result in enumerate(results):
                if result.get('doc_id'):
                    ranked_lists[query_id][result['doc_id']] = i + 1

        rrf_scores = defaultdict(float)
        all_doc_ids = set(res.get('doc_id') for q_results in search_results_dict.values() for res in q_results if res.get('doc_id'))
        
        for doc_id in all_doc_ids:
            for query_id, ranks in ranked_lists.items():
                rank = ranks.get(doc_id)
                if rank is not None:
                    rrf_scores[doc_id] += 1 / (k + rank)
        
        sorted_results = sorted(rrf_scores.items(), key=lambda item: item[1], reverse=True)
        return [{"doc_id": doc_id, "rrf_score": score} for doc_id, score in sorted_results]

    def _calculate_hit_rate(self, expected_ids: List[str], retrieved_ids: List[str]) -> Dict[str, float]:
        expected_set = set(expected_ids)
        hits = len(expected_set.intersection(set(retrieved_ids)))
        return {"hit_rate": hits / len(expected_ids) if expected_ids else 0.0, "hits": float(hits)}

    def _calculate_mrr(self, expected_ids: List[str], retrieved_ids: List[str]) -> float:
        for i, doc_id in enumerate(retrieved_ids):
            if doc_id in expected_ids:
                return 1.0 / (i + 1)
        return 0.0

    def _calculate_ndcg(self, expected_ids: List[str], retrieved_ids: List[str], k_val=10) -> float:
        relevance = [1 if doc_id in expected_ids else 0 for doc_id in retrieved_ids[:k_val]]
        
        dcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevance))
        
        ideal_relevance = sorted(relevance, reverse=True)
        idcg = sum((2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_relevance))
        
        return dcg / idcg if idcg > 0 else 0.0

    def _compare_performance(self, expected_ids: List[str], original_results: List[Dict], optimization_results: List[Dict], k: int = 5) -> str:
        """æ¯”è¼ƒå…©ç¨®æ–¹æ³•çš„æ€§èƒ½ï¼Œä½¿ç”¨ MRR ä½œç‚ºä¸»è¦æŒ‡æ¨™"""
        # æå–æ–‡ä»¶IDåˆ—è¡¨
        original_ids = [doc.get('document_id') for doc in original_results[:k]]
        optimized_ids = [doc.get('document_id') for doc in optimization_results[:k]]

        # è¨ˆç®— MRR (Mean Reciprocal Rank)
        original_mrr = self._calculate_mrr(expected_ids, original_ids)
        optimized_mrr = self._calculate_mrr(expected_ids, optimized_ids)

        # ä½¿ç”¨ MRR ä¾†åˆ¤æ–·æ€§èƒ½è®ŠåŒ–ï¼Œè¨­å®šé–¾å€¼ç‚º 0.01 ä»¥é¿å…å¾®å°æµ®é»æ•¸å·®ç•°
        mrr_difference = optimized_mrr - original_mrr
        
        if mrr_difference > 0.01:  # MRR æå‡è¶…é 0.01
            return "IMPROVED"
        elif mrr_difference < -0.01:  # MRR ä¸‹é™è¶…é 0.01
            return "REGRESSED"
        else:
            return "NEUTRAL"

    def _aggregate_results(self, baseline_metrics: List[Dict], optimized_metrics: List[Dict], comparison_stats: Dict, total_cases: int, eval_params: Dict) -> Dict:
        def calculate_mean_scores(metrics_list):
            if not metrics_list:
                return {"hit_rate": {"hit_rate": 0.0}, "mrr": 0.0, "ndcg": 0.0}
            
            avg_hit_rate = np.mean([m['hit_rate']['hit_rate'] for m in metrics_list])
            avg_mrr = np.mean([m['mrr'] for m in metrics_list])
            avg_ndcg = np.mean([m['ndcg'] for m in metrics_list])
            
            return {"hit_rate": {"hit_rate": avg_hit_rate}, "mrr": avg_mrr, "ndcg": avg_ndcg}
        
        baseline_scores = calculate_mean_scores(baseline_metrics)
        optimized_scores = calculate_mean_scores(optimized_metrics)
        
        # è¨ˆç®—æ¡ˆä¾‹ç´šåˆ¥åˆ†æ
        improved_cases = comparison_stats.get('IMPROVED', 0)
        degraded_cases = comparison_stats.get('REGRESSED', 0)
        unchanged_cases = comparison_stats.get('NEUTRAL', 0)
        processed_cases = improved_cases + degraded_cases + unchanged_cases
        
        # ç”Ÿæˆæ™ºèƒ½å»ºè­°
        mrr_improvement = optimized_scores['mrr'] - baseline_scores['mrr']
        mrr_improvement_pct = (mrr_improvement / max(baseline_scores['mrr'], 0.001)) * 100
        
        recommendations = []
        if mrr_improvement_pct > 15:
            recommendations.append("ğŸš€ å¼·çƒˆå»ºè­°å•Ÿç”¨æŸ¥è©¢é‡å¯«ï¼æ€§èƒ½æå‡éå¸¸é¡¯è‘—ã€‚")
        elif mrr_improvement_pct > 5:
            recommendations.append("âœ… å»ºè­°å•Ÿç”¨æŸ¥è©¢é‡å¯«ã€‚å®ƒå°æª¢ç´¢æ€§èƒ½æœ‰æ˜é¡¯çš„æ­£é¢å½±éŸ¿ã€‚")
        elif mrr_improvement_pct > 0:
            recommendations.append("ğŸ‘ æŸ¥è©¢é‡å¯«å¸¶ä¾†äº†è¼•å¾®çš„æ€§èƒ½æå‡ï¼Œå¯ä»¥è€ƒæ…®å•Ÿç”¨ã€‚")
        elif mrr_improvement_pct == 0:
            recommendations.append("âš–ï¸ æŸ¥è©¢é‡å¯«å°æ€§èƒ½æ²’æœ‰å½±éŸ¿ã€‚å¯ä»¥æ ¹æ“šè¨ˆç®—æˆæœ¬æ±ºå®šæ˜¯å¦å•Ÿç”¨ã€‚")
        
        # æ·»åŠ å…·é«”çš„æ”¹é€²å»ºè­°
        if improved_cases > 0:
            recommendations.append(f"ğŸ“Š æœ‰ {improved_cases}/{processed_cases} å€‹æ¡ˆä¾‹ç²å¾—æ”¹é€²ï¼Œå»ºè­°åˆ†ææˆåŠŸæ¡ˆä¾‹çš„æ¨¡å¼ã€‚")
        
        if degraded_cases > 0:
            recommendations.append(f"âš ï¸ æœ‰ {degraded_cases}/{processed_cases} å€‹æ¡ˆä¾‹æ€§èƒ½ä¸‹é™ï¼Œå»ºè­°æª¢æŸ¥é‡å¯«è³ªé‡ã€‚")

        final_results = {
            "evaluation_parameters": eval_params,
            "total_test_cases": total_cases,
            "processed_cases": processed_cases,
            "performance_comparison": dict(comparison_stats),
            "baseline_performance": {
                "description": "Using original query with semantic search.",
                "hit_rate": baseline_scores['hit_rate'],
                "mrr": baseline_scores['mrr'],
                "ndcg": baseline_scores['ndcg']
            },
            "optimized_performance": {
                "description": "Using AI-rewritten queries with semantic search and RRF fusion.",
                "hit_rate": optimized_scores['hit_rate'],
                "mrr": optimized_scores['mrr'],
                "ndcg": optimized_scores['ndcg']
            },
            "performance_delta": {
                "hit_rate_change": optimized_scores['hit_rate']['hit_rate'] - baseline_scores['hit_rate']['hit_rate'],
                "mrr_change": optimized_scores['mrr'] - baseline_scores['mrr'],
                "ndcg_change": optimized_scores['ndcg'] - baseline_scores['ndcg']
            },
            "case_level_analysis": {
                "improved_cases": improved_cases,
                "degraded_cases": degraded_cases,
                "unchanged_cases": unchanged_cases
            },
            "recommendations": recommendations,
            "query_rewrite_analysis": {
                "average_rewrites_per_query": eval_params.get('query_rewrite_count', 3),
                "successful_rewrite_rate": 100.0  # é€™å€‹éœ€è¦åœ¨å¯¦éš›è©•ä¼°ä¸­è¨ˆç®—
            }
        }
        return final_results
    
    def _aggregate_smart_trigger_results(
        self, 
        probe_metrics: List[Dict],
        no_rewrite_metrics: List[Dict],
        always_rewrite_metrics: List[Dict], 
        smart_trigger_metrics: List[Dict], 
        trigger_stats: Dict,
        total_cases: int, 
        eval_params: Dict
    ) -> Dict:
        """èšåˆæ™ºèƒ½è§¸ç™¼ç­–ç•¥çš„è©•ä¼°çµæœ"""
        
        def calculate_mean_scores(metrics_list):
            if not metrics_list:
                return {"hit_rate": {"hit_rate": 0.0}, "mrr": 0.0, "ndcg": 0.0}
            
            avg_hit_rate = np.mean([m['hit_rate']['hit_rate'] for m in metrics_list])
            avg_mrr = np.mean([m['mrr'] for m in metrics_list])
            avg_ndcg = np.mean([m['ndcg'] for m in metrics_list])
            
            return {"hit_rate": {"hit_rate": avg_hit_rate}, "mrr": avg_mrr, "ndcg": avg_ndcg}
        
        probe_scores = calculate_mean_scores(probe_metrics)
        no_rewrite_scores = calculate_mean_scores(no_rewrite_metrics)
        always_rewrite_scores = calculate_mean_scores(always_rewrite_metrics)
        smart_trigger_scores = calculate_mean_scores(smart_trigger_metrics)
        
        processed_cases = len(no_rewrite_metrics)
        
        # è¨ˆç®—æ™ºèƒ½è§¸ç™¼çš„æ•ˆç‡åˆ†æ
        cost_saving_estimation = trigger_stats["skip_rewrite_cases"] / max(processed_cases, 1) * 100
        trigger_rate = trigger_stats["trigger_rewrite_cases"] / max(processed_cases, 1) * 100
        
        # è¨ˆç®—æ±ºç­–æº–ç¢ºç‡
        total_skip_decisions = trigger_stats["skip_rewrite_cases"]
        total_trigger_decisions = trigger_stats["trigger_rewrite_cases"]
        
        skip_accuracy = (trigger_stats["skip_correct_decisions"] / max(total_skip_decisions, 1)) * 100
        trigger_accuracy = (trigger_stats["trigger_correct_decisions"] / max(total_trigger_decisions, 1)) * 100
        overall_accuracy = ((trigger_stats["skip_correct_decisions"] + trigger_stats["trigger_correct_decisions"]) / max(processed_cases, 1)) * 100
        
        # ç”Ÿæˆæ™ºèƒ½å»ºè­°
        smart_vs_baseline_improvement = smart_trigger_scores['mrr'] - no_rewrite_scores['mrr']
        smart_vs_always_rewrite_comparison = smart_trigger_scores['mrr'] - always_rewrite_scores['mrr']
        
        recommendations = []
        
        # æ€§èƒ½åˆ†æ
        if smart_vs_baseline_improvement > 0.05:
            recommendations.append(f"ğŸš€ æ™ºèƒ½è§¸ç™¼ç­–ç•¥ç›¸æ¯”åŸºæº–æª¢ç´¢æœ‰é¡¯è‘—æå‡ (MRR {smart_vs_baseline_improvement:+.2%})ï¼")
        elif smart_vs_baseline_improvement > 0.01:
            recommendations.append(f"âœ… æ™ºèƒ½è§¸ç™¼ç­–ç•¥ç›¸æ¯”åŸºæº–æª¢ç´¢æœ‰è¼•å¾®æå‡ (MRR {smart_vs_baseline_improvement:+.2%})ã€‚")
        
        if smart_vs_always_rewrite_comparison >= -0.01:
            recommendations.append("ğŸ¯ æ™ºèƒ½è§¸ç™¼ç­–ç•¥é”åˆ°äº†èˆ‡ç¸½æ˜¯é‡å¯«ç›¸ç•¶çš„æ€§èƒ½ï¼ŒåŒæ™‚ç¯€çœäº†è¨ˆç®—æˆæœ¬ã€‚")
        else:
            recommendations.append(f"âš ï¸ æ™ºèƒ½è§¸ç™¼ç­–ç•¥çš„æ€§èƒ½ç•¥ä½æ–¼ç¸½æ˜¯é‡å¯« ({smart_vs_always_rewrite_comparison:.2%})ï¼Œå»ºè­°èª¿æ•´è§¸ç™¼é–€æª»ã€‚")
        
        # æ•ˆç‡åˆ†æ
        if cost_saving_estimation > 50:
            recommendations.append(f"ğŸ’° æ™ºèƒ½è§¸ç™¼ç­–ç•¥è·³éäº† {cost_saving_estimation:.1f}% çš„é‡å¯«æ“ä½œï¼Œå¤§å¹…ç¯€çœè¨ˆç®—æˆæœ¬ã€‚")
        elif cost_saving_estimation > 30:
            recommendations.append(f"ğŸ’¡ æ™ºèƒ½è§¸ç™¼ç­–ç•¥è·³éäº† {cost_saving_estimation:.1f}% çš„é‡å¯«æ“ä½œï¼Œæœ‰æ•ˆç¯€çœæˆæœ¬ã€‚")
        
        # æ±ºç­–æº–ç¢ºæ€§åˆ†æ
        if overall_accuracy > 80:
            recommendations.append(f"ğŸ¯ æ™ºèƒ½è§¸ç™¼çš„æ±ºç­–æº–ç¢ºç‡å¾ˆé«˜ ({overall_accuracy:.1f}%)ï¼Œç­–ç•¥é‹ä½œè‰¯å¥½ã€‚")
        elif overall_accuracy > 60:
            recommendations.append(f"ğŸ‘ æ™ºèƒ½è§¸ç™¼çš„æ±ºç­–æº–ç¢ºç‡å°šå¯ ({overall_accuracy:.1f}%)ï¼Œå¯è€ƒæ…®å¾®èª¿é–€æª»ã€‚")
        else:
            recommendations.append(f"âš ï¸ æ™ºèƒ½è§¸ç™¼çš„æ±ºç­–æº–ç¢ºç‡è¼ƒä½ ({overall_accuracy:.1f}%)ï¼Œå»ºè­°é‡æ–°è©•ä¼°è§¸ç™¼é–€æª»ã€‚")
        
        # é–€æª»èª¿æ•´å»ºè­°
        confidence_threshold = eval_params.get('confidence_threshold', 0.75)
        if trigger_stats["skip_wrong_decisions"] > trigger_stats["skip_correct_decisions"]:
            recommendations.append(f"ğŸ“‰ å»ºè­°é™ä½è§¸ç™¼é–€æª» (ç•¶å‰: {confidence_threshold})ï¼Œä»¥æ¸›å°‘éŒ¯èª¤çš„è·³éæ±ºç­–ã€‚")
        elif trigger_stats["trigger_wrong_decisions"] > trigger_stats["trigger_correct_decisions"]:
            recommendations.append(f"ğŸ“ˆ å»ºè­°æé«˜è§¸ç™¼é–€æª» (ç•¶å‰: {confidence_threshold})ï¼Œä»¥æ¸›å°‘ä¸å¿…è¦çš„é‡å¯«ã€‚")
        
        results = {
            "evaluation_type": "smart_trigger_strategy",
            "evaluation_parameters": eval_params,
            "total_test_cases": total_cases,
            "processed_cases": processed_cases,
            
            "probe_search_performance": {
                "description": "æ¢é‡æœç´¢ - åƒ…å°æ‘˜è¦å‘é‡æœç´¢ (ç”¨æ–¼è§¸ç™¼æ±ºç­–)",
                "hit_rate": probe_scores['hit_rate'],
                "mrr": probe_scores['mrr'],
                "ndcg": probe_scores['ndcg']
            },
            
            "no_rewrite_performance": {
                "description": "åŸºæº–æª¢ç´¢ - åŸå§‹æŸ¥è©¢ + RRFèåˆæœç´¢",
                "hit_rate": no_rewrite_scores['hit_rate'],
                "mrr": no_rewrite_scores['mrr'],
                "ndcg": no_rewrite_scores['ndcg']
            },

            "always_rewrite_performance": {
                "description": "ç¸½æ˜¯é‡å¯« - å°æ‰€æœ‰æŸ¥è©¢é€²è¡ŒAIé‡å¯«å’ŒRRFèåˆ",
                "hit_rate": always_rewrite_scores['hit_rate'],
                "mrr": always_rewrite_scores['mrr'],
                "ndcg": always_rewrite_scores['ndcg']
            },
            
            "smart_trigger_performance": {
                "description": f"æ™ºèƒ½è§¸ç™¼ - æ ¹æ“šæ¢é‡åˆ†æ•¸ ({eval_params.get('confidence_threshold', 'N/A')}) æ±ºå®šæ˜¯å¦é‡å¯«",
                "hit_rate": smart_trigger_scores['hit_rate'],
                "mrr": smart_trigger_scores['mrr'],
                "ndcg": smart_trigger_scores['ndcg']
            },
            
            "performance_comparison": {
                "smart_vs_baseline": {
                    "hit_rate_change": smart_trigger_scores['hit_rate']['hit_rate'] - no_rewrite_scores['hit_rate']['hit_rate'],
                    "mrr_change": smart_vs_baseline_improvement,
                    "ndcg_change": smart_trigger_scores['ndcg'] - no_rewrite_scores['ndcg']
                },
                "smart_vs_always_rewrite": {
                    "hit_rate_change": smart_trigger_scores['hit_rate']['hit_rate'] - always_rewrite_scores['hit_rate']['hit_rate'],
                    "mrr_change": smart_vs_always_rewrite_comparison,
                    "ndcg_change": smart_trigger_scores['ndcg'] - always_rewrite_scores['ndcg']
                },
                "always_rewrite_vs_baseline": {
                    "hit_rate_change": always_rewrite_scores['hit_rate']['hit_rate'] - no_rewrite_scores['hit_rate']['hit_rate'],
                    "mrr_change": always_rewrite_scores['mrr'] - no_rewrite_scores['mrr'],
                    "ndcg_change": always_rewrite_scores['ndcg'] - no_rewrite_scores['ndcg']
                }
            },
            
            "trigger_analysis": {
                "skip_rewrite_rate": cost_saving_estimation,
                "trigger_rewrite_rate": trigger_rate,
                "skip_decision_accuracy": skip_accuracy,
                "trigger_decision_accuracy": trigger_accuracy,
                "overall_decision_accuracy": overall_accuracy,
                "cost_saving_estimation_percentage": cost_saving_estimation
            },
            
            "decision_breakdown": trigger_stats,
            "recommendations": recommendations
        }
        
        return results
    
    def print_results(self, results: Dict[str, Any]):
        """ä»¥å°äººé¡å‹å¥½çš„æ ¼å¼è¼¸å‡ºè©•ä¼°çµæœ - æ”¹å–„ç‰ˆæœ¬ï¼Œåƒè€ƒ evaluate_vector_retrieval.py"""
        print("\n" + "="*100)
        print("ğŸš€ æŸ¥è©¢é‡å¯«æª¢ç´¢ vs åŸºæº–æª¢ç´¢ - è©³ç´°æ€§èƒ½åˆ†æå ±å‘Š")
        print("="*100)
        
        # åŸºæœ¬çµ±è¨ˆä¿¡æ¯
        total_cases = results.get("total_test_cases", 0)
        processed_cases = results.get("processed_cases", 0)
        eval_params = results.get("evaluation_parameters", {})
        
        print(f"\nğŸ“‹ è©•ä¼°æ¦‚æ³:")
        print(f"   ğŸ¯ ç¸½æ¸¬è©¦æ¡ˆä¾‹æ•¸: {total_cases}")
        print(f"   âœ… å·²è™•ç†æ¡ˆä¾‹æ•¸: {processed_cases}")
        print(f"   âš™ï¸  è©•ä¼°åƒæ•¸: Top-K={eval_params.get('top_k', 'N/A')}, " +
              f"ç›¸ä¼¼åº¦é–¾å€¼={eval_params.get('similarity_threshold', 'N/A')}, " +
              f"æŸ¥è©¢é‡å¯«æ•¸é‡={eval_params.get('query_rewrite_count', 'N/A')}")
        
        # é¡¯ç¤ºåŸºæº–æ¸¬è©¦çµæœ
        baseline_metrics = results.get("baseline_performance", {})
        print(f"\nğŸ“Š åŸºæº–æ¸¬è©¦çµæœ (åŸå§‹æŸ¥è©¢):")
        if baseline_metrics:
            baseline_mrr = baseline_metrics.get("mrr", 0.0)
            baseline_hr = baseline_metrics.get("hit_rate", {}).get("hit_rate", 0.0)
            baseline_ndcg = baseline_metrics.get("ndcg", 0.0)
            
            print(f"   - MRR (Mean Reciprocal Rank): {baseline_mrr:.4f}")
            print(f"   - Hit Rate: {baseline_hr:.4f}")
            print(f"   - nDCG: {baseline_ndcg:.4f}")
        
        # é¡¯ç¤ºå„ªåŒ–æ¸¬è©¦çµæœ
        optimized_metrics = results.get("optimized_performance", {})
        print(f"\nğŸš€ å„ªåŒ–æ¸¬è©¦çµæœ (AIé‡å¯«æŸ¥è©¢ + RRFèåˆ):")
        if optimized_metrics:
            optimized_mrr = optimized_metrics.get("mrr", 0.0)
            optimized_hr = optimized_metrics.get("hit_rate", {}).get("hit_rate", 0.0)
            optimized_ndcg = optimized_metrics.get("ndcg", 0.0)
            
            print(f"   - MRR (Mean Reciprocal Rank): {optimized_mrr:.4f}")
            print(f"   - Hit Rate: {optimized_hr:.4f}")
            print(f"   - nDCG: {optimized_ndcg:.4f}")
        
        # æ€§èƒ½å°æ¯”åˆ†æ
        print(f"\nğŸ“ˆ æ€§èƒ½å°æ¯”åˆ†æ:")
        
        # MRRå°æ¯”
        mrr_improvement = optimized_mrr - baseline_mrr
        mrr_improvement_pct = (mrr_improvement / max(baseline_mrr, 0.001)) * 100
        
        print(f"   ğŸ¯ MRR å°æ¯”:")
        print(f"      åŸºæº–: {baseline_mrr:.4f} â†’ å„ªåŒ–: {optimized_mrr:.4f} (è®ŠåŒ–: {mrr_improvement:+.4f}, {mrr_improvement_pct:+.2f}%)")
        
        # Hit Rateå°æ¯”
        print(f"   ğŸ“Š Hit Rate å°æ¯”:")
        hr_improvement = optimized_hr - baseline_hr
        hr_improvement_pct = (hr_improvement / max(baseline_hr, 0.001)) * 100
        print(f"      å¹³å‡: {baseline_hr:.4f} â†’ {optimized_hr:.4f} (è®ŠåŒ–: {hr_improvement:+.4f}, {hr_improvement_pct:+.2f}%)")
        
        # nDCGå°æ¯”
        print(f"   ğŸ“ˆ nDCG å°æ¯”:")
        ndcg_improvement = optimized_ndcg - baseline_ndcg
        ndcg_improvement_pct = (ndcg_improvement / max(baseline_ndcg, 0.001)) * 100
        print(f"      å¹³å‡: {baseline_ndcg:.4f} â†’ {optimized_ndcg:.4f} (è®ŠåŒ–: {ndcg_improvement:+.4f}, {ndcg_improvement_pct:+.2f}%)")
        
        # æ¡ˆä¾‹ç´šåˆ¥åˆ†æ
        case_improvements = results.get("case_level_analysis", {})
        if case_improvements:
            improved_cases = case_improvements.get("improved_cases", 0)
            degraded_cases = case_improvements.get("degraded_cases", 0)
            unchanged_cases = case_improvements.get("unchanged_cases", 0)
            
            print(f"\nğŸ” æ¡ˆä¾‹ç´šåˆ¥åˆ†æ:")
            print(f"   ğŸ“ˆ æ€§èƒ½æå‡æ¡ˆä¾‹: {improved_cases} ({improved_cases/max(processed_cases, 1)*100:.1f}%)")
            print(f"   ğŸ“‰ æ€§èƒ½ä¸‹é™æ¡ˆä¾‹: {degraded_cases} ({degraded_cases/max(processed_cases, 1)*100:.1f}%)")
            print(f"   âš–ï¸  æ€§èƒ½æŒå¹³æ¡ˆä¾‹: {unchanged_cases} ({unchanged_cases/max(processed_cases, 1)*100:.1f}%)")
        
        # æ™ºèƒ½å»ºè­°
        recommendations = results.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ æ™ºèƒ½å»ºè­°:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        # æŸ¥è©¢é‡å¯«æ•ˆæœåˆ†æ
        rewrite_analysis = results.get("query_rewrite_analysis", {})
        if rewrite_analysis:
            print(f"\nğŸ¤– æŸ¥è©¢é‡å¯«æ•ˆæœåˆ†æ:")
            avg_rewrites = rewrite_analysis.get("average_rewrites_per_query", 0)
            successful_rewrites = rewrite_analysis.get("successful_rewrite_rate", 0)
            print(f"   å¹³å‡æ¯æŸ¥è©¢é‡å¯«æ•¸é‡: {avg_rewrites:.1f}")
            print(f"   æˆåŠŸé‡å¯«ç‡: {successful_rewrites:.1f}%")
        
        print("="*100 + "\n")
    
    def print_smart_trigger_results(self, results: Dict[str, Any]):
        """è¼¸å‡ºæ™ºèƒ½è§¸ç™¼ç­–ç•¥çš„è©•ä¼°çµæœ"""
        print("\n" + "="*100)
        print("ï¿½ï¿½ æ™ºèƒ½è§¸ç™¼ç­–ç•¥ vs ç¸½æ˜¯é‡å¯« vs åŸºæº–æª¢ç´¢ - ç¶œåˆæ€§èƒ½åˆ†æå ±å‘Š")
        print("="*100)
        
        # åŸºæœ¬çµ±è¨ˆä¿¡æ¯
        total_cases = results.get("total_test_cases", 0)
        processed_cases = results.get("processed_cases", 0)
        eval_params = results.get("evaluation_parameters", {})
        
        print(f"\nğŸ“‹ è©•ä¼°æ¦‚æ³:")
        print(f"   ğŸ¯ ç¸½æ¸¬è©¦æ¡ˆä¾‹æ•¸: {total_cases}")
        print(f"   âœ… å·²è™•ç†æ¡ˆä¾‹æ•¸: {processed_cases}")
        print(f"   âš™ï¸  è©•ä¼°åƒæ•¸: Top-K={eval_params.get('top_k', 'N/A')}, " +
              f"ç›¸ä¼¼åº¦é–¾å€¼={eval_params.get('similarity_threshold', 'N/A')}, " +
              f"è§¸ç™¼é–€æª»={eval_params.get('confidence_threshold', 'N/A')}")
        
        # å››ç¨®ç­–ç•¥çš„æ€§èƒ½æ¯”è¼ƒ
        probe_metrics = results.get("probe_search_performance", {})
        no_rewrite_metrics = results.get("no_rewrite_performance", {})
        always_rewrite_metrics = results.get("always_rewrite_performance", {})
        smart_trigger_metrics = results.get("smart_trigger_performance", {})
        
        print(f"\nğŸ“Š å››ç¨®ç­–ç•¥æ€§èƒ½æ¯”è¼ƒ (MRR: å¹³å‡å€’æ•¸æ’å, Hit Rate: å‘½ä¸­ç‡, nDCG: æ¨™æº–åŒ–æŠ˜æ‰£ç´¯ç©å¢ç›Š):")
        
        # è¡¨æ ¼å½¢å¼é¡¯ç¤º
        print(f"{'ç­–ç•¥':<16} {'MRR':<8} {'Hit Rate':<10} {'nDCG':<8} {'èªªæ˜'}")
        print(f"{'-'*16} {'-'*8} {'-'*10} {'-'*8} {'-'*30}")
        
        if probe_metrics:
            probe_mrr = probe_metrics.get("mrr", 0.0)
            probe_hr = probe_metrics.get("hit_rate", {}).get("hit_rate", 0.0)
            probe_ndcg = probe_metrics.get("ndcg", 0.0)
            print(f"{'æ¢é‡æœç´¢ (æ±ºç­–ç”¨)':<16} {probe_mrr:<8.4f} {probe_hr:<10.4f} {probe_ndcg:<8.4f} {'åƒ…æ‘˜è¦æœç´¢ï¼Œç”¨æ–¼è§¸ç™¼æ±ºç­–'}")
        
        if no_rewrite_metrics:
            no_rewrite_mrr = no_rewrite_metrics.get("mrr", 0.0)
            no_rewrite_hr = no_rewrite_metrics.get("hit_rate", {}).get("hit_rate", 0.0)
            no_rewrite_ndcg = no_rewrite_metrics.get("ndcg", 0.0)
            print(f"{'åŸºæº–æª¢ç´¢ (RRF)':<16} {no_rewrite_mrr:<8.4f} {no_rewrite_hr:<10.4f} {no_rewrite_ndcg:<8.4f} {'åŸå§‹æŸ¥è©¢ + RRFèåˆ'}")

        if always_rewrite_metrics:
            always_mrr = always_rewrite_metrics.get("mrr", 0.0)
            always_hr = always_rewrite_metrics.get("hit_rate", {}).get("hit_rate", 0.0)
            always_ndcg = always_rewrite_metrics.get("ndcg", 0.0)
            print(f"{'ç¸½æ˜¯é‡å¯«':<16} {always_mrr:<8.4f} {always_hr:<10.4f} {always_ndcg:<8.4f} {'AIé‡å¯«æŸ¥è©¢ + RRFèåˆ'}")
        
        if smart_trigger_metrics:
            smart_mrr = smart_trigger_metrics.get("mrr", 0.0)
            smart_hr = smart_trigger_metrics.get("hit_rate", {}).get("hit_rate", 0.0)
            smart_ndcg = smart_trigger_metrics.get("ndcg", 0.0)
            print(f"{'æ™ºèƒ½è§¸ç™¼':<16} {smart_mrr:<8.4f} {smart_hr:<10.4f} {smart_ndcg:<8.4f} {'æ ¹æ“šæ¢é‡åˆ†æ•¸å‹•æ…‹é¸æ“‡ç­–ç•¥'}")
        
        # æ€§èƒ½è®ŠåŒ–åˆ†æ
        performance_comparison = results.get("performance_comparison", {})
        
        print(f"\nğŸ“ˆ æ€§èƒ½è®ŠåŒ–åˆ†æ (ä»¥ MRR ç‚ºä¸»è¦æŒ‡æ¨™):")
        smart_vs_baseline = performance_comparison.get("smart_vs_baseline", {})
        always_vs_baseline = performance_comparison.get("always_rewrite_vs_baseline", {})
        smart_vs_always = performance_comparison.get("smart_vs_always_rewrite", {})
        
        if smart_vs_baseline:
            mrr_change_baseline = smart_vs_baseline.get("mrr_change", 0.0)
            mrr_change_pct_baseline = (mrr_change_baseline / max(no_rewrite_mrr, 0.001)) * 100
            print(f"   ğŸ¯ æ™ºèƒ½è§¸ç™¼ vs åŸºæº–æª¢ç´¢:")
            print(f"      MRR è®ŠåŒ–: {mrr_change_baseline:+.4f} ({mrr_change_pct_baseline:+.2f}%)")

        if always_vs_baseline:
            mrr_change_always_vs_baseline = always_vs_baseline.get("mrr_change", 0.0)
            mrr_change_pct_always_vs_baseline = (mrr_change_always_vs_baseline / max(no_rewrite_mrr, 0.001)) * 100
            print(f"   ğŸš€ ç¸½æ˜¯é‡å¯« vs åŸºæº–æª¢ç´¢:")
            print(f"      MRR è®ŠåŒ–: {mrr_change_always_vs_baseline:+.4f} ({mrr_change_pct_always_vs_baseline:+.2f}%)")
        
        if smart_vs_always:
            mrr_change_always = smart_vs_always.get("mrr_change", 0.0)
            mrr_change_pct_always = (mrr_change_always / max(always_mrr, 0.001)) * 100
            print(f"   âš–ï¸  æ™ºèƒ½è§¸ç™¼ vs ç¸½æ˜¯é‡å¯«:")
            print(f"      MRR è®ŠåŒ–: {mrr_change_always:+.4f} ({mrr_change_pct_always:+.2f}%)")
        
        # è§¸ç™¼åˆ†æ
        trigger_analysis = results.get("trigger_analysis", {})
        if trigger_analysis:
            print(f"\nğŸ§  æ™ºèƒ½è§¸ç™¼åˆ†æ:")
            skip_rate = trigger_analysis.get("skip_rewrite_rate", 0.0)
            trigger_rate = trigger_analysis.get("trigger_rewrite_rate", 0.0)
            overall_accuracy = trigger_analysis.get("overall_decision_accuracy", 0.0)
            cost_saving = trigger_analysis.get("cost_saving_estimation_percentage", 0.0)
            
            print(f"   ğŸ“Š è§¸ç™¼çµ±è¨ˆ:")
            print(f"      è·³éé‡å¯«æ¯”ä¾‹: {skip_rate:.1f}%")
            print(f"      è§¸ç™¼é‡å¯«æ¯”ä¾‹: {trigger_rate:.1f}%")
            print(f"      æ±ºç­–æº–ç¢ºç‡: {overall_accuracy:.1f}%")
            print(f"      æˆæœ¬ç¯€çœä¼°è¨ˆ: {cost_saving:.1f}%")
        
        # æ±ºç­–å“è³ªåˆ†æ
        decision_breakdown = results.get("decision_breakdown", {})
        if decision_breakdown:
            print(f"\nğŸ¯ æ±ºç­–å“è³ªåˆ†æ:")
            skip_correct = decision_breakdown.get("skip_correct_decisions", 0)
            skip_wrong = decision_breakdown.get("skip_wrong_decisions", 0)
            trigger_correct = decision_breakdown.get("trigger_correct_decisions", 0)
            trigger_wrong = decision_breakdown.get("trigger_wrong_decisions", 0)
            
            print(f"   âœ… æ­£ç¢ºçš„è·³éæ±ºç­–: {skip_correct} å€‹")
            print(f"   âŒ éŒ¯èª¤çš„è·³éæ±ºç­–: {skip_wrong} å€‹")
            print(f"   âœ… æ­£ç¢ºçš„è§¸ç™¼æ±ºç­–: {trigger_correct} å€‹")
            print(f"   âŒ éŒ¯èª¤çš„è§¸ç™¼æ±ºç­–: {trigger_wrong} å€‹")
        
        # æ™ºèƒ½å»ºè­°
        recommendations = results.get("recommendations", [])
        if recommendations:
            print(f"\nğŸ’¡ æ™ºèƒ½å»ºè­°:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        print("="*100 + "\n")
    
    def print_detailed_case_analysis(self, results: Dict[str, Any]):
        """è¼¸å‡ºè©³ç´°çš„æ¡ˆä¾‹åˆ†æï¼ˆç•¶å•Ÿç”¨è©³ç´°æ¨¡å¼æ™‚ï¼‰"""
        if not results.get("evaluation_parameters", {}).get("verbose_mode", False):
            return
            
        case_details = results.get("detailed_case_results", [])
        if not case_details:
            return
        
        print("\n" + "ğŸ”" + "="*90)
        print("ğŸ“ è©³ç´°æ¡ˆä¾‹åˆ†æå ±å‘Š")
        print("ğŸ”" + "="*90)
        
        for i, case in enumerate(case_details, 1):
            question = case.get("question", "")[:80] + "..." if len(case.get("question", "")) > 80 else case.get("question", "")
            
            # For smart trigger, we compare smart vs baseline(no_rewrite)
            if results.get("evaluation_type") == "smart_trigger_strategy":
                baseline_mrr = case.get("no_rewrite_metrics", {}).get("mrr", 0.0)
                optimized_mrr = case.get("smart_trigger_metrics", {}).get("mrr", 0.0)
                optimized_label = "æ™ºèƒ½è§¸ç™¼"
            else: # For standard evaluation
                baseline_mrr = case.get("baseline_metrics", {}).get("mrr", 0.0)
                optimized_mrr = case.get("optimized_metrics", {}).get("mrr", 0.0)
                optimized_label = "å„ªåŒ–"

            improvement = optimized_mrr - baseline_mrr
            
            # ç‹€æ…‹åœ–æ¨™
            if improvement > 0.1:
                status_icon = "ğŸš€"  # é¡¯è‘—æå‡
            elif improvement > 0.01:
                status_icon = "âœ…"  # è¼•å¾®æå‡
            elif improvement < -0.01:
                status_icon = "âŒ"  # ä¸‹é™
            else:
                status_icon = "âš–ï¸"   # æŒå¹³
            
            print(f"\nğŸ“‹ æ¡ˆä¾‹ {i:2d}: {status_icon}")
            print(f"   å•é¡Œ: {question}")
            print(f"   åŸºæº– MRR: {baseline_mrr:.4f} | {optimized_label} MRR: {optimized_mrr:.4f} | è®ŠåŒ–: {improvement:+.4f}")
            
            # é¡¯ç¤ºé‡å¯«æŸ¥è©¢
            rewritten_queries = case.get("rewritten_queries", [])
            if rewritten_queries:
                print(f"   é‡å¯«æŸ¥è©¢ ({len(rewritten_queries)} å€‹):")
                for j, rq in enumerate(rewritten_queries, 1):
                    rq_short = rq[:60] + "..." if len(rq) > 60 else rq
                    print(f"     {j}. {rq_short}")
            
            # é¡¯ç¤ºæ‰¾åˆ°çš„ç›¸é—œæ–‡ä»¶
            expected_ids = set(case.get("expected_doc_ids", []))
            baseline_found = set(case.get("baseline_retrieved_ids", [])) & expected_ids
            optimized_found = set(case.get("optimized_retrieved_ids", [])) & expected_ids
            
            if baseline_found or optimized_found:
                print(f"   æ‰¾åˆ°ç›¸é—œæ–‡ä»¶:")
                print(f"     åŸºæº–: {list(baseline_found)} ({len(baseline_found)}/{len(expected_ids)})")
                print(f"     å„ªåŒ–: {list(optimized_found)} ({len(optimized_found)}/{len(expected_ids)})")
        
        print("ğŸ”" + "="*90 + "\n")

    def save_results(self, results: Dict[str, Any], output_path: str):
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=4, ensure_ascii=False)
            logger.info(f"è©•ä¼°çµæœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜çµæœå¤±æ•—: {e}", exc_info=True)

    async def run_evaluation_flow(self, dataset_path: str, eval_params: Dict):
        logger.info(f"å¾ {dataset_path} åŠ è¼‰æ¸¬è©¦æ•¸æ“šé›†")
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                test_cases = json.load(f)
            logger.info(f"æˆåŠŸåŠ è¼‰ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
        except Exception as e:
            logger.error(f"åŠ è¼‰æ¸¬è©¦æ•¸æ“šé›†å¤±æ•—: {e}", exc_info=True)
            return

        try:
            await self.initialize_services()
            
            # æ ¹æ“šè©•ä¼°æ¨¡å¼é¸æ“‡è©•ä¼°æ–¹æ³•
            evaluation_mode = eval_params.get('evaluation_mode', 'standard')
            
            if evaluation_mode == 'smart_trigger':
                logger.info("ğŸ§  åŸ·è¡Œæ™ºèƒ½è§¸ç™¼ç­–ç•¥è©•ä¼°")
                results = await self.evaluate_smart_trigger_strategy(test_cases, eval_params)
                self.print_smart_trigger_results(results)
                output_prefix = "smart_trigger_evaluation"
            else:
                logger.info("ğŸ“Š åŸ·è¡Œæ¨™æº–æŸ¥è©¢é‡å¯«è©•ä¼°")
                results = await self.evaluate_retrieval_accuracy(test_cases, eval_params)
                self.print_results(results)
                output_prefix = "standard_evaluation"
            
            # è¼¸å‡ºè©³ç´°æ¡ˆä¾‹åˆ†æï¼ˆå¦‚æœå•Ÿç”¨è©³ç´°æ¨¡å¼ï¼‰
            self.print_detailed_case_analysis(results)
            
            output_dir = os.path.join(os.path.dirname(__file__), 'extractions')
            os.makedirs(output_dir, exist_ok=True)
            timestamp = uuid.uuid4().hex[:8]
            output_filename = f"{output_prefix}_results_{timestamp}.json"
            output_path = os.path.join(output_dir, output_filename)
            self.save_results(results, output_path)

        finally:
            if self.session:
                await self.session.close()
                logger.info("HTTP session å·²é—œé–‰")

async def main():
    parser = argparse.ArgumentParser(description="è©•ä¼°æŸ¥è©¢é‡å¯«å°æª¢ç´¢æº–ç¢ºæ€§çš„å½±éŸ¿ - æ”¯æ´æ™ºèƒ½è§¸ç™¼ç­–ç•¥")
    parser.add_argument("--dataset", type=str, default="test_one.json", help="åŒ…å«æŸ¥è©¢å’Œé æœŸæ–‡æª”IDçš„JSONæ•¸æ“šé›†æ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--top_k", type=int, default=10, help="æª¢ç´¢æ™‚è¿”å›çš„æ–‡æª”æ•¸é‡")
    parser.add_argument("--similarity_threshold", type=float, default=0.2, help="æª¢ç´¢æ™‚çš„ç›¸ä¼¼åº¦é–¾å€¼")
    parser.add_argument("--query_rewrite_count", type=int, default=3, help="ç‚ºæ¯å€‹åŸå§‹æŸ¥è©¢ç”Ÿæˆå¤šå°‘å€‹é‡å¯«ç‰ˆæœ¬")
    parser.add_argument("--verbose", action="store_true", help="å•Ÿç”¨è©³ç´°æ¨¡å¼ï¼Œé¡¯ç¤ºæ¯å€‹æ¸¬è©¦æ¡ˆä¾‹çš„è©³ç´°çµæœ")
    parser.add_argument("--save-detailed", action="store_true", help="ä¿å­˜è©³ç´°çš„å€‹åˆ¥æ¡ˆä¾‹åˆ†æçµæœ")
    
    # æ–°å¢æ™ºèƒ½è§¸ç™¼ç›¸é—œåƒæ•¸
    parser.add_argument("--mode", type=str, choices=['standard', 'smart_trigger'], default='standard', 
                       help="è©•ä¼°æ¨¡å¼: 'standard' ç‚ºæ¨™æº–é‡å¯«è©•ä¼°, 'smart_trigger' ç‚ºæ™ºèƒ½è§¸ç™¼ç­–ç•¥è©•ä¼°")
    parser.add_argument("--confidence_threshold", type=float, default=0.75, 
                       help="æ™ºèƒ½è§¸ç™¼çš„ç½®ä¿¡åº¦é–€æª» (åƒ…åœ¨ smart_trigger æ¨¡å¼ä¸‹ç”Ÿæ•ˆ)")
    
    args = parser.parse_args()

    eval_params = {
        "top_k": args.top_k,
        "similarity_threshold": args.similarity_threshold,
        "query_rewrite_count": args.query_rewrite_count,
        "verbose_mode": args.verbose,
        "save_detailed_analysis": args.save_detailed,
        "evaluation_mode": args.mode,
        "confidence_threshold": args.confidence_threshold
    }

    evaluator = QueryOptimizationRetrievalEvaluator()
    dataset_full_path = os.path.join(os.path.dirname(__file__), args.dataset)
    await evaluator.run_evaluation_flow(dataset_full_path, eval_params)

if __name__ == "__main__":
    asyncio.run(main()) 