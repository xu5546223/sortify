"""
å‘é‡åŒ–ç­–ç•¥è©•ä¼°è…³æœ¬

å°ˆé–€è©•ä¼° AI é‚è¼¯åˆ†å¡Š vs å›ºå®šå¤§å°åˆ†å¡Š çš„å¬å›æº–ç¢ºåº¦
ä¸åŒ…å«æŸ¥è©¢é‡å¯«ï¼Œç´”å‘é‡å¬å›æ¸¬è©¦

è©•ä¼°æŒ‡æ¨™ï¼š
- Hit Rate @K: å‰ K å€‹çµæœä¸­æ˜¯å¦åŒ…å«æ­£ç¢ºæ–‡æª”
- MRR (Mean Reciprocal Rank): æ­£ç¢ºæ–‡æª”çš„å¹³å‡å€’æ•¸æ’å
- nDCG @K: æ¨™æº–åŒ–æŠ˜æç´¯ç©å¢ç›Š

ä½¿ç”¨æ–¹å¼ï¼š
    python evaluate_chunking_strategy.py --dataset QA_dataset.json --top_k 5
"""

import json
import asyncio
import logging
import sys
import argparse
from typing import List, Dict, Any, Optional
from datetime import datetime
import numpy as np
import aiohttp
from dotenv import load_dotenv
import os

# --- .env è¼‰å…¥ ---
script_dir = os.path.dirname(os.path.abspath(__file__))
dotenv_path = os.path.join(script_dir, '.env')
if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path, override=True)
else:
    print(f"è­¦å‘Š: æ‰¾ä¸åˆ° .env æ–‡ä»¶: {dotenv_path}")

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ChunkingStrategyEvaluator:
    """å‘é‡åŒ–ç­–ç•¥è©•ä¼°å™¨ - ç´”å‘é‡å¬å›æ¸¬è©¦"""
    
    def __init__(self):
        self.api_base_url = os.getenv('API_URL', 'http://localhost:8000')
        self.api_username = os.getenv('USERNAME')
        self.api_password = os.getenv('PASSWORD')
        
        if not self.api_username or not self.api_password:
            raise ValueError("è«‹åœ¨ .env æ–‡ä»¶ä¸­è¨­ç½® USERNAME å’Œ PASSWORD")
        
        self.session: Optional[aiohttp.ClientSession] = None
        self.access_token: Optional[str] = None
        
        logger.info(f"è©•ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼ŒAPI: {self.api_base_url}")
    
    async def initialize(self):
        """åˆå§‹åŒ– API é€£æ¥"""
        self.session = aiohttp.ClientSession()
        
        # ç™»å…¥ç²å– token
        login_url = f"{self.api_base_url}/api/v1/auth/token"
        login_data = {"username": self.api_username, "password": self.api_password}
        
        try:
            async with self.session.post(login_url, data=login_data) as response:
                response.raise_for_status()
                result = await response.json()
                self.access_token = result.get("access_token")
                
                if self.access_token:
                    logger.info("âœ… ç™»å…¥æˆåŠŸ")
                else:
                    raise ValueError("ç™»å…¥éŸ¿æ‡‰ä¸­æ²’æœ‰ access_token")
        except Exception as e:
            logger.error(f"âŒ ç™»å…¥å¤±æ•—: {e}")
            raise
    
    async def close(self):
        """é—œé–‰é€£æ¥"""
        if self.session:
            await self.session.close()
    
    def _get_headers(self) -> Dict[str, str]:
        """ç²å–èªè­‰æ¨™é ­"""
        return {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json"
        }
    
    async def search_vectors(
        self, 
        query: str, 
        top_k: int = 5,
        similarity_threshold: float = 0.3
    ) -> List[Dict]:
        """åŸ·è¡Œå‘é‡æœç´¢"""
        url = f"{self.api_base_url}/api/v1/vector-db/semantic-search"
        
        payload = {
            "query": query,
            "top_k": top_k,
            "similarity_threshold": similarity_threshold,
            "enable_hybrid_search": True,  # ä½¿ç”¨å…©éšæ®µæ··åˆæœç´¢
            "enable_diversity_optimization": False  # é—œé–‰å¤šæ¨£æ€§å„ªåŒ–ï¼Œç´”å¬å›æ¸¬è©¦
        }
        
        try:
            async with self.session.post(url, json=payload, headers=self._get_headers()) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    error_text = await response.text()
                    logger.error(f"æœç´¢å¤±æ•— ({response.status}): {error_text[:200]}")
                    return []
        except Exception as e:
            logger.error(f"æœç´¢ç•°å¸¸: {e}")
            return []
    
    def calculate_hit_rate(self, expected_ids: List[str], retrieved_ids: List[str], k: int) -> float:
        """è¨ˆç®— Hit Rate @K"""
        expected_set = set(expected_ids)
        retrieved_set = set(retrieved_ids[:k])
        return 1.0 if expected_set.intersection(retrieved_set) else 0.0
    
    def calculate_mrr(self, expected_ids: List[str], retrieved_ids: List[str]) -> float:
        """è¨ˆç®— MRR (Mean Reciprocal Rank)"""
        expected_set = set(expected_ids)
        for rank, doc_id in enumerate(retrieved_ids, 1):
            if doc_id in expected_set:
                return 1.0 / rank
        return 0.0
    
    def calculate_ndcg(self, expected_ids: List[str], retrieved_ids: List[str], k: int) -> float:
        """è¨ˆç®— nDCG @K"""
        expected_set = set(expected_ids)
        
        # DCG
        relevance = [1 if doc_id in expected_set else 0 for doc_id in retrieved_ids[:k]]
        dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance))
        
        # IDCG
        num_relevant = min(k, len(expected_ids))
        idcg = sum(1 / np.log2(i + 2) for i in range(num_relevant))
        
        return dcg / idcg if idcg > 0 else 0.0
    
    async def evaluate_dataset(
        self, 
        test_cases: List[Dict],
        top_k: int = 5,
        similarity_threshold: float = 0.3,
        verbose: bool = False
    ) -> Dict[str, Any]:
        """è©•ä¼°æ•´å€‹æ•¸æ“šé›†"""
        
        logger.info(f"é–‹å§‹è©•ä¼° {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹...")
        logger.info(f"åƒæ•¸: top_k={top_k}, similarity_threshold={similarity_threshold}")
        
        results = []
        hit_counts = {1: 0, 3: 0, 5: 0, 10: 0}
        mrr_sum = 0.0
        ndcg_sums = {1: 0.0, 3: 0.0, 5: 0.0, 10: 0.0}
        
        # æŒ‰å•é¡Œé¡å‹åˆ†çµ„çµ±è¨ˆ
        stats_by_type = {}
        
        for i, case in enumerate(test_cases):
            question = case.get('question', '')
            expected_doc_ids = case.get('expected_relevant_doc_ids', [])
            question_type = case.get('question_type', 'unknown')
            
            if not question or not expected_doc_ids:
                continue
            
            # åŸ·è¡Œæœç´¢
            search_results = await self.search_vectors(question, top_k, similarity_threshold)
            retrieved_ids = [r.get('document_id', '') for r in search_results]
            
            # è¨ˆç®—æŒ‡æ¨™
            mrr = self.calculate_mrr(expected_doc_ids, retrieved_ids)
            mrr_sum += mrr
            
            for k in [1, 3, 5, 10]:
                if self.calculate_hit_rate(expected_doc_ids, retrieved_ids, k) > 0:
                    hit_counts[k] += 1
                ndcg_sums[k] += self.calculate_ndcg(expected_doc_ids, retrieved_ids, k)
            
            # æŒ‰é¡å‹çµ±è¨ˆ
            if question_type not in stats_by_type:
                stats_by_type[question_type] = {'count': 0, 'hits': 0, 'mrr_sum': 0.0}
            stats_by_type[question_type]['count'] += 1
            stats_by_type[question_type]['mrr_sum'] += mrr
            if self.calculate_hit_rate(expected_doc_ids, retrieved_ids, top_k) > 0:
                stats_by_type[question_type]['hits'] += 1
            
            # è¨˜éŒ„è©³ç´°çµæœ
            case_result = {
                'question': question[:80] + '...' if len(question) > 80 else question,
                'question_type': question_type,
                'expected_doc_ids': expected_doc_ids,
                'retrieved_doc_ids': retrieved_ids[:5],
                'hit': bool(set(expected_doc_ids) & set(retrieved_ids[:top_k])),
                'mrr': mrr
            }
            results.append(case_result)
            
            # é€²åº¦é¡¯ç¤º
            if verbose or (i + 1) % 10 == 0:
                hit_symbol = "âœ…" if case_result['hit'] else "âŒ"
                print(f"  [{i+1:3d}/{len(test_cases)}] {hit_symbol} MRR={mrr:.3f} | {question[:50]}...")
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        n = len(results)
        if n == 0:
            return {"error": "æ²’æœ‰æœ‰æ•ˆçš„æ¸¬è©¦æ¡ˆä¾‹"}
        
        evaluation_result = {
            "evaluation_type": "chunking_strategy_retrieval",
            "timestamp": datetime.now().isoformat(),
            "total_cases": len(test_cases),
            "processed_cases": n,
            "parameters": {
                "top_k": top_k,
                "similarity_threshold": similarity_threshold
            },
            "overall_metrics": {
                "hit_rate": {
                    "@1": hit_counts[1] / n,
                    "@3": hit_counts[3] / n,
                    "@5": hit_counts[5] / n,
                    "@10": hit_counts[10] / n
                },
                "mrr": mrr_sum / n,
                "ndcg": {
                    "@1": ndcg_sums[1] / n,
                    "@3": ndcg_sums[3] / n,
                    "@5": ndcg_sums[5] / n,
                    "@10": ndcg_sums[10] / n
                }
            },
            "metrics_by_question_type": {}
        }
        
        # æŒ‰å•é¡Œé¡å‹çš„æŒ‡æ¨™
        for qtype, stats in stats_by_type.items():
            if stats['count'] > 0:
                evaluation_result["metrics_by_question_type"][qtype] = {
                    "count": stats['count'],
                    "hit_rate": stats['hits'] / stats['count'],
                    "mrr": stats['mrr_sum'] / stats['count']
                }
        
        return evaluation_result
    
    def print_results(self, results: Dict[str, Any]):
        """æ ¼å¼åŒ–è¼¸å‡ºè©•ä¼°çµæœ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å‘é‡åŒ–ç­–ç•¥å¬å›è©•ä¼°çµæœ")
        print("=" * 60)
        
        metrics = results.get("overall_metrics", {})
        
        print(f"\nğŸ“ˆ æ•´é«”æŒ‡æ¨™ (å…± {results.get('processed_cases', 0)} å€‹æ¡ˆä¾‹)")
        print("-" * 40)
        
        # Hit Rate
        hit_rate = metrics.get("hit_rate", {})
        print(f"  Hit Rate @1:  {hit_rate.get('@1', 0):.2%}")
        print(f"  Hit Rate @3:  {hit_rate.get('@3', 0):.2%}")
        print(f"  Hit Rate @5:  {hit_rate.get('@5', 0):.2%}")
        print(f"  Hit Rate @10: {hit_rate.get('@10', 0):.2%}")
        
        # MRR
        print(f"\n  MRR: {metrics.get('mrr', 0):.4f}")
        
        # nDCG
        ndcg = metrics.get("ndcg", {})
        print(f"\n  nDCG @1:  {ndcg.get('@1', 0):.4f}")
        print(f"  nDCG @3:  {ndcg.get('@3', 0):.4f}")
        print(f"  nDCG @5:  {ndcg.get('@5', 0):.4f}")
        print(f"  nDCG @10: {ndcg.get('@10', 0):.4f}")
        
        # æŒ‰å•é¡Œé¡å‹
        by_type = results.get("metrics_by_question_type", {})
        if by_type:
            print(f"\nğŸ“‹ æŒ‰å•é¡Œé¡å‹åˆ†æ")
            print("-" * 40)
            for qtype, stats in by_type.items():
                print(f"  {qtype}:")
                print(f"    æ•¸é‡: {stats['count']}")
                print(f"    Hit Rate: {stats['hit_rate']:.2%}")
                print(f"    MRR: {stats['mrr']:.4f}")
        
        print("\n" + "=" * 60)


async def main():
    parser = argparse.ArgumentParser(description='å‘é‡åŒ–ç­–ç•¥å¬å›è©•ä¼°')
    parser.add_argument('--dataset', type=str, default='QA_dataset.json', help='æ¸¬è©¦æ•¸æ“šé›†è·¯å¾‘')
    parser.add_argument('--top_k', type=int, default=5, help='æª¢ç´¢çµæœæ•¸é‡')
    parser.add_argument('--threshold', type=float, default=0.3, help='ç›¸ä¼¼åº¦é–¾å€¼')
    parser.add_argument('--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°é€²åº¦')
    parser.add_argument('--output', type=str, default=None, help='çµæœè¼¸å‡ºæ–‡ä»¶')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡')
    
    args = parser.parse_args()
    
    # è¼‰å…¥æ•¸æ“šé›†
    dataset_path = os.path.join(script_dir, args.dataset)
    if not os.path.exists(dataset_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ•¸æ“šé›†: {dataset_path}")
        sys.exit(1)
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    
    if args.limit:
        test_cases = test_cases[:args.limit]
    
    print(f"ğŸ“‚ è¼‰å…¥æ•¸æ“šé›†: {args.dataset}")
    print(f"ğŸ“ æ¸¬è©¦æ¡ˆä¾‹æ•¸: {len(test_cases)}")
    
    # åŸ·è¡Œè©•ä¼°
    evaluator = ChunkingStrategyEvaluator()
    
    try:
        await evaluator.initialize()
        
        results = await evaluator.evaluate_dataset(
            test_cases,
            top_k=args.top_k,
            similarity_threshold=args.threshold,
            verbose=args.verbose
        )
        
        # è¼¸å‡ºçµæœ
        evaluator.print_results(results)
        
        # ä¿å­˜çµæœ
        if args.output:
            output_path = os.path.join(script_dir, args.output)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ çµæœå·²ä¿å­˜åˆ°: {output_path}")
        
    finally:
        await evaluator.close()


if __name__ == "__main__":
    asyncio.run(main())
